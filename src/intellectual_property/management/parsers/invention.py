"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –µ–¥–∏–Ω–æ–≥–æ DataFrame –¥–ª—è —Å–≤—è–∑–µ–π
"""

import logging
import gc
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict

import pandas as pd
from django.db import models, transaction
from django.utils.text import slugify
from tqdm import tqdm

from intellectual_property.models import IPObject, IPType, Person
from core.models import Organization

from .base import BaseFIPSParser
from ..utils.progress import batch_iterator

logger = logging.getLogger(__name__)


class InventionParser(BaseFIPSParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–≤—è–∑–µ–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–¥–∏–Ω—ã–π DataFrame –¥–ª—è –≤—Å–µ—Ö —Å–≤—è–∑–µ–π (–∞–≤—Ç–æ—Ä—ã + –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏)
    """

    def get_ip_type(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø –†–ò–î 'invention'"""
        return IPType.objects.filter(slug='invention').first()

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è CSV"""
        return ['registration number', 'invention name']

    def _has_data_changed(self, obj, new_data):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç–∞
        """
        fields_to_check = [
            ('name', obj.name, new_data['name']),
            ('application_date', obj.application_date, new_data['application_date']),
            ('registration_date', obj.registration_date, new_data['registration_date']),
            ('patent_starting_date', obj.patent_starting_date, new_data['patent_starting_date']),
            ('expiration_date', obj.expiration_date, new_data['expiration_date']),
            ('actual', obj.actual, new_data['actual']),
            ('publication_url', obj.publication_url, new_data['publication_url']),
            ('abstract', obj.abstract, new_data['abstract']),
            ('claims', obj.claims, new_data['claims']),
            ('creation_year', obj.creation_year, new_data['creation_year']),
        ]

        for field_name, old_val, new_val in fields_to_check:
            if old_val != new_val:
                return True
        return False

    def parse_dataframe(self, df, catalogue):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame
        """
        self.stdout.write("\nüîπ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π")

        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –†–ò–î
        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'invention' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats

        upload_date = catalogue.upload_date.date() if catalogue.upload_date else None

        # =====================================================================
        # –®–ê–ì 1: –°–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
        # =====================================================================
        self.stdout.write("üîπ –ß—Ç–µ–Ω–∏–µ CSV –∏ —Å–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        reg_num_to_row = {}
        skipped_empty = 0
        
        for idx, row in df.iterrows():
            reg_num = self.clean_string(row.get('registration number'))
            if reg_num:
                reg_num_to_row[reg_num] = row
            else:
                skipped_empty += 1

        self.stdout.write(f"üîπ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: {len(reg_num_to_row)} (–ø—Ä–æ–ø—É—â–µ–Ω–æ –ø—É—Å—Ç—ã—Ö: {skipped_empty})")

        # =====================================================================
        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î
        # =====================================================================
        self.stdout.write("üîπ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î")
        
        existing_objects = {}
        batch_size = 500
        reg_numbers = list(reg_num_to_row.keys())
        
        with tqdm(total=len(reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—á–∫–∞–º–∏", unit="–∑–∞–ø") as pbar:
            for i in range(0, len(reg_numbers), batch_size):
                batch_numbers = reg_numbers[i:i+batch_size]
                
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_numbers,
                    ip_type=ip_type
                ).select_related('ip_type'):
                    existing_objects[obj.registration_number] = obj
                
                pbar.update(len(batch_numbers))

        self.stdout.write(f"üîπ –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(existing_objects)}")

        # =====================================================================
        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è IPObject
        # =====================================================================
        self.stdout.write("üîπ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject")
        
        to_create = []
        to_update = []
        skipped_by_date = []
        unchanged_count = 0
        error_reg_numbers = []

        relations_data = []
        
        with tqdm(total=len(reg_num_to_row), desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject", unit="–∑–∞–ø") as pbar:
            for reg_num, row in reg_num_to_row.items():
                try:
                    if not self.command.force and upload_date and reg_num in existing_objects:
                        existing = existing_objects[reg_num]
                        if existing.updated_at and existing.updated_at.date() >= upload_date:
                            skipped_by_date.append(reg_num)
                            pbar.update(1)
                            continue

                    name = self.clean_string(row.get('invention name'))
                    if name:
                        name = self.rid_formatter.format(name)
                    else:
                        name = f"–ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ ‚Ññ{reg_num}"

                    application_date = self.parse_date(row.get('application date'))
                    registration_date = self.parse_date(row.get('registration date'))
                    patent_starting_date = self.parse_date(row.get('patent starting date'))
                    expiration_date = self.parse_date(row.get('expiration date'))
                    actual = self.parse_bool(row.get('actual'))
                    publication_url = self.clean_string(row.get('publication URL'))
                    abstract = self.clean_string(row.get('abstract'))
                    claims = self.clean_string(row.get('claims'))

                    creation_year = None
                    if application_date:
                        creation_year = application_date.year
                    elif registration_date:
                        creation_year = registration_date.year

                    obj_data = {
                        'registration_number': reg_num,
                        'ip_type_id': ip_type.id,
                        'name': name,
                        'application_date': application_date,
                        'registration_date': registration_date,
                        'patent_starting_date': patent_starting_date,
                        'expiration_date': expiration_date,
                        'actual': actual,
                        'publication_url': publication_url,
                        'abstract': abstract,
                        'claims': claims,
                        'creation_year': creation_year,
                    }

                    if reg_num in existing_objects:
                        if self._has_data_changed(existing_objects[reg_num], obj_data):
                            to_update.append(obj_data)
                        else:
                            unchanged_count += 1
                    else:
                        to_create.append(obj_data)

                    # –ê–≤—Ç–æ—Ä—ã
                    authors_str = row.get('authors')
                    if not pd.isna(authors_str) and authors_str:
                        authors = self.parse_authors(authors_str)
                        for author in authors:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': author['full_name'],
                                'entity_type': 'person',
                                'relation_type': 'author',
                                'entity_data': author
                            })

                    # –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏
                    holders_str = row.get('patent holders')
                    if not pd.isna(holders_str) and holders_str:
                        holders = self.parse_patent_holders(holders_str)
                        for holder in holders:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': holder,
                                'entity_type': None,
                                'relation_type': 'holder',
                                'entity_data': {'full_name': holder}
                            })

                except Exception as e:
                    error_reg_numbers.append(reg_num)
                    if len(error_reg_numbers) < 10:
                        self.stdout.write(self.style.ERROR(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    elif len(error_reg_numbers) == 10:
                        self.stdout.write(self.style.WARNING("\n‚ö†Ô∏è ... –∏ –¥–∞–ª–µ–µ –æ—à–∏–±–∫–∏ –ø–æ–¥–∞–≤–ª—è—é—Ç—Å—è"))
                    
                    logger.error(f"Error preparing invention {reg_num}: {e}", exc_info=True)

                pbar.update(1)

        self.stdout.write(f"üîπ –ò—Ç–æ–≥–æ: –Ω–æ–≤—ã—Ö={len(to_create)}, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ={len(to_update)}, "
                         f"–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={unchanged_count}, –æ—à–∏–±–æ–∫={len(error_reg_numbers)}")

        stats['skipped_by_date'] = len(skipped_by_date)
        stats['skipped'] += len(skipped_by_date)
        stats['errors'] = len(error_reg_numbers)
        stats['unchanged'] = unchanged_count

        # =====================================================================
        # –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ IPObject
        # =====================================================================
        if to_create and not self.command.dry_run:
            self.stdout.write(f"üîπ –°–æ–∑–¥–∞–Ω–∏–µ {len(to_create)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_create), desc="–°–æ–∑–¥–∞–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['created'] = self._bulk_create_objects(to_create, pbar)

        if to_update and not self.command.dry_run:
            self.stdout.write(f"üîπ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {len(to_update)} –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_update), desc="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['updated'] = self._bulk_update_objects(to_update, existing_objects, pbar)

        # =====================================================================
        # –®–ê–ì 5: –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ reg_number -> ip_id
        # =====================================================================
        self.stdout.write("üîπ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        all_reg_numbers = list(set(
            list(existing_objects.keys()) + 
            [data['registration_number'] for data in to_create]
        ))
        
        reg_to_ip = {}
        with tqdm(total=len(all_reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ ID –æ–±—ä–µ–∫—Ç–æ–≤", unit="–∑–∞–ø") as pbar:
            batch_size = 1000
            for i in range(0, len(all_reg_numbers), batch_size):
                batch_nums = all_reg_numbers[i:i+batch_size]
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_nums,
                    ip_type=ip_type
                ).only('id', 'registration_number'):
                    reg_to_ip[obj.registration_number] = obj.id
                pbar.update(len(batch_nums))

        self.stdout.write(f"üîπ –ó–∞–≥—Ä—É–∂–µ–Ω–æ ID –¥–ª—è {len(reg_to_ip)} –æ–±—ä–µ–∫—Ç–æ–≤")

        # =====================================================================
        # –®–ê–ì 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π DataFrame
        # =====================================================================
        if relations_data and not self.command.dry_run:
            self.stdout.write("üîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π")
            self._process_relations_dataframe(relations_data, reg_to_ip)

        gc.collect()

        stats['processed'] = len(df) - stats['skipped'] - stats['errors']

        self.stdout.write(self.style.SUCCESS("\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"   –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats['unchanged']}")
        self.stdout.write(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']} (–∏–∑ –Ω–∏—Ö –ø–æ –¥–∞—Ç–µ: {stats['skipped_by_date']})")
        self.stdout.write(f"   –û—à–∏–±–æ–∫: {stats['errors']}")

        return stats

    def _bulk_create_objects(self, to_create: List[Dict], pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        created_count = 0
        batch_size = 1000

        for batch in batch_iterator(to_create, batch_size):
            create_objects = [IPObject(**data) for data in batch]
            IPObject.objects.bulk_create(create_objects, batch_size=batch_size)
            created_count += len(batch)
            pbar.update(len(batch))

        return created_count

    def _bulk_update_objects(self, to_update: List[Dict], existing_objects: Dict, pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        updated_count = 0
        BATCH_UPDATE_SIZE = 500

        for batch in batch_iterator(to_update, BATCH_UPDATE_SIZE):
            with transaction.atomic():
                for data in batch:
                    obj = existing_objects[data['registration_number']]
                    update_fields = []
                    for field, value in data.items():
                        if field != 'registration_number' and getattr(obj, field) != value:
                            setattr(obj, field, value)
                            update_fields.append(field)
                    if update_fields:
                        obj.save(update_fields=update_fields)
                        updated_count += 1
            pbar.update(len(batch))

        return updated_count

    def _process_relations_dataframe(self, relations_data: List[Dict], reg_to_ip: Dict):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Å–≤—è–∑–µ–π —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π DataFrame"""
        if not relations_data:
            self.stdout.write("   –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–≤—è–∑–µ–π")
            return

        self.stdout.write("   –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å–≤—è–∑–µ–π")
        df_relations = pd.DataFrame(relations_data)
        
        self.stdout.write(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π —Å–≤—è–∑–µ–π: {len(df_relations)}")
        self.stdout.write(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤: {df_relations['reg_number'].nunique()}")

        self.stdout.write("   –î–æ–±–∞–≤–ª–µ–Ω–∏–µ ID –æ–±—ä–µ–∫—Ç–æ–≤")
        df_relations['ip_id'] = df_relations['reg_number'].map(reg_to_ip)

        missing_ip = df_relations['ip_id'].isna().sum()
        if missing_ip > 0:
            self.stdout.write(self.style.WARNING(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ {missing_ip} —Å–≤—è–∑–µ–π —Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º–∏ ID –æ–±—ä–µ–∫—Ç–æ–≤"))
            df_relations = df_relations.dropna(subset=['ip_id']).copy()
        
        df_relations['ip_id'] = df_relations['ip_id'].astype(int)

        # =====================================================================
        # –®–ê–ì 6.1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
        # =====================================================================
        self.stdout.write("   –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Natasha")
        
        unique_entities = df_relations[['entity_name', 'entity_type']].drop_duplicates()
        holders_to_check = unique_entities[unique_entities['entity_type'].isna()]['entity_name'].tolist()

        if holders_to_check:
            self.stdout.write(f"   –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è {len(holders_to_check)} –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π")
            entity_type_map = self.type_detector.detect_type_batch(holders_to_check)

            mask = df_relations['entity_type'].isna()
            df_relations.loc[mask, 'entity_type'] = \
                df_relations.loc[mask, 'entity_name'].map(entity_type_map)

        type_stats = df_relations['entity_type'].value_counts().to_dict()
        self.stdout.write(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤: –ª—é–¥–∏={type_stats.get('person', 0)}, "
                         f"–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏={type_stats.get('organization', 0)}")

        # =====================================================================
        # –®–ê–ì 6.2: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û)
        # =====================================================================
        unique_entities = df_relations[['entity_name', 'entity_type']].drop_duplicates()
        
        persons_df = unique_entities[unique_entities['entity_type'] == 'person']
        orgs_df = unique_entities[unique_entities['entity_type'] == 'organization']

        person_map = {}
        if not persons_df.empty:
            self.stdout.write(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(persons_df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π")
            person_map = self._create_persons_bulk(persons_df)

        org_map = {}
        if not orgs_df.empty:
            self.stdout.write(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(orgs_df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
            org_map = self._create_organizations_bulk(orgs_df)

        # =====================================================================
        # –®–ê–ì 6.3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π
        # =====================================================================
        self.stdout.write("   –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ –ë–î")
        
        authors_df = df_relations[df_relations['relation_type'] == 'author'].copy()
        holders_df = df_relations[df_relations['relation_type'] == 'holder'].copy()

        author_relations = []
        if not authors_df.empty:
            authors_df['person_id'] = authors_df['entity_name'].map(
                {name: p.ceo_id for name, p in person_map.items()}
            )
            authors_unique = authors_df[['ip_id', 'person_id']].drop_duplicates()
            author_relations = [(row['ip_id'], row['person_id']) 
                               for _, row in authors_unique.iterrows()]
            self.stdout.write(f"   –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(author_relations)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤")

        holder_person_relations = []
        holder_org_relations = []
        
        if not holders_df.empty:
            holders_persons = holders_df[holders_df['entity_type'] == 'person'].copy()
            if not holders_persons.empty:
                holders_persons['person_id'] = holders_persons['entity_name'].map(
                    {name: p.ceo_id for name, p in person_map.items()}
                )
                holders_persons_unique = holders_persons[['ip_id', 'person_id']].drop_duplicates()
                holder_person_relations = [(row['ip_id'], row['person_id']) 
                                          for _, row in holders_persons_unique.iterrows()]
                self.stdout.write(f"   –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(holder_person_relations)} —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–ª—é–¥–µ–π")

            holders_orgs = holders_df[holders_df['entity_type'] == 'organization'].copy()
            if not holders_orgs.empty:
                holders_orgs['org_id'] = holders_orgs['entity_name'].map(
                    {name: o.organization_id for name, o in org_map.items()}
                )
                holders_orgs_unique = holders_orgs[['ip_id', 'org_id']].drop_duplicates()
                holder_org_relations = [(row['ip_id'], row['org_id']) 
                                       for _, row in holders_orgs_unique.iterrows()]
                self.stdout.write(f"   –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(holder_org_relations)} —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")

        # =====================================================================
        # –®–ê–ì 6.4: –ú–∞—Å—Å–æ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π
        # =====================================================================
        if author_relations:
            self.stdout.write("   –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤")
            ip_ids = list(set(ip_id for ip_id, _ in author_relations))
            with tqdm(total=len(ip_ids), desc="   –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤", unit="ip") as pbar:
                self._delete_author_relations(ip_ids, pbar)
            
            with tqdm(total=len(author_relations), desc="   –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤", unit="—Å–≤") as pbar:
                self._create_author_relations(author_relations, pbar)

        if holder_person_relations:
            self.stdout.write("   –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π (–ª—é–¥–∏)")
            ip_ids = list(set(ip_id for ip_id, _ in holder_person_relations))
            with tqdm(total=len(ip_ids), desc="   –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π", unit="ip") as pbar:
                self._delete_holder_person_relations(ip_ids, pbar)
            
            with tqdm(total=len(holder_person_relations), desc="   –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π", unit="—Å–≤") as pbar:
                self._create_holder_person_relations(holder_person_relations, pbar)

        if holder_org_relations:
            self.stdout.write("   –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π (–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)")
            ip_ids = list(set(ip_id for ip_id, _ in holder_org_relations))
            with tqdm(total=len(ip_ids), desc="   –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π", unit="ip") as pbar:
                self._delete_holder_org_relations(ip_ids, pbar)
            
            with tqdm(total=len(holder_org_relations), desc="   –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π", unit="—Å–≤") as pbar:
                self._create_holder_org_relations(holder_org_relations, pbar)

        self.stdout.write(self.style.SUCCESS("   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Å–≤—è–∑–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞"))

    def _create_persons_bulk(self, persons_df: pd.DataFrame) -> Dict:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ª—é–¥–µ–π –∏–∑ DataFrame —Å –∏–Ω–¥–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        """
        person_map = {}
        all_names = persons_df['entity_name'].tolist()
        total_names = len(all_names)
        
        self.stdout.write(f"      –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_names}")
        
        # –®–ê–ì 1: –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ª—é–¥–µ–π (–±—ã—Å—Ç—Ä—ã–π, –±–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞)
        self.stdout.write(f"      –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ª—é–¥–µ–π –≤ –ë–î...")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–º–µ–Ω–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        name_conditions = models.Q()
        name_to_parts = {}
        
        for name in all_names:
            parts = name.split()
            if len(parts) >= 2:
                last = parts[0]
                first = parts[1]
                middle = parts[2] if len(parts) > 2 else ''
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –º–∞–ø–ø–∏–Ω–≥–∞
                name_to_parts[name] = (last, first, middle)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —É—Å–ª–æ–≤–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
                if middle:
                    name_conditions |= models.Q(last_name=last, first_name=first, middle_name=middle)
                else:
                    name_conditions |= models.Q(last_name=last, first_name=first) & \
                                    (models.Q(middle_name='') | models.Q(middle_name__isnull=True))

        # –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ª—é–¥–µ–π –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
        existing_persons = {}
        if name_conditions:
            found_count = 0
            for person in Person.objects.filter(name_conditions).only(
                'ceo_id', 'last_name', 'first_name', 'middle_name', 'ceo'
            ):
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
                for name, (last, first, middle) in name_to_parts.items():
                    if (person.last_name == last and 
                        person.first_name == first and 
                        (not middle or person.middle_name == middle)):
                        existing_persons[name] = person
                        self.person_cache[name] = person
                        found_count += 1
                        break
            
            self.stdout.write(f"      –ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {found_count}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–≤—ã—Ö –ª—é–¥–µ–π
        new_names = [name for name in all_names if name not in existing_persons]
        new_count = len(new_names)
        
        self.stdout.write(f"      –ù–æ–≤—ã—Ö –ª—é–¥–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {new_count}")
        
        if new_names:
            # –®–ê–ì 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è
            self.stdout.write(f"      –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è...")
            
            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
            existing_slugs = set(Person.objects.values_list('slug', flat=True)[:100000])
            
            people_to_create = []
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –¥–ª—è bulk_create
            for name in new_names:
                parts = name.split()
                if len(parts) >= 2:
                    last_name = parts[0]
                    first_name = parts[1]
                    middle_name = parts[2] if len(parts) > 2 else ''
                    
                    name_parts_list = [last_name, first_name]
                    if middle_name:
                        name_parts_list.append(middle_name)
                    
                    base_slug = slugify(' '.join(name_parts_list)) or 'person'
                    unique_slug = base_slug
                    counter = 1
                    while unique_slug in existing_slugs:
                        unique_slug = f"{base_slug}-{counter}"
                        counter += 1
                    existing_slugs.add(unique_slug)
                    
                    person = Person(
                        ceo_id=max_id + len(people_to_create) + 1,
                        ceo=name,
                        last_name=last_name,
                        first_name=first_name,
                        middle_name=middle_name or '',
                        slug=unique_slug
                    )
                    people_to_create.append(person)
            
            # –®–ê–ì 3: –ú–∞—Å—Å–æ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Å –∏–Ω–¥–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            self.stdout.write(f"      –°–æ–∑–¥–∞–Ω–∏–µ –ª—é–¥–µ–π –ø–∞—á–∫–∞–º–∏ –ø–æ 500...")
            
            batch_size = 500
            created_count = 0
            
            for batch in batch_iterator(people_to_create, batch_size):
                Person.objects.bulk_create(batch, batch_size=batch_size)
                created_count += len(batch)
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 5000 –∑–∞–ø–∏—Å–µ–π
                if created_count % 5000 == 0 or created_count == new_count:
                    percent = (created_count / new_count) * 100
                    self.stdout.write(f"         –°–æ–∑–¥–∞–Ω–æ {created_count}/{new_count} ({percent:.1f}%)")
            
            # –®–ê–ì 4: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ª—é–¥–µ–π –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞
            self.stdout.write(f"      –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ª—é–¥–µ–π –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞...")
            
            created_names = [p.ceo for p in people_to_create]
            batch_size = 1000
            
            for i in range(0, len(created_names), batch_size):
                batch_names = created_names[i:i+batch_size]
                for person in Person.objects.filter(ceo__in=batch_names):
                    person_map[person.ceo] = person
                    self.person_cache[person.ceo] = person

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ª—é–¥–µ–π –≤ –º–∞–ø–ø–∏–Ω–≥
        person_map.update(existing_persons)
        
        self.stdout.write(f"      ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ª—é–¥–µ–π: {len(person_map)}")
        
        return person_map

    def _create_organizations_bulk(self, orgs_df: pd.DataFrame) -> Dict:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏–∑ DataFrame —Å –∏–Ω–¥–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        """
        org_map = {}
        all_names = orgs_df['entity_name'].tolist()
        total_names = len(all_names)
        
        self.stdout.write(f"      –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_names}")
        
        # –®–ê–ì 1: –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        self.stdout.write(f"      –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –≤ –ë–î...")
        
        existing_orgs = {}
        for org in Organization.objects.filter(name__in=all_names).only('organization_id', 'name'):
            existing_orgs[org.name] = org
            self.organization_cache[org.name] = org
        
        self.stdout.write(f"      –ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {len(existing_orgs)}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–≤—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        new_names = [name for name in all_names if name not in existing_orgs]
        new_count = len(new_names)
        
        self.stdout.write(f"      –ù–æ–≤—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {new_count}")
        
        if new_names:
            # –®–ê–ì 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è
            self.stdout.write(f"      –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è...")
            
            max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
            existing_slugs = set(Organization.objects.values_list('slug', flat=True)[:50000])
            
            orgs_to_create = []
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –¥–ª—è bulk_create
            for name in new_names:
                base_slug = slugify(name[:50]) or 'organization'
                unique_slug = base_slug
                counter = 1
                while unique_slug in existing_slugs:
                    unique_slug = f"{base_slug}-{counter}"
                    counter += 1
                existing_slugs.add(unique_slug)
                
                org = Organization(
                    organization_id=max_id + len(orgs_to_create) + 1,
                    name=name,
                    full_name=name,
                    short_name=name[:500] if len(name) > 500 else name,
                    slug=unique_slug,
                    register_opk=False,
                    strategic=False,
                )
                orgs_to_create.append(org)
            
            # –®–ê–ì 3: –ú–∞—Å—Å–æ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Å –∏–Ω–¥–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            self.stdout.write(f"      –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –ø–∞—á–∫–∞–º–∏ –ø–æ 500...")
            
            batch_size = 500
            created_count = 0
            
            for batch in batch_iterator(orgs_to_create, batch_size):
                Organization.objects.bulk_create(batch, batch_size=batch_size)
                created_count += len(batch)
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 5000 –∑–∞–ø–∏—Å–µ–π
                if created_count % 5000 == 0 or created_count == new_count:
                    percent = (created_count / new_count) * 100
                    self.stdout.write(f"         –°–æ–∑–¥–∞–Ω–æ {created_count}/{new_count} ({percent:.1f}%)")
            
            # –®–ê–ì 4: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞
            self.stdout.write(f"      –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞...")
            
            created_names = [o.name for o in orgs_to_create]
            batch_size = 1000
            
            for i in range(0, len(created_names), batch_size):
                batch_names = created_names[i:i+batch_size]
                for org in Organization.objects.filter(name__in=batch_names):
                    org_map[org.name] = org
                    self.organization_cache[org.name] = org

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –≤ –º–∞–ø–ø–∏–Ω–≥
        org_map.update(existing_orgs)
        
        self.stdout.write(f"      ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π: {len(org_map)}")
        
        return org_map

    def _create_persons_from_dataframe(self, persons_df: pd.DataFrame, pbar) -> Dict:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ª—é–¥–µ–π –∏–∑ DataFrame (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥, –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        """
        return self._create_persons_bulk(persons_df)

    def _create_organizations_from_dataframe(self, orgs_df: pd.DataFrame, pbar) -> Dict:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏–∑ DataFrame (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥, –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        """
        return self._create_organizations_bulk(orgs_df)

    def _delete_author_relations(self, ip_ids: List[int], pbar):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤"""
        delete_batch_size = 500
        for i in range(0, len(ip_ids), delete_batch_size):
            batch_ids = ip_ids[i:i+delete_batch_size]
            IPObject.authors.through.objects.filter(
                ipobject_id__in=batch_ids
            ).delete()
            pbar.update(len(batch_ids))

    def _delete_holder_person_relations(self, ip_ids: List[int], pbar):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–ª—é–¥–µ–π"""
        delete_batch_size = 500
        for i in range(0, len(ip_ids), delete_batch_size):
            batch_ids = ip_ids[i:i+delete_batch_size]
            IPObject.owner_persons.through.objects.filter(
                ipobject_id__in=batch_ids
            ).delete()
            pbar.update(len(batch_ids))

    def _delete_holder_org_relations(self, ip_ids: List[int], pbar):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        delete_batch_size = 500
        for i in range(0, len(ip_ids), delete_batch_size):
            batch_ids = ip_ids[i:i+delete_batch_size]
            IPObject.owner_organizations.through.objects.filter(
                ipobject_id__in=batch_ids
            ).delete()
            pbar.update(len(batch_ids))

    def _create_author_relations(self, relations: List[Tuple[int, int]], pbar):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤"""
        create_batch_size = 2000
        for batch in batch_iterator(relations, create_batch_size):
            through_objs = [
                IPObject.authors.through(
                    ipobject_id=ip_id,
                    person_id=person_id
                )
                for ip_id, person_id in batch
            ]
            IPObject.authors.through.objects.bulk_create(
                through_objs, batch_size=2000, ignore_conflicts=True
            )
            pbar.update(len(batch))

    def _create_holder_person_relations(self, relations: List[Tuple[int, int]], pbar):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–ª—é–¥–µ–π"""
        create_batch_size = 2000
        for batch in batch_iterator(relations, create_batch_size):
            through_objs = [
                IPObject.owner_persons.through(
                    ipobject_id=ip_id,
                    person_id=person_id
                )
                for ip_id, person_id in batch
            ]
            IPObject.owner_persons.through.objects.bulk_create(
                through_objs, batch_size=2000, ignore_conflicts=True
            )
            pbar.update(len(batch))

    def _create_holder_org_relations(self, relations: List[Tuple[int, int]], pbar):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        create_batch_size = 2000
        for batch in batch_iterator(relations, create_batch_size):
            through_objs = [
                IPObject.owner_organizations.through(
                    ipobject_id=ip_id,
                    organization_id=org_id
                )
                for ip_id, org_id in batch
            ]
            IPObject.owner_organizations.through.objects.bulk_create(
                through_objs, batch_size=2000, ignore_conflicts=True
            )
            pbar.update(len(batch))