"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
"""

import logging
from collections import defaultdict
import gc
import re

from django.db import models
from django.db.models import Q
from django.utils.text import slugify
from django.db import transaction
from tqdm import tqdm
import pandas as pd

from intellectual_property.models import IPObject, IPType, Person
from core.models import Organization
from .base import BaseFIPSParser

logger = logging.getLogger(__name__)


class InventionParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='invention').first()

    def get_required_columns(self):
        return ['registration number', 'invention name']

    def _has_data_changed(self, obj, new_data):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ"""
        fields_to_check = [
            ('name', obj.name, new_data['name']),
            ('application_date', obj.application_date, new_data['application_date']),
            ('registration_date', obj.registration_date, new_data['registration_date']),
            ('patent_starting_date', obj.patent_starting_date, new_data['patent_starting_date']),
            ('expiration_date', obj.expiration_date, new_data['expiration_date']),
            ('actual', obj.actual, new_data['actual']),
            ('publication_url', obj.publication_url, new_data['publication_url']),
            ('abstract', obj.abstract, new_data['abstract']),
            ('claims', obj.claims, new_data['claims']),
            ('creation_year', obj.creation_year, new_data['creation_year']),
        ]

        for field_name, old_val, new_val in fields_to_check:
            if old_val != new_val:
                return True
        return False

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π..."))

        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'invention' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats

        upload_date = catalogue.upload_date.date() if catalogue.upload_date else None

        # –®–ê–ì 1: –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞
        self.stdout.write("  üì• –ß—Ç–µ–Ω–∏–µ CSV...")
        all_reg_numbers = []
        reg_num_to_row = {}

        with tqdm(total=len(df), desc="     –ü—Ä–æ–≥—Ä–µ—Å—Å", unit=" –∑–∞–ø", leave=False) as pbar:
            for idx, row in df.iterrows():
                reg_num = self.clean_string(row.get('registration number'))
                if reg_num:
                    all_reg_numbers.append(reg_num)
                    reg_num_to_row[reg_num] = row
                pbar.update(1)

        self.stdout.write(f"  üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: {len(all_reg_numbers)}")

        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏
        self.stdout.write("  üîç –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î...")
        existing_objects = {}
        batch_size = 500

        with tqdm(total=len(all_reg_numbers), desc="     –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—á–∫–∞–º–∏", unit=" –∑–∞–ø") as pbar:
            for i in range(0, len(all_reg_numbers), batch_size):
                batch_numbers = all_reg_numbers[i:i+batch_size]
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_numbers,
                    ip_type=ip_type
                ).select_related('ip_type'):
                    existing_objects[obj.registration_number] = obj
                pbar.update(len(batch_numbers))

        self.stdout.write(f"  üìä –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(existing_objects)}")

        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏
        self.stdout.write("  üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        to_create = []
        to_update = []
        skipped_by_date = []
        unchanged_count = 0
        error_reg_numbers = []

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–∏—Å–∫–∏ –¥–ª—è —Å–≤—è–∑–µ–π
        authors_data = []
        holders_data = []
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        with tqdm(total=len(reg_num_to_row), desc="     –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–µ–π", unit=" –∑–∞–ø") as pbar:
            for reg_num, row in reg_num_to_row.items():
                try:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –¥–∞—Ç–µ
                    if not self.command.force and upload_date and reg_num in existing_objects:
                        existing = existing_objects[reg_num]
                        if existing.updated_at and existing.updated_at.date() >= upload_date:
                            skipped_by_date.append(reg_num)
                            pbar.update(1)
                            continue

                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                    name = self.clean_string(row.get('invention name'))
                    if name:
                        name = self.rid_formatter.format(name)
                    else:
                        name = f"–ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ ‚Ññ{reg_num}"

                    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã
                    application_date = self.parse_date(row.get('application date'))
                    registration_date = self.parse_date(row.get('registration date'))
                    patent_starting_date = self.parse_date(row.get('patent starting date'))
                    expiration_date = self.parse_date(row.get('expiration date'))
                    actual = self.parse_bool(row.get('actual'))
                    publication_url = self.clean_string(row.get('publication URL'))
                    abstract = self.clean_string(row.get('abstract'))
                    claims = self.clean_string(row.get('claims'))

                    creation_year = None
                    if application_date:
                        creation_year = application_date.year
                    elif registration_date:
                        creation_year = registration_date.year

                    obj_data = {
                        'registration_number': reg_num,
                        'ip_type': ip_type,
                        'name': name,
                        'application_date': application_date,
                        'registration_date': registration_date,
                        'patent_starting_date': patent_starting_date,
                        'expiration_date': expiration_date,
                        'actual': actual,
                        'publication_url': publication_url,
                        'abstract': abstract,
                        'claims': claims,
                        'creation_year': creation_year,
                    }

                    if reg_num in existing_objects:
                        if self._has_data_changed(existing_objects[reg_num], obj_data):
                            to_update.append(obj_data)
                        else:
                            unchanged_count += 1
                    else:
                        to_create.append(obj_data)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤—è–∑–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
                    authors_str = row.get('authors')
                    if not pd.isna(authors_str) and authors_str:
                        authors = self.parse_authors(authors_str)
                        for author in authors:
                            authors_data.append((reg_num, author))

                    holders_str = row.get('patent holders')
                    if not pd.isna(holders_str) and holders_str:
                        holders = self.parse_patent_holders(holders_str)
                        for holder in holders:
                            holders_data.append((reg_num, holder))

                except Exception as e:
                    error_reg_numbers.append(reg_num)
                    self.stdout.write(self.style.ERROR(f"\n  ‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    logger.error(f"Error preparing invention {reg_num}: {e}", exc_info=True)

                pbar.update(1)
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ (–∫–∞–∂–¥—ã–µ 10000 –∑–∞–ø–∏—Å–µ–π)
                if pbar.n % 10000 == 0:
                    gc.collect()

        stats['skipped_by_date'] = len(skipped_by_date)
        stats['skipped'] += len(skipped_by_date)
        stats['errors'] = len(error_reg_numbers)
        stats['unchanged'] = unchanged_count

        self.stdout.write(f"     –ò—Ç–æ–≥–æ: –Ω–æ–≤—ã—Ö={len(to_create)}, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ={len(to_update)}, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={unchanged_count}")

        # –®–ê–ì 4: –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
        if to_create and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –°–æ–∑–¥–∞–Ω–∏–µ {len(to_create)} –∑–∞–ø–∏—Å–µ–π...")
            created_count = self._bulk_create_objects(to_create)
            stats['created'] = created_count

        # –®–ê–ì 5: –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        if to_update and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {len(to_update)} –∑–∞–ø–∏—Å–µ–π...")
            updated_count = self._bulk_update_objects(to_update, existing_objects)
            stats['updated'] = updated_count

        # –®–ê–ì 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤
        if authors_data and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ ({len(set(r for r, _ in authors_data))} –∑–∞–ø–∏—Å–µ–π)...")
            self._process_authors_optimized(existing_objects, authors_data)

        # –®–ê–ì 7: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
        if holders_data and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π ({len(set(r for r, _ in holders_data))} –∑–∞–ø–∏—Å–µ–π)...")
            self._process_holders_optimized(existing_objects, holders_data)

        stats['processed'] = len(df) - stats['skipped'] - stats['errors']
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –≤ –∫–æ–Ω—Ü–µ
        gc.collect()

        self.stdout.write(self.style.SUCCESS(f"  ‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"     –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats['unchanged']}, "
                         f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –≤—Å–µ–≥–æ: {stats['skipped']} (–∏–∑ –Ω–∏—Ö –ø–æ –¥–∞—Ç–µ: {stats['skipped_by_date']}), "
                         f"–û—à–∏–±–æ–∫: {stats['errors']}")

        return stats

    def _bulk_create_objects(self, to_create):
        """–ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–∞–º—è—Ç–∏"""
        created_count = 0
        batch_size = 1000
        
        with tqdm(total=len(to_create), desc="     –°–æ–∑–¥–∞–Ω–∏–µ", unit=" –∑–∞–ø") as pbar:
            for i in range(0, len(to_create), batch_size):
                batch_data = to_create[i:i+batch_size]
                create_objects = [IPObject(**data) for data in batch_data]
                IPObject.objects.bulk_create(create_objects, batch_size=batch_size)
                created_count += len(batch_data)
                pbar.update(len(batch_data))
                
                # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
                if i % 10000 == 0:
                    gc.collect()
        
        return created_count

    def _bulk_update_objects(self, to_update, existing_objects):
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤"""
        updated_count = 0
        BATCH_UPDATE_SIZE = 500
        
        with tqdm(total=len(to_update), desc="     –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ", unit=" –∑–∞–ø") as pbar:
            for i in range(0, len(to_update), BATCH_UPDATE_SIZE):
                batch_data = to_update[i:i+BATCH_UPDATE_SIZE]
                with transaction.atomic():
                    for data in batch_data:
                        obj = existing_objects[data['registration_number']]
                        update_fields = []
                        for field, value in data.items():
                            if field != 'registration_number' and getattr(obj, field) != value:
                                setattr(obj, field, value)
                                update_fields.append(field)
                        if update_fields:
                            obj.save(update_fields=update_fields)
                            updated_count += 1
                pbar.update(len(batch_data))
                
                # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
                if i % 5000 == 0:
                    gc.collect()
        
        return updated_count

    def _process_authors_optimized(self, existing_objects, authors_data):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º Natasha"""
        self.stdout.write(f"     ‚ö° –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)...")
        
        # –®–ê–ì 1: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∞–≤—Ç–æ—Ä–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ª–æ–≤–∞—Ä–µ–π
        self.stdout.write("        –®–∞–≥ 1/6: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ –≤–º–µ—Å—Ç–æ defaultdict –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        author_to_ips = {}
        author_details = {}
        
        with tqdm(total=len(authors_data), desc="           –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞", unit=" –∑–∞–ø") as pbar:
            for reg_num, author in authors_data:
                ip_object = existing_objects.get(reg_num)
                if not ip_object:
                    pbar.update(1)
                    continue
                
                # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á
                key = f"{author['last_name']}|{author['first_name']}|{author['middle_name']}"
                
                # –î–æ–±–∞–≤–ª—è–µ–º IP –∫ –∞–≤—Ç–æ—Ä—É
                if key not in author_to_ips:
                    author_to_ips[key] = set()
                    author_details[key] = author
                author_to_ips[key].add(ip_object.pk)
                
                pbar.update(1)
        
        all_keys = list(author_to_ips.keys())
        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤: {len(all_keys)}")
        
        # –®–ê–ì 2: –ü–æ–∏—Å–∫ –≤ –ë–î (–ø–∞—á–∫–∞–º–∏)
        self.stdout.write("        –®–∞–≥ 2/6: –ü–æ–∏—Å–∫ –≤ –ë–î...")
        existing_people = {}
        BATCH_SIZE = 100
        
        with tqdm(total=len(all_keys), desc="           –ü–æ–∏—Å–∫", unit=" –∫–ª—é—á") as pbar:
            for i in range(0, len(all_keys), BATCH_SIZE):
                batch_keys = all_keys[i:i+BATCH_SIZE]
                
                # –°—Ç—Ä–æ–∏–º —É—Å–ª–æ–≤–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
                name_conditions = Q()
                for key in batch_keys:
                    last, first, middle = key.split('|')
                    if middle:
                        name_conditions |= Q(
                            last_name=last,
                            first_name=first,
                            middle_name=middle
                        )
                    else:
                        name_conditions |= Q(
                            last_name=last,
                            first_name=first,
                            middle_name__isnull=True
                        ) | Q(
                            last_name=last,
                            first_name=first,
                            middle_name=''
                        )
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
                for person in Person.objects.filter(name_conditions).only(
                    'id', 'last_name', 'first_name', 'middle_name'
                ):
                    key = f"{person.last_name}|{person.first_name}|{person.middle_name or ''}"
                    existing_people[key] = person
                    self.person_cache[key] = person
                
                pbar.update(len(batch_keys))
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞
                if i % 1000 == 0:
                    gc.collect()
        
        self.stdout.write(f"        –ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {len(existing_people)}")
        
        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
        self.stdout.write("        –®–∞–≥ 3/6: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
        new_keys = [key for key in all_keys if key not in existing_people]
        
        if new_keys:
            # –ü–æ–ª—É—á–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π ID –∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ slugs
            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
            existing_slugs = set(Person.objects.values_list('slug', flat=True)[:100000])
            
            people_to_create = []
            key_to_new_person = {}
            
            with tqdm(total=len(new_keys), desc="           –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞", unit=" –∫–ª—é—á") as pbar:
                for key in new_keys:
                    author = author_details[key]
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è –¥–ª—è slug
                    name_parts = [author['last_name'], author['first_name']]
                    if author['middle_name']:
                        name_parts.append(author['middle_name'])
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π slug
                    base_slug = slugify(' '.join(name_parts).strip())
                    if not base_slug:
                        base_slug = 'person'
                    
                    unique_slug = base_slug
                    counter = 1
                    while unique_slug in existing_slugs:
                        unique_slug = f"{base_slug}-{counter}"
                        counter += 1
                    existing_slugs.add(unique_slug)
                    
                    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Person
                    person = Person(
                        ceo_id=max_id + len(people_to_create) + 1,
                        ceo=author['full_name'],
                        last_name=author['last_name'],
                        first_name=author['first_name'],
                        middle_name=author['middle_name'] or '',
                        slug=unique_slug
                    )
                    people_to_create.append(person)
                    key_to_new_person[key] = person
                    
                    pbar.update(1)
                    
                    # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                    if len(people_to_create) >= 500:
                        Person.objects.bulk_create(people_to_create, batch_size=500)
                        people_to_create = []
                        gc.collect()
            
            # –°–æ–∑–¥–∞–µ–º –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è
            if people_to_create:
                Person.objects.bulk_create(people_to_create, batch_size=500)
            
            self.stdout.write(f"        –°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤: {len(new_keys)}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à
            for person in Person.objects.filter(ceo__in=[author_details[key]['full_name'] for key in new_keys]):
                key = f"{person.last_name}|{person.first_name}|{person.middle_name or ''}"
                existing_people[key] = person
                self.person_cache[key] = person
        
        # –®–ê–ì 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π
        self.stdout.write("        –®–∞–≥ 4/6: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π...")
        
        through_objs = []
        
        with tqdm(total=len(all_keys), desc="           –°–±–æ—Ä —Å–≤—è–∑–µ–π", unit=" –∞–≤—Ç–æ—Ä") as pbar:
            for key in all_keys:
                person = existing_people.get(key)
                if not person:
                    pbar.update(1)
                    continue
                
                for ip_id in author_to_ips[key]:
                    through_objs.append(
                        IPObject.authors.through(
                            ipobject_id=ip_id,
                            person_id=person.pk
                        )
                    )
                pbar.update(1)
        
        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {len(through_objs)}")
        
        # –®–ê–ì 5: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        if through_objs:
            self.stdout.write("        –®–∞–≥ 5/6: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π...")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID IP-–æ–±—ä–µ–∫—Ç–æ–≤
            ip_ids = list(set(obj.ipobject_id for obj in through_objs))
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ —Å –ü–†–û–ì–†–ï–°–°-–ë–ê–†–û–ú
            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π –¥–ª—è {len(ip_ids)} IP-–æ–±—ä–µ–∫—Ç–æ–≤...")
            delete_batch_size = 500
            deleted_total = 0
            
            with tqdm(total=len(ip_ids), desc="           –£–¥–∞–ª–µ–Ω–∏–µ", unit=" ip") as pbar:
                for i in range(0, len(ip_ids), delete_batch_size):
                    batch_ip_ids = ip_ids[i:i+delete_batch_size]
                    deleted, _ = IPObject.authors.through.objects.filter(
                        ipobject_id__in=batch_ip_ids
                    ).delete()
                    deleted_total += deleted
                    pbar.update(len(batch_ip_ids))
            
            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π: {deleted_total}")
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏ –ø–∞—á–∫–∞–º–∏
            create_batch_size = 2000
            created_count = 0
            
            with tqdm(total=len(through_objs), desc="           –î–æ–±–∞–≤–ª–µ–Ω–∏–µ", unit=" —Å–≤—è–∑—å") as pbar:
                for i in range(0, len(through_objs), create_batch_size):
                    batch = through_objs[i:i+create_batch_size]
                    IPObject.authors.through.objects.bulk_create(
                        batch, 
                        batch_size=create_batch_size,
                        ignore_conflicts=True
                    )
                    created_count += len(batch)
                    pbar.update(len(batch))
            
            self.stdout.write(f"           –°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π: {created_count}")
        
        # –®–ê–ì 6: –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        self.stdout.write("        –®–∞–≥ 6/6: –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏...")
        gc.collect()
        
        self.stdout.write(f"        ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def _process_holders_optimized(self, existing_objects, holders_data):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º Natasha"""
        self.stdout.write(f"     ‚ö° –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)...")
        
        # –®–ê–ì 1: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
        self.stdout.write("        –®–∞–≥ 1/7: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞
        holder_to_ips = {}
        all_holders_set = set()
        
        with tqdm(total=len(holders_data), desc="           –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞", unit=" –∑–∞–ø") as pbar:
            for reg_num, holder in holders_data:
                ip_object = existing_objects.get(reg_num)
                if ip_object:
                    all_holders_set.add(holder)
                    if holder not in holder_to_ips:
                        holder_to_ips[holder] = set()
                    holder_to_ips[holder].add(ip_object.pk)
                pbar.update(1)
        
        all_holders = list(all_holders_set)
        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π: {len(all_holders)}")
        
        # –®–ê–ì 2: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
        self.stdout.write("        –®–∞–≥ 2/7: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤...")
        person_holders = []
        org_holders = []
        
        with tqdm(total=len(all_holders), desc="           –ê–Ω–∞–ª–∏–∑", unit=" –æ–±") as pbar:
            for holder in all_holders:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º type_detector —Å Natasha
                if self.type_detector.detect_type(holder) == 'person':
                    person_holders.append(holder)
                else:
                    org_holders.append(holder)
                pbar.update(1)
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞
                if pbar.n % 1000 == 0:
                    gc.collect()
        
        self.stdout.write(f"        –õ—é–¥–∏: {len(person_holders)}, –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: {len(org_holders)}")
        
        # –®–ê–ì 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–ø–∞—á–∫–∞–º–∏)
        self.stdout.write("        –®–∞–≥ 3/7: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π...")
        org_map = self._batch_process_organizations_with_progress(org_holders)
        
        # –®–ê–ì 4: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª—é–¥–µ–π (–ø–∞—á–∫–∞–º–∏)
        self.stdout.write("        –®–∞–≥ 4/7: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª—é–¥–µ–π...")
        person_map = self._batch_process_persons_with_progress(person_holders)
        
        # –®–ê–ì 5: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π
        self.stdout.write("        –®–∞–≥ 5/7: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π...")
        
        org_relations = []
        person_relations = []
        
        with tqdm(total=len(all_holders), desc="           –°–±–æ—Ä —Å–≤—è–∑–µ–π", unit=" –æ–±–ª") as pbar:
            for holder in all_holders:
                ip_ids = holder_to_ips.get(holder, set())
                if holder in org_map:
                    org_id = org_map[holder].pk
                    for ip_id in ip_ids:
                        org_relations.append((ip_id, org_id))
                elif holder in person_map:
                    person_id = person_map[holder].pk
                    for ip_id in ip_ids:
                        person_relations.append((ip_id, person_id))
                pbar.update(1)
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞
                if len(org_relations) + len(person_relations) > 100000:
                    gc.collect()
        
        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏: {len(org_relations)}")
        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏: {len(person_relations)}")
        
        # –®–ê–ì 6: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏
        if org_relations:
            self.stdout.write("        –®–∞–≥ 6/7: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏...")
            self._create_org_relations_with_progress(org_relations)
        
        # –®–ê–ì 7: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏
        if person_relations:
            self.stdout.write("        –®–∞–≥ 7/7: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏...")
            self._create_person_relations_with_progress(person_relations)
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        gc.collect()
        
        self.stdout.write(f"        ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def _batch_process_organizations_with_progress(self, org_holders):
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
        if not org_holders:
            return {}
        
        org_map = {}
        CHUNK_SIZE = 1000
        total_orgs = len(org_holders)
        
        self.stdout.write(f"        –û–±—Ä–∞–±–æ—Ç–∫–∞ {total_orgs} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —á–∞—Å—Ç—è–º–∏ –ø–æ {CHUNK_SIZE}...")
        
        for chunk_start in range(0, total_orgs, CHUNK_SIZE):
            chunk_end = min(chunk_start + CHUNK_SIZE, total_orgs)
            chunk_holders = org_holders[chunk_start:chunk_end]
            
            # –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤ —ç—Ç–æ–π —á–∞—Å—Ç–∏
            existing_orgs = {}
            for org in Organization.objects.filter(name__in=chunk_holders).only('id', 'name'):
                existing_orgs[org.name] = org
                self.organization_cache[org.name] = org
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤ —ç—Ç–æ–π —á–∞—Å—Ç–∏
            orgs_to_create = []
            for holder in chunk_holders:
                if holder not in existing_orgs and holder not in self.organization_cache:
                    max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
                    new_id = max_id + len(orgs_to_create) + 1
                    
                    base_slug = slugify(holder[:50])
                    if not base_slug:
                        base_slug = 'organization'
                    
                    unique_slug = base_slug
                    counter = 1
                    while Organization.objects.filter(slug=unique_slug).exists() or any(o.slug == unique_slug for o in orgs_to_create):
                        unique_slug = f"{base_slug}-{counter}"
                        counter += 1
                    
                    org = Organization(
                        organization_id=new_id,
                        name=holder,
                        full_name=holder,
                        short_name=holder[:500] if len(holder) > 500 else holder,
                        slug=unique_slug,
                        register_opk=False,
                        strategic=False,
                    )
                    orgs_to_create.append(org)
                    self.organization_cache[holder] = org
            
            # –°–æ–∑–¥–∞–µ–º –≤ –ë–î
            if orgs_to_create:
                batch_size = 500
                for i in range(0, len(orgs_to_create), batch_size):
                    batch = orgs_to_create[i:i+batch_size]
                    Organization.objects.bulk_create(batch, batch_size=batch_size)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥
            for holder in chunk_holders:
                if holder in existing_orgs:
                    org_map[holder] = existing_orgs[holder]
                elif holder in self.organization_cache:
                    org_map[holder] = self.organization_cache[holder]
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å
            progress = (chunk_end / total_orgs) * 100
            self.stdout.write(f"           –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%")
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            del existing_orgs
            del orgs_to_create
            gc.collect()
        
        return org_map

    def _batch_process_persons_with_progress(self, person_holders):
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ª—é–¥–µ–π —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
        if not person_holders:
            return {}
        
        person_map = {}
        CHUNK_SIZE = 500
        total_people = len(person_holders)
        
        self.stdout.write(f"        –û–±—Ä–∞–±–æ—Ç–∫–∞ {total_people} –ª—é–¥–µ–π —á–∞—Å—Ç—è–º–∏ –ø–æ {CHUNK_SIZE}...")
        
        for chunk_start in range(0, total_people, CHUNK_SIZE):
            chunk_end = min(chunk_start + CHUNK_SIZE, total_people)
            chunk_holders = person_holders[chunk_start:chunk_end]
            
            # –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
            existing_people = {}
            for holder in chunk_holders:
                parts = holder.split()
                if len(parts) >= 2:
                    last_name = parts[0]
                    first_name = parts[1]
                    middle_name = parts[2] if len(parts) > 2 else ''
                    
                    persons = Person.objects.filter(
                        last_name=last_name,
                        first_name=first_name
                    )
                    if middle_name:
                        persons = persons.filter(middle_name=middle_name)
                    
                    person = persons.first()
                    if person:
                        existing_people[holder] = person
                        self.person_cache[holder] = person
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö
            people_to_create = []
            for holder in chunk_holders:
                if holder not in existing_people and holder not in self.person_cache:
                    parts = holder.split()
                    if len(parts) >= 2:
                        last_name = parts[0]
                        first_name = parts[1]
                        middle_name = parts[2] if len(parts) > 2 else ''
                        
                        name_parts = [last_name, first_name]
                        if middle_name:
                            name_parts.append(middle_name)
                        
                        base_slug = slugify(' '.join(name_parts))
                        if not base_slug:
                            base_slug = 'person'
                        
                        unique_slug = base_slug
                        counter = 1
                        while Person.objects.filter(slug=unique_slug).exists() or any(p.slug == unique_slug for p in people_to_create):
                            unique_slug = f"{base_slug}-{counter}"
                            counter += 1
                        
                        max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
                        new_id = max_id + len(people_to_create) + 1
                        
                        person = Person(
                            ceo_id=new_id,
                            ceo=holder,
                            last_name=last_name,
                            first_name=first_name,
                            middle_name=middle_name or '',
                            slug=unique_slug
                        )
                        people_to_create.append(person)
                        self.person_cache[holder] = person
            
            # –°–æ–∑–¥–∞–µ–º –≤ –ë–î
            if people_to_create:
                batch_size = 500
                for i in range(0, len(people_to_create), batch_size):
                    batch = people_to_create[i:i+batch_size]
                    Person.objects.bulk_create(batch, batch_size=batch_size)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥
            for holder in chunk_holders:
                if holder in existing_people:
                    person_map[holder] = existing_people[holder]
                elif holder in self.person_cache:
                    person_map[holder] = self.person_cache[holder]
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å
            progress = (chunk_end / total_people) * 100
            self.stdout.write(f"           –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%")
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            del existing_people
            del people_to_create
            gc.collect()
        
        return person_map

    def _create_org_relations_with_progress(self, org_relations):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
        if not org_relations:
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ IP ID
        ip_ids = list(set(ip_id for ip_id, _ in org_relations))
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        delete_batch_size = 500
        deleted_total = 0
        
        with tqdm(total=len(ip_ids), desc="           –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π", unit=" ip") as pbar:
            for i in range(0, len(ip_ids), delete_batch_size):
                batch_ip_ids = ip_ids[i:i+delete_batch_size]
                deleted, _ = IPObject.owner_organizations.through.objects.filter(
                    ipobject_id__in=batch_ip_ids
                ).delete()
                deleted_total += deleted
                pbar.update(len(batch_ip_ids))
        
        self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π: {deleted_total}")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏
        through_objs = [
            IPObject.owner_organizations.through(
                ipobject_id=ip_id,
                organization_id=org_id
            )
            for ip_id, org_id in org_relations
        ]
        
        create_batch_size = 2000
        with tqdm(total=len(through_objs), desc="           –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π", unit=" —Å–≤") as pbar:
            for i in range(0, len(through_objs), create_batch_size):
                batch = through_objs[i:i+create_batch_size]
                IPObject.owner_organizations.through.objects.bulk_create(
                    batch, 
                    batch_size=create_batch_size,
                    ignore_conflicts=True
                )
                pbar.update(len(batch))

    def _create_person_relations_with_progress(self, person_relations):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
        if not person_relations:
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ IP ID
        ip_ids = list(set(ip_id for ip_id, _ in person_relations))
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        delete_batch_size = 500
        deleted_total = 0
        
        with tqdm(total=len(ip_ids), desc="           –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π", unit=" ip") as pbar:
            for i in range(0, len(ip_ids), delete_batch_size):
                batch_ip_ids = ip_ids[i:i+delete_batch_size]
                deleted, _ = IPObject.owner_persons.through.objects.filter(
                    ipobject_id__in=batch_ip_ids
                ).delete()
                deleted_total += deleted
                pbar.update(len(batch_ip_ids))
        
        self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π: {deleted_total}")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏
        through_objs = [
            IPObject.owner_persons.through(
                ipobject_id=ip_id,
                person_id=person_id
            )
            for ip_id, person_id in person_relations
        ]
        
        create_batch_size = 2000
        with tqdm(total=len(through_objs), desc="           –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π", unit=" —Å–≤") as pbar:
            for i in range(0, len(through_objs), create_batch_size):
                batch = through_objs[i:i+create_batch_size]
                IPObject.owner_persons.through.objects.bulk_create(
                    batch, 
                    batch_size=create_batch_size,
                    ignore_conflicts=True
                )
                pbar.update(len(batch))