"""
–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ —Ç–∏–ø—ã –†–ò–î: –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è, –ø–æ–ª–µ–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏, –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã,
—Ç–æ–ø–æ–ª–æ–≥–∏–∏ –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º, –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –≠–í–ú –∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.
"""

import logging
import re
from datetime import datetime
from typing import List, Dict, Any

from django.db import models
from django.core.management.base import BaseCommand, CommandError
from django.utils.text import slugify
from django.utils import timezone
from tqdm import tqdm
import pandas as pd
import os
import pymorphy2
from pymorphy2 import MorphAnalyzer

from intellectual_property.models import (
    FipsOpenDataCatalogue, IPType, IPObject
)
from core.models import (
    Person, Organization, 
    FOIV, Country, RFRepresentative,
    OrganizationNormalizationRule
)

logger = logging.getLogger(__name__)


class MorphAnalyzerWrapper:
    """
    –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è pymorphy2 —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    _instance = None
    _cache = {}
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.morph = pymorphy2.MorphAnalyzer()
        return cls._instance
    
    def parse(self, word: str):
        """–†–∞–∑–±–æ—Ä —Å–ª–æ–≤–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if word in self._cache:
            return self._cache[word]
        result = self.morph.parse(word)
        self._cache[word] = result
        return result
    
    def is_name(self, word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –∏–º–µ–Ω–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º"""
        if not word:
            return False
        parsed = self.parse(word)[0]
        tags = str(parsed.tag)
        return any(tag in tags for tag in ['Name', 'Surn', 'Patr'])
    
    def is_preposition(self, word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –ø—Ä–µ–¥–ª–æ–≥–æ–º/—Å–æ—é–∑–æ–º"""
        if not word:
            return False
        parsed = self.parse(word)[0]
        return 'PREP' in str(parsed.tag) or 'CONJ' in str(parsed.tag)
    
    def normalize_case(self, word: str, is_first: bool = False) -> str:
        """
        –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å–ª–æ–≤–∞ –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        """
        if not word or len(word) <= 1:
            return word
        
        # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –∏ –¥–ª–∏–Ω–Ω–æ–µ - –≤–æ–∑–º–æ–∂–Ω–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞
        if word.isupper() and len(word) > 1:
            return word
        
        # –ï—Å–ª–∏ —Å–ª–æ–≤–æ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –±—É–∫–≤—É (–∏–Ω–∏—Ü–∏–∞–ª)
        if len(word) == 1:
            return word.upper() + '.'
        
        # –û—á–∏—â–∞–µ–º –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        clean_word = re.sub(r'[.,;:!?()\[\]{}"\']', '', word)
        if not clean_word:
            return word
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é
        parsed = self.parse(clean_word)[0]
        tags = str(parsed.tag)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–≥–∏—Å—Ç—Ä
        if is_first:
            # –ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ - —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã
            result = clean_word[0].upper() + clean_word[1:].lower()
        elif 'Name' in tags or 'Surn' in tags or 'Patr' in tags:
            # –ò–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ - —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã
            result = clean_word[0].upper() + clean_word[1:].lower()
        elif 'PREP' in tags or 'CONJ' in tags:
            # –ü—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–æ—é–∑—ã - —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã
            result = clean_word.lower()
        else:
            # –û–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞ - —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã
            result = clean_word.lower()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        if word != clean_word:
            punctuation = word[len(clean_word):]
            result += punctuation
        
        return result


class OrganizationNormalizer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–∞–≤–∏–ª –∏–∑ –ë–î
    """
    
    def __init__(self):
        self.rules_cache = None
        self.morph = MorphAnalyzerWrapper()
        self.load_rules()
    
    def load_rules(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∏–∑ –ë–î –≤ –∫—ç—à"""
        rules = OrganizationNormalizationRule.objects.all().order_by('priority')
        self.rules_cache = [
            {
                'original': rule.original_text.lower(),
                'replacement': rule.replacement_text.lower(),
                'type': rule.rule_type,
                'priority': rule.priority
            }
            for rule in rules
        ]
    
    def reload_rules(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª (–ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ë–î)"""
        self.load_rules()
    
    def normalize(self, name: str) -> Dict[str, Any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–∞–≤–∏–ª –∏–∑ –ë–î
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        """
        if pd.isna(name) or not name:
            return {'normalized': '', 'keywords': [], 'original': name}
        
        original = str(name).strip()
        name_lower = original.lower()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ –ë–î
        normalized = name_lower
        if self.rules_cache:
            for rule in self.rules_cache:
                if rule['type'] == 'ignore':
                    # –î–ª—è –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã—Ö —Å–ª–æ–≤ –ø—Ä–æ—Å—Ç–æ —É–¥–∞–ª—è–µ–º –∏—Ö
                    pattern = r'\b' + re.escape(rule['original']) + r'\b'
                    normalized = re.sub(pattern, '', normalized)
                else:
                    # –î–ª—è –∑–∞–º–µ–Ω
                    pattern = r'\b' + re.escape(rule['original']) + r'\b'
                    normalized = re.sub(pattern, rule['replacement'], normalized)
        
        # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –≤—Å–µ—Ö –≤–∏–¥–æ–≤
        normalized = re.sub(r'["\'¬´¬ª‚Äû‚Äú‚Äù]', '', normalized)
        
        # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –∫—Ä–æ–º–µ –¥–µ—Ñ–∏—Å–∞
        normalized = re.sub(r'[^\w\s-]', ' ', normalized)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        normalized = ' '.join(normalized.split())
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        keywords = self._extract_keywords(original)
        
        return {
            'normalized': normalized,
            'keywords': keywords,
            'original': original
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        keywords = []
        
        # –°–ª–æ–≤–∞ –≤ –∫–∞–≤—ã—á–∫–∞—Ö (–æ–±—ã—á–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ)
        quoted = re.findall(r'"([^"]+)"', text)
        for q in quoted:
            words = q.lower().split()
            keywords.extend([w for w in words if len(w) > 3])
        
        # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤)
        abbreviations = re.findall(r'\b[–ê-–Ø–ÅA-Z]{2,}\b', text)
        keywords.extend([a.lower() for a in abbreviations if len(a) >= 2])
        
        return list(set(keywords))
    
    def format_organization_name(self, name: str) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–µ–≥–∏—Å—Ç—Ä–æ–º
        """
        if not name:
            return name
        
        # –°–ø–∏—Å–æ–∫ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å–µ–≥–¥–∞ –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
        ALWAYS_UPPER = {
            '–û–û–û', '–ó–ê–û', '–û–ê–û', '–ê–û', '–ü–ê–û', '–ù–ê–û',
            '–§–ì–£–ü', '–§–ì–ë–£', '–§–ì–ê–û–£', '–§–ì–ê–£', '–§–ì–ö–£',
            '–ù–ò–ò', '–ö–ë', '–û–ö–ë', '–°–ö–ë', '–¶–ö–ë', '–ü–ö–ë',
            '–ù–ü–û', '–ù–ü–ü', '–ù–ü–§', '–ù–ü–¶', '–ù–ò–¶',
            '–ú–£–ü', '–ì–£–ü', '–ò–ß–ü', '–¢–û–û', '–ê–û–ó–¢', '–ê–û–û–¢',
            '–†–§', '–†–ê–ù', '–°–û –†–ê–ù', '–£—Ä–û –†–ê–ù', '–î–í–û –†–ê–ù',
            '–ú–ì–£', '–°–ü–±–ì–£', '–ú–§–¢–ò', '–ú–ò–§–ò', '–ú–ì–¢–£', '–ú–ê–ò',
            '–§–ò–ê–ù', '–ú–ò–ê–ù', '–ò–ü–ú', '–ò–ü–ú–µ—Ö', '–ò–ü–ü–ò',
            '–¶–ê–ì–ò', '–¶–ò–ê–ú', '–í–ò–ê–ú', '–í–ò–õ–°', '–í–ò–ú–°', '–í–ù–ò–ò'
        }
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ –∫–∞–≤—ã—á–∫–∞–º
        parts = re.split(r'(")', name)
        result = []
        in_quotes = False
        
        for part in parts:
            if part == '"':
                in_quotes = not in_quotes
                result.append(part)
            elif in_quotes:
                # –ß–∞—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ –∫–∞–≤—ã—á–µ–∫ - —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                words = part.split()
                formatted_words = []
                for word in words:
                    word_upper = word.upper()
                    if word_upper in ALWAYS_UPPER:
                        formatted_words.append(word_upper)
                    elif self.morph.is_name(word):
                        # –ò–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ - —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã
                        formatted_words.append(word[0].upper() + word[1:].lower())
                    else:
                        # –û–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞ - —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã
                        formatted_words.append(word.lower())
                result.append(' '.join(formatted_words))
            else:
                # –ß–∞—Å—Ç—å –≤–Ω–µ –∫–∞–≤—ã—á–µ–∫ - —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∏ –æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞
                words = part.split()
                formatted_words = []
                for word in words:
                    word_clean = word.strip('.,;:()')
                    word_upper = word_clean.upper()
                    if word_upper in ALWAYS_UPPER:
                        formatted_words.append(word_upper)
                    elif word_clean.isupper() and len(word_clean) > 1:
                        # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                        formatted_words.append(word_clean)
                    else:
                        # –û–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞ - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
                        formatted_words.append(word)
                result.append(' '.join(formatted_words))
        
        return ''.join(result)


class EntityTypeDetector:
    """
    –î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pymorphy2
    """
    
    def __init__(self):
        self.morph = MorphAnalyzerWrapper()
    
    def is_person(self, text: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º –ª–∏—Ü–æ–º
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–º–µ–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö
        """
        if not text or len(text) < 6:
            return False
        
        text = text.strip()
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ - —Å—Ä–∞–∑—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º False
        org_indicators = ['–û–û–û', '–ó–ê–û', '–ê–û', '–ü–ê–û', '–§–ì–£–ü', '–§–ì–ë–£', 
                         '–û–±—â–µ—Å—Ç–≤–æ', '–ö–æ–º–ø–∞–Ω–∏—è', '–ö–æ—Ä–ø–æ—Ä–∞—Ü–∏—è', '–ó–∞–≤–æ–¥', '–ò–Ω—Å—Ç–∏—Ç—É—Ç']
        if any(ind in text for ind in org_indicators):
            return False
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = re.findall(r'[–ê-–Ø–ÅA-Z][–∞-—è—ëa-z]*\.?', text)
        
        # –ï—Å–ª–∏ —Å–ª–æ–≤ –º–µ–Ω—å—à–µ 2 - –Ω–µ –§–ò–û
        if len(words) < 2:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –Ω–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –∏–º–µ–Ω–∞–º
        name_count = 0
        for word in words:
            clean_word = word.rstrip('.')
            if self.morph.is_name(clean_word):
                name_count += 1
        
        # –ï—Å–ª–∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–ª–æ–≤ - –∏–º–µ–Ω–∞, –≤–µ—Ä–æ—è—Ç–Ω–æ —ç—Ç–æ –§–ò–û
        return name_count >= len(words) - 1
    
    def detect_type(self, text: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 'person' –∏–ª–∏ 'organization'
        """
        if self.is_person(text):
            return 'person'
        return 'organization'


class RIDNameFormatter:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –†–ò–î —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pymorphy2
    """
    
    def __init__(self):
        self.morph = MorphAnalyzerWrapper()
        
        # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å–µ–≥–¥–∞ –æ—Å—Ç–∞—é—Ç—Å—è –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
        self.KEEP_UPPER = {
            # –•–∏–º–∏—è –∏ –±–∏–æ–ª–æ–≥–∏—è
            '–î–ù–ö', '–†–ù–ö', '–ü–¶–†', '–ò–§–ê', '–≠–î–¢–ê', '–ê–¢–§', '–ê–î–§', '–ù–ê–î', '–ù–ê–î–§',
            'COVID-19', 'SARS-COV-2', '–í–ò–ß', '–°–ü–ò–î',
            
            # –ï–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
            '¬∞C', '¬∞F', 'K', '–ú', '–°–ú', '–ú–ú', '–ö–ú', '–ö–ì', '–ì', '–ú–ì', '–ú–ö–ì',
            '–õ', '–ú–õ', '–ú–ö–õ', '–°', '–ú–°', '–ú–ö–°', '–ú–ò–ù', '–ß', '–°–£–¢',
            '–ü–ê', '–ö–ü–ê', '–ú–ü–ê', '–ì–ü–ê', '–ê–¢–ú', '–ë–ê–†', '–ú–ú –†–¢. –°–¢.',
            '–ê', '–í', '–í–¢', '–ö–í–¢', '–ú–í–¢', '–ì–í–¢', '–û–ú', '–§', '–ì–ù', '–¢–õ',
            '–ë–ò–¢', '–ë–ê–ô–¢', '–ö–ë', '–ú–ë', '–ì–ë', '–¢–ë', '–ì–¶', '–ö–ì–¶', '–ú–ì–¶', '–ì–ì–¶',
            
            # –•–∏–º–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            'H', 'HE', 'LI', 'BE', 'B', 'C', 'N', 'O', 'F', 'NE', 'NA', 'MG',
            'AL', 'SI', 'P', 'S', 'CL', 'AR', 'K', 'CA', 'SC', 'TI', 'V',
            'CR', 'MN', 'FE', 'CO', 'NI', 'CU', 'ZN', 'GA', 'GE', 'AS', 'SE',
            'HCL', 'H2SO4', 'HNO3', 'H3PO4', 'NAOH', 'KOH', 'NH3', 'CO2',
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
            '–ì–û–°–¢', '–¢–£', '–°–ù–∏–ü', '–°–ü', '–°–∞–Ω–ü–∏–ù', 'ISO', 'IEC', 'IEEE',
            'USB', 'HDMI', 'WI-FI', 'LTE', '5G', 'CPU', 'GPU', 'RAM', 'ROM',
            'CAD', 'CAM', 'CAE', 'PLM', 'PDM', 'ERP', 'CRM', 'MES',
            
            # –ü–∞—Ç–µ–Ω—Ç–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            '–ú–ü–ö', '–ú–ö–¢–£', '–ú–ö–ü–û', '–ù–ò–û–ö–†', '–†–ò–î', '–ò–°', '–û–ò–°', '–§–ò–ü–°',
        }
        
        # –ü—Ä–µ–¥–ª–æ–≥–∏, —Å–æ—é–∑—ã, —á–∞—Å—Ç–∏—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å–µ–≥–¥–∞ —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã
        self.LOWERCASE_WORDS = {
            '–≤', '–Ω–∞', '—Å', '—Å–æ', '—É', '–∫', '–∫–æ', '–æ', '–æ–±', '–æ—Ç', '–¥–æ',
            '–¥–ª—è', '–±–µ–∑', '–Ω–∞–¥', '–ø–æ–¥', '–∏–∑', '–ø–æ', '–∑–∞', '–ø—Ä–æ', '—á–µ—Ä–µ–∑',
            '–∏', '–∞', '–Ω–æ', '–¥–∞', '–∏–ª–∏', '–ª–∏–±–æ', '–∂–µ', '–∫–∞–∫', '—Ç–∞–∫',
            '—á—Ç–æ', '—á—Ç–æ–±—ã', '–µ—Å–ª–∏', '—Ö–æ—Ç—è', '–ø—Ä–∏', '–≤–æ', '–æ–±–æ',
            'and', 'or', 'but', 'if', 'then', 'else', 'for', 'to', 'with',
            'by', 'from', 'at', 'in', 'on', 'of', 'the', 'a', 'an',
        }
    
    def format(self, text: str) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –†–ò–î —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–µ–≥–∏—Å—Ç—Ä–æ–º
        """
        if not text or not isinstance(text, str):
            return text
        
        if len(text.strip()) <= 1:
            return text
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'(?<=[.!?])\s+(?=[–ê-–Ø–ÅA-Z])', text)
        formatted_sentences = []
        
        for sentence in sentences:
            if not sentence.strip():
                continue
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–æ–±–µ–ª—ã
            words = re.split(r'(\s+)', sentence)
            formatted_words = []
            
            is_first_word = True
            
            for word in words:
                # –ü—Ä–æ–±–µ–ª—ã –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                if word.isspace():
                    formatted_words.append(word)
                    continue
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª–æ–≤–æ
                formatted_word = self._format_word(word, is_first_word)
                formatted_words.append(formatted_word)
                
                # –°–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –Ω–µ –±—É–¥–µ—Ç –ø–µ—Ä–≤—ã–º –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
                if not word.isspace():
                    is_first_word = False
            
            formatted_sentences.append(''.join(formatted_words))
        
        result = ' '.join(formatted_sentences)
        
        # –û—á–∏—Å—Ç–∫–∞
        result = re.sub(r'\s+([,;:.])', r'\1', result)
        result = ' '.join(result.split())
        
        return result
    
    def _format_word(self, word: str, is_first: bool) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É
        word_upper = word.upper().strip('.,;:!?()')
        if word_upper in self.KEEP_UPPER:
            return word_upper
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–Ω–∏—Ü–∏–∞–ª—ã
        if re.match(r'^[–ê-–Ø–ÅA-Z]\.$', word) or re.match(r'^[–ê-–Ø–ÅA-Z]\.[–ê-–Ø–ÅA-Z]\.$', word):
            return word.upper()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —á–∏—Å–ª–∞
        if word.isdigit():
            return word
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —á–∏—Å–ª–∞ —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è
        unit_match = re.match(r'^(\d+(?:[.,]\d+)?)([–∞-—è—ëa-z¬∞]+)$', word, re.IGNORECASE)
        if unit_match:
            number, unit = unit_match.groups()
            unit_upper = unit.upper()
            if unit_upper in self.KEEP_UPPER:
                return number + unit_upper
            return number + unit.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –¥–µ—Ñ–∏—Å
        if '-' in word:
            parts = word.split('-')
            formatted_parts = []
            for part in parts:
                formatted_parts.append(self._format_word(part, False))
            return '-'.join(formatted_parts)
        
        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        clean_word = re.sub(r'[.,;:!?()]', '', word)
        if not clean_word:
            return word
        
        # –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é
        word_lower = clean_word.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–æ—é–∑—ã
        if word_lower in self.LOWERCASE_WORDS or self.morph.is_preposition(clean_word):
            result = word_lower
        elif is_first:
            # –ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ - —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã
            result = clean_word[0].upper() + clean_word[1:].lower()
        elif self.morph.is_name(clean_word):
            # –ò–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ - —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã
            result = clean_word[0].upper() + clean_word[1:].lower()
        else:
            # –û–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞ - —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã
            result = clean_word.lower()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        if word != clean_word:
            punctuation = word[len(clean_word):]
            result += punctuation
        
        return result


class PersonNameFormatter:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–º–µ–Ω –ª—é–¥–µ–π
    """
    
    def __init__(self):
        self.morph = MorphAnalyzerWrapper()
    
    def format(self, name: str) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –§–ò–û —á–µ–ª–æ–≤–µ–∫–∞
        –§–û–ú–ò–ù –ê–†–¢–ï–ú –í–õ–ê–î–ò–ú–ò–†–û–í–ò–ß -> –§–æ–º–∏–Ω –ê—Ä—Ç–µ–º –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á
        """
        if not name:
            return name
        
        parts = name.split()
        formatted_parts = []
        
        for part in parts:
            if not part:
                continue
            
            # –ò–Ω–∏—Ü–∏–∞–ª—ã
            if '.' in part:
                initials = [p for p in part if p.isalpha()]
                formatted_parts.append(''.join([i.upper() + '.' for i in initials]))
                continue
            
            # –û–±—ã—á–Ω–æ–µ —Å–ª–æ–≤–æ
            clean_part = part.strip('.')
            if clean_part.isupper() and len(clean_part) > 1:
                # –í–µ—Ä–æ—è—Ç–Ω–æ, —Ñ–∞–º–∏–ª–∏—è –∏–ª–∏ –∏–º—è –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
                formatted_parts.append(clean_part[0].upper() + clean_part[1:].lower())
            else:
                # –£–∂–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –∏–ª–∏ —Å–º–µ—à–∞–Ω–Ω–æ–º
                formatted_parts.append(part)
        
        return ' '.join(formatted_parts)


class BaseFIPSParser:
    """
    –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –§–ò–ü–°.
    –°–æ–¥–µ—Ä–∂–∏—Ç –æ–±—â–∏–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏.
    """
    
    def __init__(self, command):
        self.command = command
        self.stdout = command.stdout
        self.style = command.style
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏ —Ñ–æ—Ä–º–∞—Ç–µ—Ä–æ–≤
        self.morph = MorphAnalyzerWrapper()
        self.org_normalizer = OrganizationNormalizer()
        self.type_detector = EntityTypeDetector()
        self.rid_formatter = RIDNameFormatter()
        self.person_formatter = PersonNameFormatter()
        
        # –ö—ç—à–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.country_cache = {}
        self.person_cache = {}
        self.organization_cache = {}
        self.foiv_cache = {}
        self.rf_rep_cache = {}
        self.city_cache = {}
        self.activity_type_cache = {}
        self.ceo_position_cache = {}
    
    def get_ip_type(self):
        """–î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –¥–æ—á–µ—Ä–Ω–∏—Ö –∫–ª–∞—Å—Å–∞—Ö"""
        raise NotImplementedError
    
    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –†–ò–î"""
        raise NotImplementedError
    
    def parse_dataframe(self, df, catalogue):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame"""
        raise NotImplementedError
    
    def clean_string(self, value):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if pd.isna(value) or value is None:
            return ''
        value = str(value).strip()
        if value in ['', 'None', 'null', 'NULL', 'nan']:
            return ''
        return value
    
    def parse_date(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        if pd.isna(value) or not value:
            return None
        
        date_str = str(value).strip()
        if not date_str:
            return None
        
        for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
            try:
                return datetime.strptime(date_str, fmt).date()
            except (ValueError, TypeError):
                continue
        
        try:
            return pd.to_datetime(date_str).date()
        except (ValueError, TypeError):
            return None
    
    def parse_bool(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –±—É–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if pd.isna(value) or not value:
            return False
        
        value = str(value).lower().strip()
        return value in ['1', 'true', 'yes', '–¥–∞', '–¥–µ–π—Å—Ç–≤—É–µ—Ç', 't', '1.0', '–∞–∫—Ç–∏–≤–µ–Ω']
    
    def get_or_create_country(self, code):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –ø–æ –∫–æ–¥—É –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏ Country
        """
        if not code or pd.isna(code):
            return None
        
        code = str(code).upper().strip()
        if len(code) != 2:
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if code in self.country_cache:
            return self.country_cache[code]
        
        try:
            # –ò—â–µ–º —Å—Ç—Ä–∞–Ω—É –ø–æ –¥–≤—É—Ö–±—É–∫–≤–µ–Ω–Ω–æ–º—É –∫–æ–¥—É
            country = Country.objects.filter(code=code).first()
            if country:
                self.country_cache[code] = country
                return country
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –¥–≤—É—Ö–±—É–∫–≤–µ–Ω–Ω–æ–º—É, –ø—Ä–æ–±—É–µ–º –ø–æ —Ç—Ä–µ—Ö–±—É–∫–≤–µ–Ω–Ω–æ–º—É
            country = Country.objects.filter(code_alpha3=code).first()
            if country:
                self.country_cache[code] = country
                return country
            
            self.stdout.write(self.style.WARNING(f"  –°—Ç—Ä–∞–Ω–∞ —Å –∫–æ–¥–æ–º {code} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ë–î"))
            return None
            
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω—ã {code}: {e}"))
            return None
    
    def parse_authors(self, authors_str):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –∞–≤—Ç–æ—Ä–∞–º–∏"""
        if pd.isna(authors_str) or not authors_str:
            return []
        
        authors_str = str(authors_str)
        authors_list = re.split(r'[\n,]\s*', authors_str)
        
        result = []
        for author in authors_list:
            author = author.strip()
            if not author or author == '""' or author == 'null':
                continue
            
            author = author.strip('"')
            author = re.sub(r'\s*\([A-Z]{2}\)', '', author)
            author = self.person_formatter.format(author)
            
            parts = author.split()
            
            if len(parts) >= 2:
                last_name = parts[0]
                first_name = parts[1] if len(parts) > 1 else ''
                middle_name = parts[2] if len(parts) > 2 else ''
                
                first_name_clean = first_name.replace('.', '')
                middle_name_clean = middle_name.replace('.', '')
                
                result.append({
                    'last_name': last_name,
                    'first_name': first_name_clean,
                    'middle_name': middle_name_clean,
                    'full_name': author,
                })
            else:
                result.append({
                    'last_name': author,
                    'first_name': '',
                    'middle_name': '',
                    'full_name': author,
                })
        
        return result
    
    def parse_patent_holders(self, holders_str):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—è–º–∏"""
        if pd.isna(holders_str) or not holders_str:
            return []
        
        holders_str = str(holders_str)
        holders_list = re.split(r'[\n]\s*', holders_str)
        
        result = []
        for holder in holders_list:
            holder = holder.strip().strip('"')
            if not holder or holder == 'null' or holder == 'None':
                continue
            
            holder = re.sub(r'\s*\([A-Z]{2}\)', '', holder)
            result.append(holder)
        
        return result
    
    def find_or_create_person(self, person_data):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞"""
        cache_key = f"{person_data['last_name']}|{person_data['first_name']}|{person_data['middle_name']}"
        
        if cache_key in self.person_cache:
            return self.person_cache[cache_key]
        
        persons = Person.objects.filter(
            last_name=person_data['last_name'],
            first_name=person_data['first_name']
        )
        
        if person_data['middle_name']:
            persons = persons.filter(middle_name=person_data['middle_name'])
        
        if persons.exists():
            person = persons.first()
            self.person_cache[cache_key] = person
            return person
        
        try:
            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
            new_id = max_id + 1
            
            if 'full_name' in person_data:
                full_name = person_data['full_name']
            else:
                full_name_parts = [person_data['last_name'], person_data['first_name']]
                if person_data['middle_name']:
                    full_name_parts.append(person_data['middle_name'])
                full_name = ' '.join(full_name_parts)
                full_name = self.person_formatter.format(full_name)
            
            person = Person.objects.create(
                ceo_id=new_id,
                ceo=full_name,
                last_name=person_data['last_name'],
                first_name=person_data['first_name'],
                middle_name=person_data['middle_name']
            )
            self.person_cache[cache_key] = person
            return person
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Person: {e}"))
            return None
    
    def find_or_create_person_from_name(self, full_name):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞ –ø–æ –ø–æ–ª–Ω–æ–º—É –∏–º–µ–Ω–∏"""
        if pd.isna(full_name) or not full_name:
            return None
        
        full_name = str(full_name).strip().strip('"')
        full_name = self.person_formatter.format(full_name)
        
        if full_name in self.person_cache:
            return self.person_cache[full_name]
        
        parts = full_name.split()
        
        if len(parts) >= 2:
            last_name = parts[0]
            first_name = parts[1] if len(parts) > 1 else ''
            middle_name = parts[2] if len(parts) > 2 else ''
            
            first_name_clean = first_name.replace('.', '')
            middle_name_clean = middle_name.replace('.', '')
            
            person_data = {
                'last_name': last_name,
                'first_name': first_name_clean,
                'middle_name': middle_name_clean,
                'full_name': full_name,
            }
        else:
            person_data = {
                'last_name': full_name,
                'first_name': '',
                'middle_name': '',
                'full_name': full_name,
            }
        
        return self.find_or_create_person(person_data)
    
    def find_similar_organization(self, org_name):
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö —Ç—Ä–µ—Ö –ø–æ–ª–µ–π –Ω–∞–∑–≤–∞–Ω–∏—è
        """
        if pd.isna(org_name) or not org_name:
            return None
        
        org_name = str(org_name).strip().strip('"')
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –ª—é–±—ã–º –∏–∑ –ø–æ–ª–µ–π –Ω–∞–∑–≤–∞–Ω–∏—è
        direct_match = Organization.objects.filter(
            models.Q(name=org_name) |
            models.Q(full_name=org_name) |
            models.Q(short_name=org_name)
        ).first()
        if direct_match:
            return direct_match
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        norm_data = self.org_normalizer.normalize(org_name)
        normalized = norm_data['normalized']
        keywords = norm_data['keywords']
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤–æ –≤—Å–µ—Ö –ø–æ–ª—è—Ö
        for keyword in keywords:
            if len(keyword) >= 3:
                similar = Organization.objects.filter(
                    models.Q(name__icontains=keyword) |
                    models.Q(full_name__icontains=keyword) |
                    models.Q(short_name__icontains=keyword)
                ).first()
                if similar:
                    return similar
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ –ø–æ –ø–µ—Ä–≤—ã–º 30 —Å–∏–º–≤–æ–ª–∞–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
        if len(normalized) > 30:
            prefix = normalized[:30]
            similar = Organization.objects.filter(
                models.Q(name__icontains=prefix) |
                models.Q(full_name__icontains=prefix) |
                models.Q(short_name__icontains=prefix)
            ).first()
            if similar:
                return similar
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ü–æ–∏—Å–∫ –ø–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—é –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –≤ full_name
        full_match = Organization.objects.filter(
            models.Q(full_name__icontains=org_name)
        ).first()
        if full_match:
            return full_match
        
        return None
    
    def find_or_create_organization(self, org_name):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        if pd.isna(org_name) or not org_name:
            return None
        
        org_name = str(org_name).strip().strip('"')
        
        if not org_name or org_name == 'null' or org_name == 'None':
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if org_name in self.organization_cache:
            return self.organization_cache[org_name]
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        similar = self.find_similar_organization(org_name)
        if similar:
            self.organization_cache[org_name] = similar
            return similar
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
        formatted_name = self.org_normalizer.format_organization_name(org_name)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ slug
        norm_data = self.org_normalizer.normalize(org_name)
        normalized = norm_data['normalized']
        
        base_slug = slugify(normalized[:50])
        if not base_slug:
            base_slug = 'organization'
        
        unique_slug = base_slug
        counter = 1
        while Organization.objects.filter(slug=unique_slug).exists():
            unique_slug = f"{base_slug}-{counter}"
            counter += 1
        
        try:
            max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
            new_id = max_id + 1
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é —Å –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º
            org = Organization.objects.create(
                organization_id=new_id,
                name=formatted_name,
                full_name=formatted_name,
                short_name=formatted_name[:500] if len(formatted_name) > 500 else formatted_name,
                slug=unique_slug,
                register_opk=False,
                strategic=False,
            )
            
            self.organization_cache[org_name] = org
            return org
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Organization: {e}"))
            return None
    
    def find_or_create_foiv(self, holder_text):
        """–ü–æ–∏—Å–∫ –§–û–ò–í –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—è"""
        if pd.isna(holder_text) or not holder_text:
            return None
        
        holder_text = str(holder_text).strip().strip('"')
        
        if holder_text in self.foiv_cache:
            return self.foiv_cache[holder_text]
        
        try:
            all_foivs = FOIV.objects.all()
            for foiv in all_foivs:
                if foiv.short_name and foiv.short_name.lower() in holder_text.lower():
                    self.foiv_cache[holder_text] = foiv
                    return foiv
                
                words = foiv.short_name.split()
                for word in words:
                    if len(word) > 3 and word.lower() in holder_text.lower():
                        self.foiv_cache[holder_text] = foiv
                        return foiv
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –§–û–ò–í: {e}"))
        
        return None
    
    def find_or_create_rf_representative(self, holder_text, foiv=None):
        """
        –ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –†–§
        """
        if pd.isna(holder_text) or not holder_text:
            return None
        
        holder_text = str(holder_text).strip().strip('"')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"{holder_text}|{foiv.pk if foiv else ''}"
        if cache_key in self.rf_rep_cache:
            return self.rf_rep_cache[cache_key]
        
        try:
            # –°–Ω–∞—á–∞–ª–∞ —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –ø–æ–ª–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
            rf_rep = RFRepresentative.objects.filter(full_text=holder_text).first()
            if rf_rep:
                self.rf_rep_cache[cache_key] = rf_rep.foiv
                return rf_rep.foiv
            
            # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω foiv, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ
            if foiv:
                max_id = RFRepresentative.objects.aggregate(models.Max('rf_representative_id'))['rf_representative_id__max'] or 0
                rf_rep = RFRepresentative.objects.create(
                    rf_representative_id=max_id + 1,
                    foiv=foiv,
                    full_text=holder_text,
                    display_name=f"–†–§ –≤ –ª–∏—Ü–µ {foiv.short_name}"
                )
                self.rf_rep_cache[cache_key] = foiv
                return foiv
                
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –†–§: {e}"))
        
        return None
    
    def process_entity(self, entity_name, ip_object):
        """
        –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—É—â–Ω–æ—Å—Ç–∏
        –í—Å–µ, —á—Ç–æ –Ω–µ —Ñ–∏–∑–ª–∏—Ü–æ - –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è
        """
        if pd.isna(entity_name) or not entity_name:
            return False
        
        entity_type = self.type_detector.detect_type(entity_name)
        
        if entity_type == 'person':
            person = self.find_or_create_person_from_name(entity_name)
            if person:
                ip_object.owner_persons.add(person)
                self.stdout.write(f"        ‚úÖ –§–∏–∑–ª–∏—Ü–æ: {person.get_full_name()}")
                return True
        else:
            # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ - –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è
            org = self.find_or_create_organization(entity_name)
            if org:
                ip_object.owner_organizations.add(org)
                self.stdout.write(f"        ‚úÖ –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è: {org.name[:50]}...")
                return True
        
        return False
    
    def process_holders(self, holders_list, ip_object):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π"""
        if not holders_list:
            return
        
        for holder_name in holders_list:
            self.stdout.write(f"        –ê–Ω–∞–ª–∏–∑: {holder_name[:100]}...")
            self.process_entity(holder_name, ip_object)


class InventionParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π"""
    
    def get_ip_type(self):
        return IPType.objects.filter(slug='invention').first()
    
    def get_required_columns(self):
        return ['registration number', 'invention name']
    
    def process_row(self, row, catalogue, ip_type):
        registration_number = self.clean_string(row.get('registration number'))
        
        if not registration_number:
            return 'skipped'
        
        self.stdout.write(f"\n  üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ç–µ–Ω—Ç–∞ ‚Ññ{registration_number}")
        
        name = self.clean_string(row.get('invention name'))
        if name:
            name = self.rid_formatter.format(name)
        else:
            name = f"–ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ ‚Ññ{registration_number}"
        
        self.stdout.write(f"     –ù–∞–∑–≤–∞–Ω–∏–µ: {name[:50]}...")
        
        application_date = self.parse_date(row.get('application date'))
        registration_date = self.parse_date(row.get('registration date'))
        patent_starting_date = self.parse_date(row.get('patent starting date'))
        expiration_date = self.parse_date(row.get('expiration date'))
        
        if application_date:
            self.stdout.write(f"     –î–∞—Ç–∞ –ø–æ–¥–∞—á–∏: {application_date}")
        if registration_date:
            self.stdout.write(f"     –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏: {registration_date}")
        
        actual = self.parse_bool(row.get('actual'))
        self.stdout.write(f"     –°—Ç–∞—Ç—É—Å: {'–ê–∫—Ç–∏–≤–µ–Ω' if actual else '–ù–µ –∞–∫—Ç–∏–≤–µ–Ω'}")
        
        publication_url = self.clean_string(row.get('publication URL'))
        abstract = self.clean_string(row.get('abstract'))
        claims = self.clean_string(row.get('claims'))
        
        creation_year = None
        if application_date:
            creation_year = application_date.year
        elif registration_date:
            creation_year = registration_date.year
        
        try:
            ip_object, created = IPObject.objects.get_or_create(
                registration_number=registration_number,
                ip_type=ip_type,
                defaults={
                    'name': name,
                    'application_date': application_date,
                    'registration_date': registration_date,
                    'patent_starting_date': patent_starting_date,
                    'expiration_date': expiration_date,
                    'actual': actual,
                    'publication_url': publication_url,
                    'abstract': abstract,
                    'claims': claims,
                    'creation_year': creation_year,
                }
            )
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è IPObject {registration_number}: {e}"))
            return 'skipped'
        
        if self.command.dry_run:
            return 'created' if created else 'updated'
        
        if not created:
            update_fields = []
            
            if name and ip_object.name != name:
                ip_object.name = name
                update_fields.append('name')
            
            if application_date and ip_object.application_date != application_date:
                ip_object.application_date = application_date
                update_fields.append('application_date')
            
            if registration_date and ip_object.registration_date != registration_date:
                ip_object.registration_date = registration_date
                update_fields.append('registration_date')
            
            if patent_starting_date and ip_object.patent_starting_date != patent_starting_date:
                ip_object.patent_starting_date = patent_starting_date
                update_fields.append('patent_starting_date')
            
            if expiration_date and ip_object.expiration_date != expiration_date:
                ip_object.expiration_date = expiration_date
                update_fields.append('expiration_date')
            
            if ip_object.actual != actual:
                ip_object.actual = actual
                update_fields.append('actual')
            
            if publication_url and ip_object.publication_url != publication_url:
                ip_object.publication_url = publication_url
                update_fields.append('publication_url')
            
            if abstract and ip_object.abstract != abstract:
                ip_object.abstract = abstract
                update_fields.append('abstract')
            
            if claims and ip_object.claims != claims:
                ip_object.claims = claims
                update_fields.append('claims')
            
            if creation_year and ip_object.creation_year != creation_year:
                ip_object.creation_year = creation_year
                update_fields.append('creation_year')
            
            if update_fields:
                ip_object.save(update_fields=update_fields)
                self.stdout.write(f"     –û–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª–µ–π: {len(update_fields)}")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤
        authors_str = row.get('authors')
        if not pd.isna(authors_str) and authors_str:
            authors_data = self.parse_authors(authors_str)
            if authors_data:
                self.stdout.write(f"     üë• –ê–≤—Ç–æ—Ä—ã: {len(authors_data)} —á–µ–ª.")
                for author_data in authors_data:
                    person = self.find_or_create_person(author_data)
                    if person:
                        ip_object.authors.add(person)
                        self.stdout.write(f"        –ê–≤—Ç–æ—Ä: {author_data['full_name']}")
            else:
                self.stdout.write("     üë• –ê–≤—Ç–æ—Ä—ã: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
        else:
            self.stdout.write("     üë• –ê–≤—Ç–æ—Ä—ã: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
        holders_str = row.get('patent holders')
        if not pd.isna(holders_str) and holders_str:
            holders_list = self.parse_patent_holders(holders_str)
            if holders_list:
                self.stdout.write(f"     üè¢ –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏: {len(holders_list)}")
                self.process_holders(holders_list, ip_object)
            else:
                self.stdout.write("     üè¢ –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
        else:
            self.stdout.write("     üè¢ –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
        
        return 'created' if created else 'updated'
    
    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π..."))
        
        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'errors': 0
        }
        
        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'invention' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats
        
        with tqdm(total=len(df), desc="  –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–µ–π", unit=" –∑–∞–ø") as pbar:
            for idx, row in df.iterrows():
                try:
                    result = self.process_row(row, catalogue, ip_type)
                    
                    if result == 'created':
                        stats['created'] += 1
                    elif result == 'updated':
                        stats['updated'] += 1
                    elif result == 'skipped':
                        stats['skipped'] += 1
                    
                    stats['processed'] += 1
                    
                except Exception as e:
                    stats['errors'] += 1
                    reg_num = row.get('registration number', 'N/A')
                    self.stdout.write(self.style.ERROR(f"\n  ‚ùå –û—à–∏–±–∫–∞ –≤ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    logger.error(f"Error processing invention {reg_num}: {e}", exc_info=True)
                
                finally:
                    pbar.update(1)
        
        self.stdout.write(self.style.SUCCESS(f"  ‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"     –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']}, –û—à–∏–±–æ–∫: {stats['errors']}")
        
        return stats


# –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã (UtilityModelParser, IndustrialDesignParser –∏ —Ç.–¥.) 
# –±—É–¥—É—Ç –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–º–∏, –ø—Ä–æ—Å—Ç–æ —Å –¥—Ä—É–≥–∏–º–∏ —Ç–∏–ø–∞–º–∏ –†–ò–î


class UtilityModelParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    def get_ip_type(self):
        return IPType.objects.filter(slug='utility-model').first()
    
    def get_required_columns(self):
        return ['registration number', 'utility model name']
    
    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class IndustrialDesignParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤"""
    
    def get_ip_type(self):
        return IPType.objects.filter(slug='industrial-design').first()
    
    def get_required_columns(self):
        return ['registration number', 'industrial design name']
    
    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class IntegratedCircuitTopologyParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º"""
    
    def get_ip_type(self):
        return IPType.objects.filter(slug='integrated-circuit-topology').first()
    
    def get_required_columns(self):
        return ['registration number', 'microchip name']
    
    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä —Ç–æ–ø–æ–ª–æ–≥–∏–π –º–∏–∫—Ä–æ—Å—Ö–µ–º –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class ComputerProgramParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú"""
    
    def get_ip_type(self):
        return IPType.objects.filter(slug='computer-program').first()
    
    def get_required_columns(self):
        return ['registration number', 'program name']
    
    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class DatabaseParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö"""
    
    def get_ip_type(self):
        return IPType.objects.filter(slug='database').first()
    
    def get_required_columns(self):
        return ['registration number', 'db name']
    
    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class Command(BaseCommand):
    help = '–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞'
    
    def add_arguments(self, parser):
        parser.add_argument('--catalogue-id', type=int, help='ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')
        parser.add_argument('--ip-type', type=str, 
                        choices=['invention', 'utility-model', 'industrial-design', 
                                'integrated-circuit-topology', 'computer-program', 'database'],
                        help='–¢–∏–ø –†–ò–î –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–∞—Ä—Å—è—Ç—Å—è –≤—Å–µ)')
        parser.add_argument('--dry-run', action='store_true', help='–†–µ–∂–∏–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î')
        parser.add_argument('--encoding', type=str, default='utf-8', help='–ö–æ–¥–∏—Ä–æ–≤–∫–∞ CSV —Ñ–∞–π–ª–∞')
        parser.add_argument('--delimiter', type=str, default=',', help='–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV —Ñ–∞–π–ª–µ')
        parser.add_argument('--batch-size', type=int, default=100, help='–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è bulk-–æ–ø–µ—Ä–∞—Ü–∏–π')
        parser.add_argument('--min-year', type=int, default=2000, help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏')
        parser.add_argument('--skip-filters', action='store_true', help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏)')
        parser.add_argument('--only-active', action='store_true', help='–ü–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç—ã (actual = True)')
        parser.add_argument('--max-rows', type=int, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)')
        parser.add_argument('--force', action='store_true', help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–∂–µ –µ—Å–ª–∏ –∫–∞—Ç–∞–ª–æ–≥ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω')
        parser.add_argument('--mark-processed', action='store_true', 
                        help='–ü–æ–º–µ—Ç–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (–¥–∞–∂–µ –µ—Å–ª–∏ –±—ã–ª–∏ –æ—à–∏–±–∫–∏)')
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.parsers = {
            'invention': InventionParser(self),
            'utility-model': UtilityModelParser(self),
            'industrial-design': IndustrialDesignParser(self),
            'integrated-circuit-topology': IntegratedCircuitTopologyParser(self),
            'computer-program': ComputerProgramParser(self),
            'database': DatabaseParser(self),
        }
    
    def handle(self, *args, **options):
        self.dry_run = options['dry_run']
        self.encoding = options['encoding']
        self.delimiter = options['delimiter']
        self.batch_size = options['batch_size']
        self.min_year = options['min_year']
        self.skip_filters = options['skip_filters']
        self.only_active = options['only_active']
        self.max_rows = options.get('max_rows')
        self.force = options.get('force', False)
        self.mark_processed = options.get('mark_processed', False)
        
        if self.dry_run:
            self.stdout.write(self.style.WARNING("\nüîç –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î\n"))
        
        if self.only_active:
            self.stdout.write(self.style.WARNING("üìå –†–µ–∂–∏–º: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (actual = True)"))
        
        if self.force:
            self.stdout.write(self.style.WARNING("‚ö†Ô∏è  –†–µ–∂–∏–º: –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏)"))
        
        catalogues = self.get_catalogues(options.get('catalogue_id'), options.get('ip_type'))
        
        if not catalogues:
            raise CommandError('–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–∞—Ç–∞–ª–æ–≥–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')
        
        total_stats = {
            'catalogues': len(catalogues),
            'processed': 0,
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'errors': 0
        }
        
        for catalogue in catalogues:
            self.stdout.write(self.style.SUCCESS(f"\n{'='*60}"))
            self.stdout.write(self.style.SUCCESS(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞: {catalogue.name}"))
            self.stdout.write(self.style.SUCCESS(f"   ID: {catalogue.id}, –¢–∏–ø: {catalogue.ip_type.name if catalogue.ip_type else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'}"))
            self.stdout.write(self.style.SUCCESS(f"{'='*60}"))
            
            stats = self.process_catalogue(catalogue)
            
            for key in ['processed', 'created', 'updated', 'skipped', 'errors']:
                total_stats[key] += stats.get(key, 0)
        
        self.print_final_stats(total_stats)
    
    def get_catalogues(self, catalogue_id=None, ip_type_slug=None):
        queryset = FipsOpenDataCatalogue.objects.all()
        
        if catalogue_id:
            queryset = queryset.filter(id=catalogue_id)
        elif ip_type_slug:
            queryset = queryset.filter(ip_type__slug=ip_type_slug)
        else:
            queryset = queryset.exclude(catalogue_file='')
        
        return queryset.order_by('ip_type__id', '-publication_date')
    
    def process_catalogue(self, catalogue):
        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'errors': 0
        }
        
        if not catalogue.catalogue_file:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –£ –∫–∞—Ç–∞–ª–æ–≥–∞ ID={catalogue.id} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª"))
            stats['errors'] += 1
            return stats
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–∞—Ä—Å–∏–ª—Å—è –ª–∏ —É–∂–µ —ç—Ç–æ—Ç –∫–∞—Ç–∞–ª–æ–≥
        if not self.force and hasattr(catalogue, 'parsed_date') and catalogue.parsed_date:
            self.stdout.write(self.style.WARNING(
                f"  ‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ —É–∂–µ –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω {catalogue.parsed_date.strftime('%d.%m.%Y %H:%M')}"
            ))
            self.stdout.write(self.style.WARNING(f"     –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --force –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"))
            stats['skipped'] += 1
            return stats
        
        ip_type_slug = catalogue.ip_type.slug if catalogue.ip_type else None
        
        if ip_type_slug not in self.parsers:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –ù–µ—Ç –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è —Ç–∏–ø–∞ –†–ò–î: {ip_type_slug}"))
            stats['errors'] += 1
            return stats
        
        parser = self.parsers[ip_type_slug]
        df = self.load_csv(catalogue)
        
        if df is None or df.empty:
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å"))
            stats['skipped'] += 1
            return stats
        
        self.stdout.write(f"  üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
        
        missing_columns = self.check_required_columns(df, parser.get_required_columns())
        if missing_columns:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}"))
            stats['errors'] += 1
            return stats
        
        if not self.skip_filters:
            df = self.apply_filters(df)
        
        if df.empty:
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"))
            stats['skipped'] += 1
            return stats
        
        self.stdout.write(f"  üìä –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
        if self.max_rows and len(df) > self.max_rows:
            df = df.head(self.max_rows)
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {self.max_rows} –∑–∞–ø–∏—Å–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"))
        
        try:
            parser_stats = parser.parse_dataframe(df, catalogue)
            stats.update(parser_stats)
            
            # –ï—Å–ª–∏ –Ω–µ dry-run –∏ –Ω–µ—Ç –æ—à–∏–±–æ–∫ (–∏–ª–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–æ–º–µ—á–∞–µ–º), –æ–±–Ω–æ–≤–ª—è–µ–º –¥–∞—Ç—É –ø–∞—Ä—Å–∏–Ω–≥–∞
            if not self.dry_run and hasattr(catalogue, 'parsed_date'):
                if stats['errors'] == 0 or self.mark_processed:
                    catalogue.parsed_date = timezone.now()
                    catalogue.save(update_fields=['parsed_date'])
                    self.stdout.write(self.style.SUCCESS(f"  ‚úÖ –ö–∞—Ç–∞–ª–æ–≥ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π"))
                else:
                    self.stdout.write(self.style.WARNING(
                        f"  ‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ –Ω–µ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫ "
                        f"(–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --mark-processed –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–º–µ—Ç–∫–∏)"
                    ))
            
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}"))
            logger.error(f"Error parsing catalogue {catalogue.id}: {e}", exc_info=True)
            stats['errors'] += 1
        
        return stats
    
    def load_csv(self, catalogue):
        file_path = catalogue.catalogue_file.path
        
        if not os.path.exists(file_path):
            self.stdout.write(self.style.ERROR(f"  ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"))
            return None
        
        try:
            strategies = [
                {'encoding': self.encoding, 'delimiter': self.delimiter, 'skipinitialspace': True},
                {'encoding': 'cp1251', 'delimiter': self.delimiter, 'skipinitialspace': True},
                {'encoding': 'utf-8', 'delimiter': ';', 'skipinitialspace': True},
                {'encoding': 'cp1251', 'delimiter': ';', 'skipinitialspace': True},
                {'encoding': 'utf-8', 'delimiter': '\t', 'skipinitialspace': True},
            ]
            
            for strategy in strategies:
                try:
                    df = pd.read_csv(file_path, **strategy, dtype=str, keep_default_na=False)
                    self.stdout.write(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {strategy}")
                    
                    df.columns = [col.strip().strip('\ufeff').strip('"') for col in df.columns]
                    
                    return df
                except Exception as e:
                    continue
            
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π")
            
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}"))
            return None
    
    def check_required_columns(self, df, required_columns):
        missing = [col for col in required_columns if col not in df.columns]
        return missing
    
    def apply_filters(self, df):
        original_count = len(df)
        
        if 'registration date' in df.columns:
            df = self.filter_by_registration_year(df)
        
        if self.only_active and 'actual' in df.columns:
            df = self.filter_by_actual(df)
        
        filtered_count = len(df)
        if filtered_count < original_count:
            self.stdout.write(f"  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {original_count} ‚Üí {filtered_count} –∑–∞–ø–∏—Å–µ–π")
        
        return df
    
    def filter_by_registration_year(self, df):
        def extract_year(date_str):
            try:
                if pd.isna(date_str) or not date_str:
                    return None
                
                date_str = str(date_str).strip()
                if not date_str:
                    return None
                
                for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
                    try:
                        return datetime.strptime(date_str, fmt).year
                    except (ValueError, TypeError):
                        continue
                
                try:
                    return pd.to_datetime(date_str).year
                except (ValueError, TypeError):
                    return None
            except:
                return None
        
        self.stdout.write("  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≥–æ–¥—É —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏...")
        df['_year'] = df['registration date'].apply(extract_year)
        
        years_dist = df['_year'].value_counts().sort_index()
        years_list = list(years_dist.items())
        if len(years_list) > 0:
            self.stdout.write(f"     –î–∏–∞–ø–∞–∑–æ–Ω –≥–æ–¥–æ–≤: {years_list[0][0]:.0f} - {years_list[-1][0]:.0f}")
            self.stdout.write(f"     –ü–µ—Ä–≤—ã–µ 5: {years_list[:5]}")
            self.stdout.write(f"     –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5: {years_list[-5:]}")
        
        filtered_df = df[df['_year'] >= self.min_year].copy()
        filtered_df.drop('_year', axis=1, inplace=True)
        
        return filtered_df
    
    def filter_by_actual(self, df):
        def parse_actual(value):
            if pd.isna(value) or not value:
                return False
            value = str(value).lower().strip()
            return value in ['1', 'true', 'yes', '–¥–∞', '–¥–µ–π—Å—Ç–≤—É–µ—Ç', 't', '1.0', '–∞–∫—Ç–∏–≤–µ–Ω']
        
        df['_actual'] = df['actual'].apply(parse_actual)
        filtered_df = df[df['_actual'] == True].copy()
        filtered_df.drop('_actual', axis=1, inplace=True)
        
        return filtered_df
    
    def print_final_stats(self, stats):
        self.stdout.write(self.style.SUCCESS(f"\n{'='*60}"))
        self.stdout.write(self.style.SUCCESS("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê"))
        self.stdout.write(self.style.SUCCESS(f"{'='*60}"))
        self.stdout.write(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞—Ç–∞–ª–æ–≥–æ–≤: {stats['catalogues']}")
        self.stdout.write(f"üìù –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}")
        self.stdout.write(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ: {stats['created']}")
        self.stdout.write(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}")
        self.stdout.write(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']}")
        
        if stats['errors'] > 0:
            self.stdout.write(self.style.ERROR(f"‚ùå –û—à–∏–±–æ–∫: {stats['errors']}"))
        else:
            self.stdout.write(self.style.SUCCESS(f"‚úÖ –û—à–∏–±–æ–∫: {stats['errors']}"))
        
        if self.dry_run:
            self.stdout.write(self.style.WARNING("\nüîç –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î"))
        
        self.stdout.write(self.style.SUCCESS(f"{'='*60}"))