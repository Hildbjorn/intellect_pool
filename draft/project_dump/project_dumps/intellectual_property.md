# –§–∞–π–ª: apps.py

```
from django.apps import AppConfig


class IntellectualPropertyConfig(AppConfig):
    default_auto_field = 'django.db.models.BigAutoField'
    name = 'intellectual_property'
    verbose_name = '–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏'

```


-----

# –§–∞–π–ª: tests.py

```
from django.test import TestCase

# Create your tests here.

```


-----

# –§–∞–π–ª: urls.py

```
from django.urls import path


# –ú–∞—Ä—à—Ä—É—Ç—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
urlpatterns = [
]
```


-----

# –§–∞–π–ª: __init__.py

```

```


-----

# –§–∞–π–ª: admin\admin_fips_catalogue.py

```
from django.contrib import admin
from django.utils.translation import gettext_lazy as _
from django.utils.html import format_html
from django.urls import reverse
from django.db import models
from intellectual_property.models.models_fips_catalogue import FipsOpenDataCatalogue


@admin.register(FipsOpenDataCatalogue)
class FipsOpenDataCatalogueAdmin(admin.ModelAdmin):
    """
    –ü—Ä–æ—Å—Ç–æ–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –º–æ–¥–µ–ª–∏ FipsOpenDataCatalogue.
    """
    
    list_display = [
        'name',
        'ip_type',
        'publication_date',
        'upload_date',
    ]
    
    list_filter = [
        'ip_type',
        'publication_date',
    ]
    
    search_fields = [
        'name',
        'ip_type__name',
    ]
    
    readonly_fields = [
        'upload_date',
        'last_modified',
    ]
    
    fieldsets = (
        (None, {
            'fields': ('ip_type', 'name', 'catalogue_file', 'publication_date', 'description')
        }),
        (_('–°–ª—É–∂–µ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'), {
            'fields': ('upload_date', 'last_modified'),
            'classes': ('collapse',),
        }),
    )
    
    autocomplete_fields = ['ip_type']
    date_hierarchy = 'publication_date'

```


-----

# –§–∞–π–ª: admin\admin_ip.py

```
from django.contrib import admin
from django.utils.translation import gettext_lazy as _
from django.utils.html import format_html
from django.urls import reverse
from django.db import models
from common.admin_utils import AdminDisplayMixin, AdminImageMixin
from intellectual_property.models.models_ip import AdditionalPatent, IPImage, IPObject, IPType, ProtectionDocumentType


@admin.register(ProtectionDocumentType)
class ProtectionDocumentTypeAdmin(admin.ModelAdmin):
    """
    –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –º–æ–¥–µ–ª–∏ ProtectionDocumentType.
    """
    list_display = [
        'name',
        'description',
        'slug',
    ]
    
    search_fields = [
        'name',
        'description',
        'slug',
    ]
    
    list_filter = [
        'name',
    ]
    
    fieldsets = (
        (None, {
            'fields': ('name', 'description'),
            'description': _('–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∏–¥–µ –æ—Ö—Ä–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞')
        }),
        (_('URL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã'), {
            'fields': ('slug',),
            'description': _('–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è URL'),
            'classes': ('wide',),
        }),
    )
    
    readonly_fields = ['slug']
    
    def get_queryset(self, request):
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –ë–î.
        """
        return super().get_queryset(request).annotate(
            ip_types_count=models.Count('ip_types')
        )


@admin.register(IPType)
class IPTypeAdmin(admin.ModelAdmin):
    """
    –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –º–æ–¥–µ–ª–∏ IPType.
    """
    # –ü–æ–ª—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Å–ø–∏—Å–∫–µ
    list_display = [
        'name',
        'description',
        'protection_document_link',
        'validity_duration',
        'slug',
    ]
    
    # –ü–æ–ª—è –¥–ª—è –ø–æ–∏—Å–∫–∞
    search_fields = [
        'name',
        'description',
        'slug',
        'protection_document_type__name',
    ]
    
    # –§–∏–ª—å—Ç—Ä—ã –≤ –ø—Ä–∞–≤–æ–π –ø–∞–Ω–µ–ª–∏
    list_filter = [
        'protection_document_type',
        'name',
    ]
    
    # –ü–æ–ª—è –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    fieldsets = (
        (None, {
            'fields': ('name', 'description'),
            'description': _('–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–∏–ø–µ –†–ò–î')
        }),
        (_('–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è'), {
            'fields': ('validity_duration',),
            'description': _('–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –æ—Ö—Ä–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞'),
            'classes': ('wide',),
        }),
        (_('–°–≤—è–∑–∏'), {
            'fields': ('protection_document_type',),
            'description': _('–°–≤—è–∑—å —Å –≤–∏–¥–æ–º –æ—Ö—Ä–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞'),
            'classes': ('wide',),
        }),
        (_('URL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã'), {
            'fields': ('slug',),
            'description': _('–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è URL'),
            'classes': ('wide',),
        }),
    )
    
    # –ü–æ–ª—è —Ç–æ–ª—å–∫–æ –¥–ª—è —á—Ç–µ–Ω–∏—è
    readonly_fields = ['slug']
    
    # –§–∏–ª—å—Ç—Ä—ã –ø–æ —Å–≤—è–∑–∞–Ω–Ω—ã–º –ø–æ–ª—è–º
    autocomplete_fields = ['protection_document_type']
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ–ª—è–º
    list_display_links = ['name']
    
    def get_queryset(self, request):
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –ë–î.
        """
        return super().get_queryset(request).select_related(
            'protection_document_type'
        )
    
    def protection_document_link(self, obj):
        """
        –°—Å—ã–ª–∫–∞ –Ω–∞ —Å–≤—è–∑–∞–Ω–Ω—ã–π –≤–∏–¥ –æ—Ö—Ä–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è URL.
        """
        if obj.protection_document_type:
            try:
                # –°–ø–æ—Å–æ–± 1: –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç admin:app_model_action
                url = reverse(
                    f'admin:{obj.protection_document_type._meta.app_label}_{obj.protection_document_type._meta.model_name}_change',
                    args=[obj.protection_document_type.id]
                )
            except:
                try:
                    # –°–ø–æ—Å–æ–± 2: –ü—Ä–æ–±—É–µ–º —Ñ–æ—Ä–º–∞—Ç –±–µ–∑ app_label
                    url = reverse(
                        f'{obj.protection_document_type._meta.model_name}_change',
                        args=[obj.protection_document_type.id]
                    )
                except:
                    try:
                        # –°–ø–æ—Å–æ–± 3: –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å django.contrib.admin.utils.get_admin_url
                        from django.contrib.admin.utils import get_admin_url
                        url = get_admin_url(admin.site, obj.protection_document_type, 'change')
                    except:
                        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç –±–µ–∑ —Å—Å—ã–ª–∫–∏
                        return obj.protection_document_type.name
            
            return format_html(
                '<a href="{}">{}</a>',
                url,
                obj.protection_document_type.name
            )
        return '-'
    protection_document_link.short_description = _('–í–∏–¥ –æ—Ö—Ä–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞')
    protection_document_link.admin_order_field = 'protection_document_type__name'
    
    def get_readonly_fields(self, request, obj=None):
        """
        –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ readonly –ø–æ–ª–µ–π.
        """
        readonly_fields = list(self.readonly_fields)
        
        if obj:  # –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –æ–±—ä–µ–∫—Ç–∞
            readonly_fields.extend(['slug'])
        
        return readonly_fields
    
    def get_list_filter(self, request):
        """
        –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤.
        """
        list_filter = list(self.list_filter)
        return list_filter
    
    def save_model(self, request, obj, form, change):
        """
        –î–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏.
        """
        super().save_model(request, obj, form, change)
        
        action = _('–æ–±–Ω–æ–≤–ª–µ–Ω') if change else _('—Å–æ–∑–¥–∞–Ω')
        self.message_user(
            request,
            _('–¢–∏–ø –†–ò–î "{}" —É—Å–ø–µ—à–Ω–æ {}').format(obj.name, action),
            level='SUCCESS'
        )
    
    def response_change(self, request, obj):
        """
        –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–≤–µ—Ç–∞ –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è.
        """
        if "_continue" in request.POST:
            self.message_user(
                request,
                _('–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ "{}"').format(obj.name),
                level='INFO'
            )
        return super().response_change(request, obj)


@admin.register(AdditionalPatent)
class AdditionalPatentAdmin(admin.ModelAdmin):
    """
    –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç–µ–Ω—Ç–æ–≤.
    """
    list_display = [
        'patent_number',
        'patent_date',
        'description_short',
        'ip_objects_count',
    ]
    
    search_fields = [
        'patent_number',
        'description',
    ]
    
    list_filter = [
        'patent_date',
    ]
    
    fieldsets = (
        (None, {
            'fields': ('patent_number', 'patent_date', 'description')
        }),
    )
    
    def description_short(self, obj):
        """–ö–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞"""
        if obj.description and len(obj.description) > 50:
            return obj.description[:50] + '...'
        return obj.description
    description_short.short_description = _('–û–ø–∏—Å–∞–Ω–∏–µ')
    
    def get_queryset(self, request):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–¥—Å—á–µ—Ç–æ–º —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –†–ò–î"""
        return super().get_queryset(request).annotate(
            ip_objects_count=models.Count('ip_objects')
        )
    
    def ip_objects_count(self, obj):
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –†–ò–î, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —ç—Ç–∏–º –ø–∞—Ç–µ–Ω—Ç–æ–º"""
        count = getattr(obj, 'ip_objects_count', 0)
        if count:
            url = reverse('admin:intellectual_property_ipobject_changelist') + f'?additional_patents__id__exact={obj.id}'
            return format_html('<a href="{}">{}</a>', url, count)
        return count
    ip_objects_count.short_description = _('–°–≤—è–∑–∞–Ω–Ω—ã—Ö –†–ò–î')
    ip_objects_count.admin_order_field = 'ip_objects_count'


@admin.register(IPImage)
class IPImageAdmin(AdminImageMixin, admin.ModelAdmin):
    """
    –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –†–ò–î.
    """
    list_display = [
        'image_thumbnail',
        'title',
        'is_main',
        'sort_order',
        'ip_objects_count',
    ]
    
    search_fields = [
        'title',
        'description',
    ]
    
    list_filter = [
        'is_main',
    ]
    
    fieldsets = (
        (None, {
            'fields': ('image', 'title', 'description', 'is_main', 'sort_order')
        }),
    )
    
    def get_queryset(self, request):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–¥—Å—á–µ—Ç–æ–º —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –†–ò–î"""
        return super().get_queryset(request).annotate(
            ip_objects_count=models.Count('ip_objects')
        )
    
    def ip_objects_count(self, obj):
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –†–ò–î, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —ç—Ç–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º"""
        count = getattr(obj, 'ip_objects_count', 0)
        if count:
            url = reverse('admin:intellectual_property_ipobject_changelist') + f'?images__id__exact={obj.id}'
            return format_html('<a href="{}">{}</a>', url, count)
        return count
    ip_objects_count.short_description = _('–°–≤—è–∑–∞–Ω–Ω—ã—Ö –†–ò–î')
    ip_objects_count.admin_order_field = 'ip_objects_count'


@admin.register(IPObject)
class IPObjectAdmin(AdminDisplayMixin, admin.ModelAdmin):
    """
    –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –†–ò–î.
    """
    list_display = [
        'name',
        'ip_type',
        'registration_number',
        'actual_badge',
        'application_date',
        'expiration_date',
        'created_at_display',
    ]
    
    list_filter = [
        'ip_type',
        'actual',
        'application_date',
        'registration_date',
        'expiration_date',
    ]
    
    search_fields = [
        'name',
        'registration_number',
        'ea_application_number',
        'pct_application_number',
        'description',
        'abstract',
        'claims',
    ]
    
    filter_horizontal = [
        'authors',
        'owner_persons',
        'owner_organizations',
        'first_usage_countries',
        'additional_patents',
        'images',
        'programming_languages',
        'dbms',
        'operating_systems',
    ]
    
    autocomplete_fields = [
        'ip_type',
        'paris_convention_priority_country',
    ]
    
    readonly_fields = [
        'created_at',
        'updated_at',
        'all_owners_display',
    ]
    
    fieldsets = (
        (_('üìã –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'), {
            'fields': (
                'name',
                'ip_type',
                'actual',
            )
        }),
        
        (_('üë• –ê–≤—Ç–æ—Ä—ã –∏ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏'), {
            'fields': (
                'authors',
                'owner_persons',
                'owner_organizations',
                'all_owners_display',
            ),
            'classes': ('wide',),
        }),
        
        (_('üìÖ –î–∞—Ç—ã –∏ —Å—Ä–æ–∫–∏'), {
            'fields': (
                ('creation_year', 'publication_year', 'update_year'),
                ('application_date', 'registration_date'),
                ('patent_starting_date', 'expiration_date'),
            ),
            'classes': ('wide',),
        }),
        
        (_('üî¢ –ù–æ–º–µ—Ä–∞ –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã'), {
            'fields': (
                'registration_number',
                'revoked_patent_number',
                'publication_url',
            ),
            'classes': ('wide',),
        }),
        
        (_('üåç –ï–≤—Ä–∞–∑–∏–π—Å–∫–∞—è –∑–∞—è–≤–∫–∞'), {
            'fields': (
                'ea_application_number',
                'ea_application_date',
                ('ea_application_publish_number', 'ea_application_publish_date'),
            ),
            'classes': ('collapse',),
        }),
        
        (_('üåê PCT –∑–∞—è–≤–∫–∞'), {
            'fields': (
                'pct_application_number',
                'pct_application_date',
                ('pct_application_publish_number', 'pct_application_publish_date'),
                'pct_application_examination_start_date',
            ),
            'classes': ('collapse',),
        }),
        
        (_('üèõÔ∏è –ü–∞—Ä–∏–∂—Å–∫–∞—è –∫–æ–Ω–≤–µ–Ω—Ü–∏—è'), {
            'fields': (
                'paris_convention_priority_number',
                'paris_convention_priority_date',
                'paris_convention_priority_country',
            ),
            'classes': ('collapse',),
        }),
        
        (_('üìç –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–ø–æ–ª–æ–≥–∏–∏'), {
            'fields': (
                'first_usage_date',
                'first_usage_countries',
            ),
            'classes': ('collapse',),
        }),
        
        (_('üìÑ –¢–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è'), {
            'fields': (
                'abstract',
                'claims',
                'description',
                'source_code_deposit',
            ),
            'classes': ('wide',),
        }),
        
        (_('üìé –°–≤—è–∑–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã'), {
            'fields': (
                'additional_patents',
                'images',
            ),
            'classes': ('wide',),
        }),
        
        (_('üíª IT-—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞'), {
            'fields': (
                'programming_languages',
                'dbms',
                'operating_systems',
            ),
            'classes': ('collapse',),
        }),
        
        (_('‚öôÔ∏è –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'), {
            'fields': (
                'created_at',
                'updated_at',
            ),
            'classes': ('collapse',),
        }),
    )
    
    def actual_badge(self, obj):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–æ–≤–æ–π –æ—Ö—Ä–∞–Ω—ã"""
        if obj.actual:
            return format_html(
                '<span style="background-color: #28a745; color: white; padding: 3px 7px; border-radius: 10px;">‚úì –î–µ–π—Å—Ç–≤—É–µ—Ç</span>'
            )
        return format_html(
            '<span style="background-color: #dc3545; color: white; padding: 3px 7px; border-radius: 10px;">‚úó –ù–µ –¥–µ–π—Å—Ç–≤—É–µ—Ç</span>'
        )
    actual_badge.short_description = _('–°—Ç–∞—Ç—É—Å')
    actual_badge.admin_order_field = 'actual'
    
    def all_owners_display(self, obj):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π –≤ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        owners = obj.all_owners
        if owners:
            html = ['<ul style="margin: 0; padding-left: 20px;">']
            for owner in owners:
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ –∞–¥–º–∏–Ω–∫—É
                try:
                    app_label = owner._meta.app_label
                    model_name = owner._meta.model_name
                    url = reverse(f'admin:{app_label}_{model_name}_change', args=[owner.pk])
                    html.append(f'<li><a href="{url}">{owner}</a></li>')
                except:
                    html.append(f'<li>{owner}</li>')
            html.append('</ul>')
            return format_html(''.join(html))
        return '-'
    all_owners_display.short_description = _('–í—Å–µ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏')
    
    def get_queryset(self, request):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤"""
        return super().get_queryset(request).select_related(
            'ip_type',
            'paris_convention_priority_country',
        ).prefetch_related(
            'authors',
            'owner_persons',
            'owner_organizations',
            'first_usage_countries',
            'programming_languages',
            'dbms',
            'operating_systems',
        )
    
    actions = ['mark_as_actual', 'mark_as_not_actual']
    
    @admin.action(description=_('‚úÖ –û—Ç–º–µ—Ç–∏—Ç—å –∫–∞–∫ –¥–µ–π—Å—Ç–≤—É—é—â–∏–µ'))
    def mark_as_actual(self, request, queryset):
        updated = queryset.update(actual=True)
        self.message_user(request, _('–û–±–Ω–æ–≤–ª–µ–Ω–æ {} –æ–±—ä–µ–∫—Ç–æ–≤ –†–ò–î').format(updated))
    
    @admin.action(description=_('‚ùå –û—Ç–º–µ—Ç–∏—Ç—å –∫–∞–∫ –Ω–µ–¥–µ–π—Å—Ç–≤—É—é—â–∏–µ'))
    def mark_as_not_actual(self, request, queryset):
        updated = queryset.update(actual=False)
        self.message_user(request, _('–û–±–Ω–æ–≤–ª–µ–Ω–æ {} –æ–±—ä–µ–∫—Ç–æ–≤ –†–ò–î').format(updated))
```


-----

# –§–∞–π–ª: admin\__init__.py

```
import os
import glob

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∞–¥–º–∏–Ω–∫–∏ –∏–∑ –ø–∞–ø–∫–∏
admin_files = glob.glob(os.path.dirname(__file__) + "/*.py")
for module in admin_files:
    if not module.endswith('__init__.py'):
        module_name = os.path.basename(module)[:-3]
        exec(f"from .{module_name} import *")
```


-----

# –§–∞–π–ª: forms\__init__.py

```
import os
import glob

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã –∏–∑ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ
model_files = glob.glob(os.path.dirname(__file__) + "/*.py")
for module in model_files:
    if not module.endswith('__init__.py'):
        module_name = os.path.basename(module)[:-3]
        exec(f"from .{module_name} import *")
```


-----

# –§–∞–π–ª: management\help.txt

```
# –†–µ–∂–∏–º ONLY-ACTIVE: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (actual = True)
# –†–µ–∂–∏–º MIN-YEAR 2020: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π 2020 –≥–æ–¥–∞ –∏ –ø–æ–∑–∂–µ
# –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î

–î–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ —Ç–∏–ø–∞–º –†–ò–î:
--ip-type invention ‚Äî –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è
--ip-type utility-model ‚Äî –ø–æ–ª–µ–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏
--ip-type industrial-design ‚Äî –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã
--ip-type integrated-circuit-topology ‚Äî —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º
--ip-type computer-program ‚Äî –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –≠–í–ú
--ip-type database ‚Äî –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

python manage.py pars_fips_catalogue --only-active --min-year 2020 --ip-type invention --dry-run

# –¢–µ—Å—Ç –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π
python manage.py pars_fips_catalogue --only-active --min-year 2020 --ip-type invention --max-rows 10

# –í—Å–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è
python manage.py pars_fips_catalogue --only-active --ip-type invention --force

============
    HELP
============

usage: manage.py pars_fips_catalogue [-h] [--catalogue-id CATALOGUE_ID] [--ip-type {invention,utility-model,industrial-design,integrated-circuit-topology,computer-program,database}] [--dry-run] [--encoding ENCODING]
                                     [--delimiter DELIMITER] [--batch-size BATCH_SIZE] [--min-year MIN_YEAR] [--skip-filters] [--only-active] [--max-rows MAX_ROWS] [--version] [-v {0,1,2,3}] [--settings SETTINGS]      
                                     [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks]

–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞

options:
  -h, --help            show this help message and exit
  --catalogue-id CATALOGUE_ID
                        ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
  --ip-type {invention,utility-model,industrial-design,integrated-circuit-topology,computer-program,database}
                        –¢–∏–ø –†–ò–î –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–∞—Ä—Å—è—Ç—Å—è –≤—Å–µ)
  --dry-run             –†–µ–∂–∏–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
  --encoding ENCODING   –ö–æ–¥–∏—Ä–æ–≤–∫–∞ CSV —Ñ–∞–π–ª–∞
  --delimiter DELIMITER
                        –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV —Ñ–∞–π–ª–µ
  --batch-size BATCH_SIZE
                        –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è bulk-–æ–ø–µ—Ä–∞—Ü–∏–π
  --min-year MIN_YEAR   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
  --skip-filters        –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏)
  --only-active         –ü–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç—ã (actual = True)
  --max-rows MAX_ROWS   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
  --version             Show program's version number and exit.
  -v, --verbosity {0,1,2,3}
                        Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output
  --settings SETTINGS   The Python path to a settings module, e.g. "myproject.settings.main". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used.
  --pythonpath PYTHONPATH
                        A directory to add to the Python path, e.g. "/home/djangoprojects/myproject".
  --traceback           Display a full stack trace on CommandError exceptions.
  --no-color            Don't colorize the command output.
  --force-color         Force colorization of the command output.
  --skip-checks         Skip system checks.

```


-----

# –§–∞–π–ª: management\__init__.py

```

```


-----

# –§–∞–π–ª: management\commands\pars_fips_catalogue.py

```
"""
–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞.
–û–±–µ—Ä—Ç–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–ª–µ–≥–∏—Ä—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –ø–∞—Ä—Å–µ—Ä–∞–º.
"""

import logging
import os

from django.core.management.base import BaseCommand, CommandError
from django.utils import timezone
import pandas as pd

from intellectual_property.models import FipsOpenDataCatalogue

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä—ã –∏–∑ –ø–∞–∫–µ—Ç–∞ parsers (–ø—Ä–∞–≤–∏–ª—å–Ω–æ: ..parsers)
from ..parsers import (
    InventionParser, UtilityModelParser, IndustrialDesignParser,
    IntegratedCircuitTopologyParser, ComputerProgramParser, DatabaseParser
)
from ..utils.csv_loader import load_csv_with_strategies
from ..utils.filters import apply_filters

logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = '–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞'

    def add_arguments(self, parser):
        parser.add_argument('--catalogue-id', type=int, help='ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')
        parser.add_argument('--ip-type', type=str,
                        choices=['invention', 'utility-model', 'industrial-design',
                                'integrated-circuit-topology', 'computer-program', 'database'],
                        help='–¢–∏–ø –†–ò–î –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–∞—Ä—Å—è—Ç—Å—è –≤—Å–µ)')
        parser.add_argument('--dry-run', action='store_true', help='–†–µ–∂–∏–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î')
        parser.add_argument('--encoding', type=str, default='utf-8', help='–ö–æ–¥–∏—Ä–æ–≤–∫–∞ CSV —Ñ–∞–π–ª–∞')
        parser.add_argument('--delimiter', type=str, default=',', help='–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV —Ñ–∞–π–ª–µ')
        parser.add_argument('--batch-size', type=int, default=100, help='–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è bulk-–æ–ø–µ—Ä–∞—Ü–∏–π')
        parser.add_argument('--min-year', type=int, default=2000, help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏')
        parser.add_argument('--skip-filters', action='store_true', help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏)')
        parser.add_argument('--only-active', action='store_true', help='–ü–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç—ã (actual = True)')
        parser.add_argument('--max-rows', type=int, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)')
        parser.add_argument('--force', action='store_true', help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–∂–µ –µ—Å–ª–∏ –∫–∞—Ç–∞–ª–æ–≥ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω')
        parser.add_argument('--mark-processed', action='store_true',
                        help='–ü–æ–º–µ—Ç–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (–¥–∞–∂–µ –µ—Å–ª–∏ –±—ã–ª–∏ –æ—à–∏–±–∫–∏)')

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.parsers = {
            'invention': InventionParser(self),
            'utility-model': UtilityModelParser(self),
            'industrial-design': IndustrialDesignParser(self),
            'integrated-circuit-topology': IntegratedCircuitTopologyParser(self),
            'computer-program': ComputerProgramParser(self),
            'database': DatabaseParser(self),
        }

    def handle(self, *args, **options):
        self.dry_run = options['dry_run']
        self.encoding = options['encoding']
        self.delimiter = options['delimiter']
        self.batch_size = options['batch_size']
        self.min_year = options['min_year']
        self.skip_filters = options['skip_filters']
        self.only_active = options['only_active']
        self.max_rows = options.get('max_rows')
        self.force = options.get('force', False)
        self.mark_processed = options.get('mark_processed', False)

        if self.dry_run:
            self.stdout.write(self.style.WARNING("\nüîç –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î\n"))

        if self.only_active:
            self.stdout.write(self.style.WARNING("üìå –†–µ–∂–∏–º: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (actual = True)"))

        if self.force:
            self.stdout.write(self.style.WARNING("‚ö†Ô∏è  –†–µ–∂–∏–º: –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏)"))

        catalogues = self.get_catalogues(options.get('catalogue_id'), options.get('ip_type'))

        if not catalogues:
            raise CommandError('–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–∞—Ç–∞–ª–æ–≥–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')

        total_stats = {
            'catalogues': len(catalogues),
            'processed': 0,
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        for catalogue in catalogues:
            self.stdout.write(self.style.SUCCESS(f"\n{'='*60}"))
            self.stdout.write(self.style.SUCCESS(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞: {catalogue.name}"))
            self.stdout.write(self.style.SUCCESS(f"   ID: {catalogue.id}, –¢–∏–ø: {catalogue.ip_type.name if catalogue.ip_type else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'}"))
            self.stdout.write(self.style.SUCCESS(f"{'='*60}"))

            stats = self.process_catalogue(catalogue)

            for key in ['processed', 'created', 'updated', 'skipped', 'errors']:
                total_stats[key] += stats.get(key, 0)
            total_stats['skipped_by_date'] += stats.get('skipped_by_date', 0)

        self.print_final_stats(total_stats)

    def get_catalogues(self, catalogue_id=None, ip_type_slug=None):
        queryset = FipsOpenDataCatalogue.objects.all()

        if catalogue_id:
            queryset = queryset.filter(id=catalogue_id)
        elif ip_type_slug:
            queryset = queryset.filter(ip_type__slug=ip_type_slug)
        else:
            queryset = queryset.exclude(catalogue_file='')

        return queryset.order_by('ip_type__id', '-publication_date')

    def process_catalogue(self, catalogue):
        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        if not catalogue.catalogue_file:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –£ –∫–∞—Ç–∞–ª–æ–≥–∞ ID={catalogue.id} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª"))
            stats['errors'] += 1
            return stats

        if not self.force and hasattr(catalogue, 'parsed_date') and catalogue.parsed_date:
            self.stdout.write(self.style.WARNING(
                f"  ‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ —É–∂–µ –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω {catalogue.parsed_date.strftime('%d.%m.%Y %H:%M')}"
            ))
            self.stdout.write(self.style.WARNING(f"     –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --force –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"))
            stats['skipped'] += 1
            return stats

        ip_type_slug = catalogue.ip_type.slug if catalogue.ip_type else None

        if ip_type_slug not in self.parsers:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –ù–µ—Ç –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è —Ç–∏–ø–∞ –†–ò–î: {ip_type_slug}"))
            stats['errors'] += 1
            return stats

        parser = self.parsers[ip_type_slug]
        df = self.load_csv(catalogue)

        if df is None or df.empty:
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å"))
            stats['skipped'] += 1
            return stats

        self.stdout.write(f"  üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")

        missing_columns = self.check_required_columns(df, parser.get_required_columns())
        if missing_columns:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}"))
            stats['errors'] += 1
            return stats

        if not self.skip_filters:
            df = apply_filters(df, self.min_year, self.only_active, self.stdout)

        if df.empty:
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"))
            stats['skipped'] += 1
            return stats

        self.stdout.write(f"  üìä –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df)} –∑–∞–ø–∏—Å–µ–π")

        if self.max_rows and len(df) > self.max_rows:
            df = df.head(self.max_rows)
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {self.max_rows} –∑–∞–ø–∏—Å–µ–π"))

        try:
            parser_stats = parser.parse_dataframe(df, catalogue)
            stats.update(parser_stats)

            if not self.dry_run and hasattr(catalogue, 'parsed_date'):
                if stats['errors'] == 0 or self.mark_processed:
                    catalogue.parsed_date = timezone.now()
                    catalogue.save(update_fields=['parsed_date'])
                    self.stdout.write(self.style.SUCCESS(f"  ‚úÖ –ö–∞—Ç–∞–ª–æ–≥ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π"))
                else:
                    self.stdout.write(self.style.WARNING(
                        f"  ‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ –Ω–µ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫"
                    ))

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}"))
            logger.error(f"Error parsing catalogue {catalogue.id}: {e}", exc_info=True)
            stats['errors'] += 1

        return stats

    def load_csv(self, catalogue):
        file_path = catalogue.catalogue_file.path

        if not os.path.exists(file_path):
            self.stdout.write(self.style.ERROR(f"  ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"))
            return None

        df = load_csv_with_strategies(file_path, self.encoding, self.delimiter, self.stdout)
        return df

    def check_required_columns(self, df, required_columns):
        missing = [col for col in required_columns if col not in df.columns]
        return missing

    def print_final_stats(self, stats):
        self.stdout.write(self.style.SUCCESS(f"\n{'='*60}"))
        self.stdout.write(self.style.SUCCESS("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê"))
        self.stdout.write(self.style.SUCCESS(f"{'='*60}"))
        self.stdout.write(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞—Ç–∞–ª–æ–≥–æ–≤: {stats['catalogues']}")
        self.stdout.write(f"üìù –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}")
        self.stdout.write(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ: {stats['created']}")
        self.stdout.write(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}")
        self.stdout.write(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ –≤—Å–µ–≥–æ: {stats['skipped']}")
        self.stdout.write(f"   ‚îî‚îÄ –ø–æ –¥–∞—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {stats.get('skipped_by_date', 0)}")

        if stats['errors'] > 0:
            self.stdout.write(self.style.ERROR(f"‚ùå –û—à–∏–±–æ–∫: {stats['errors']}"))
        else:
            self.stdout.write(self.style.SUCCESS(f"‚úÖ –û—à–∏–±–æ–∫: {stats['errors']}"))

        if self.dry_run:
            self.stdout.write(self.style.WARNING("\nüîç –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î"))

        self.stdout.write(self.style.SUCCESS(f"{'='*60}"))
```


-----

# –§–∞–π–ª: management\commands\pars_fips_catalogue_archive.py

```
"""
–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ —Ç–∏–ø—ã –†–ò–î: –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è, –ø–æ–ª–µ–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏, –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã,
—Ç–æ–ø–æ–ª–æ–≥–∏–∏ –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º, –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –≠–í–ú –∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.
"""

import logging
import re
from datetime import datetime
from typing import Optional, Tuple, List, Dict, Any, Set
from collections import defaultdict

from django.db import models
from django.core.management.base import BaseCommand, CommandError
from django.utils.text import slugify
from django.utils import timezone
from tqdm import tqdm
import pandas as pd
import os

# –ò–º–ø–æ—Ä—Ç—ã natasha
from natasha import (
    Segmenter,
    MorphVocab,
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    NewsNERTagger,
    Doc,
    NamesExtractor
)

from intellectual_property.models import (
    FipsOpenDataCatalogue, IPType, ProtectionDocumentType,
    IPObject, AdditionalPatent, IPImage
)
from core.models import (
    City, Region, District, Person, Organization,
    FOIV, Country, RFRepresentative,
    OrganizationNormalizationRule, ActivityType, CeoPosition
)
from common.utils.text import TextUtils
from common.utils.dates import DateUtils

logger = logging.getLogger(__name__)


class RussianTextProcessor:
    """
    –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º natasha
    """

    # –°–ø–∏—Å–æ–∫ —Ä–∏–º—Å–∫–∏—Ö —Ü–∏—Ñ—Ä
    ROMAN_NUMERALS = {
        'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X',
        'XI', 'XII', 'XIII', 'XIV', 'XV', 'XVI', 'XVII', 'XVIII', 'XIX', 'XX',
        'XXI', 'XXII', 'XXIII', 'XXIV', 'XXV', 'XXX', 'XL', 'L', 'LX', 'XC',
        'C', 'CD', 'D', 'DC', 'CM', 'M'
    }

    # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
    ORG_ABBR = {
        '–û–û–û', '–ó–ê–û', '–û–ê–û', '–ê–û', '–ü–ê–û', '–ù–ê–û',
        '–§–ì–£–ü', '–§–ì–ë–£', '–§–ì–ê–û–£', '–§–ì–ê–£', '–§–ì–ö–£',
        '–ù–ò–ò', '–ö–ë', '–û–ö–ë', '–°–ö–ë', '–¶–ö–ë', '–ü–ö–ë',
        '–ù–ü–û', '–ù–ü–ü', '–ù–ü–§', '–ù–ü–¶', '–ù–ò–¶',
        '–ú–£–ü', '–ì–£–ü', '–ò–ß–ü', '–¢–û–û', '–ê–û–ó–¢', '–ê–û–û–¢',
        '–†–§', '–†–ê–ù', '–°–û –†–ê–ù', '–£—Ä–û –†–ê–ù', '–î–í–û –†–ê–ù',
        '–ú–ì–£', '–°–ü–±–ì–£', '–ú–§–¢–ò', '–ú–ò–§–ò', '–ú–ì–¢–£', '–ú–ê–ò',
        '–õ–¢–î', '–ò–ù–ö', '–ö–û', '–ì–ú–ë–•', '–ê–ì', '–°–ê', '–ù–í', '–ë–í', '–°–ï',
        '–ö–æ', 'Ltd', 'Inc', 'GmbH', 'AG', 'SA', 'NV', 'BV', 'SE',
    }

    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ natasha
        self.segmenter = Segmenter()
        self.morph_vocab = MorphVocab()
        self.emb = NewsEmbedding()
        self.morph_tagger = NewsMorphTagger(self.emb)
        self.syntax_parser = NewsSyntaxParser(self.emb)
        self.ner_tagger = NewsNERTagger(self.emb)
        self.names_extractor = NamesExtractor(self.morph_vocab)

        # –ö—ç—à–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.doc_cache = {}
        self.morph_cache = {}

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã –≤ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        self.ORG_ABBR.update(self.ROMAN_NUMERALS)

    def get_doc(self, text: str) -> Optional[Doc]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not text:
            return None

        if text in self.doc_cache:
            return self.doc_cache[text]

        doc = Doc(text)
        doc.segment(self.segmenter)
        doc.tag_morph(self.morph_tagger)
        doc.parse_syntax(self.syntax_parser)
        doc.tag_ner(self.ner_tagger)

        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
        for token in doc.tokens:
            token.lemmatize(self.morph_vocab)

        for span in doc.spans:
            span.normalize(self.morph_vocab)

        self.doc_cache[text] = doc
        return doc

    def is_roman_numeral(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∏–º—Å–∫—É—é —Ü–∏—Ñ—Ä—É"""
        if not text:
            return False
        clean_text = text.strip('.,;:!?()').upper()
        return clean_text in self.ROMAN_NUMERALS

    def is_abbr(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        if not text:
            return False
        clean_text = text.strip('.,;:!?()').upper()
        return clean_text in self.ORG_ABBR

    def is_person(self, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –§–ò–û —á–µ–ª–æ–≤–µ–∫–∞"""
        if not text or len(text) < 6:
            return False

        # –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        if any(ind in text for ind in self.ORG_ABBR if len(ind) > 2):
            return False

        org_indicators = ['–û–±—â–µ—Å—Ç–≤–æ', '–ö–æ–º–ø–∞–Ω–∏—è', '–ö–æ—Ä–ø–æ—Ä–∞—Ü–∏—è', '–ó–∞–≤–æ–¥',
                         '–ò–Ω—Å—Ç–∏—Ç—É—Ç', '–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '–ê–∫–∞–¥–µ–º–∏—è', '–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è',
                         '–§–∏—Ä–º–∞', '–¶–µ–Ω—Ç—Ä']

        if any(ind.lower() in text.lower() for ind in org_indicators):
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ NER
        doc = self.get_doc(text)
        if doc and doc.spans:
            for span in doc.spans:
                if span.type == 'PER':
                    return True

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –§–ò–û
        words = text.split()
        if 2 <= len(words) <= 4:
            name_like = 0
            for word in words:
                clean = word.rstrip('.,')
                if clean and clean[0].isupper() and len(clean) > 1:
                    name_like += 1
            return name_like >= len(words) - 1

        return False

    def extract_person_parts(self, text: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π –§–ò–û —Å –ø–æ–º–æ—â—å—é natasha"""
        matches = list(self.names_extractor(text))
        if matches:
            fact = matches[0].fact
            parts = []
            if fact.last:
                parts.append(fact.last)
            if fact.first:
                parts.append(fact.first)
            if fact.middle:
                parts.append(fact.middle)

            return {
                'last': fact.last or '',
                'first': fact.first or '',
                'middle': fact.middle or '',
                'full': ' '.join(parts)
            }

        # Fallback: —Ä—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥
        return self._parse_name_manually(text)

    def _parse_name_manually(self, text: str) -> Dict[str, str]:
        """–†—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –∏–º–µ–Ω–∏"""
        words = text.split()

        if len(words) == 3:
            return {
                'last': words[0],
                'first': words[1],
                'middle': words[2],
                'full': text
            }
        elif len(words) == 2:
            return {
                'last': words[0],
                'first': words[1],
                'middle': '',
                'full': text
            }
        else:
            return {
                'last': text,
                'first': '',
                'middle': '',
                'full': text
            }

    def format_person_name(self, name: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –§–ò–û —á–µ–ª–æ–≤–µ–∫–∞"""
        if not name:
            return name

        parts = self.extract_person_parts(name)
        if parts.get('full'):
            return parts['full']

        return name


class OrganizationNormalizer:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)"""

    def __init__(self):
        self.rules_cache = None
        self.processor = RussianTextProcessor()
        self.load_rules()

    def load_rules(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∏–∑ –ë–î"""
        try:
            rules = OrganizationNormalizationRule.objects.all().order_by('priority')
            self.rules_cache = [
                {
                    'original': rule.original_text.lower(),
                    'replacement': rule.replacement_text.lower(),
                    'type': rule.rule_type,
                    'priority': rule.priority
                }
                for rule in rules
            ]
        except Exception as e:
            self.rules_cache = []
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")

    def normalize_for_search(self, name: str) -> Dict[str, Any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –¢–û–õ–¨–ö–û –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        –°–∞–º–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –≤ CSV
        """
        if pd.isna(name) or not name:
            return {'normalized': '', 'keywords': [], 'original': name}

        original = str(name).strip()
        name_lower = original.lower()

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ –ë–î –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        normalized = name_lower
        if self.rules_cache:
            for rule in self.rules_cache:
                try:
                    if rule['type'] == 'ignore':
                        pattern = r'\b' + re.escape(rule['original']) + r'\b'
                        normalized = re.sub(pattern, '', normalized)
                    else:
                        pattern = r'\b' + re.escape(rule['original']) + r'\b'
                        normalized = re.sub(pattern, rule['replacement'], normalized)
                except Exception:
                    continue

        # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞
        normalized = re.sub(r'["\'¬´¬ª‚Äû‚Äú‚Äù]', '', normalized)
        normalized = re.sub(r'[^\w\s-]', ' ', normalized)
        normalized = ' '.join(normalized.split())

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        keywords = []

        # –°–ª–æ–≤–∞ –≤ –∫–∞–≤—ã—á–∫–∞—Ö
        quoted = re.findall(r'"([^"]+)"', original)
        for q in quoted:
            words = q.lower().split()
            keywords.extend([w for w in words if len(w) > 3])

        # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        abbrs = re.findall(r'\b[–ê-–Ø–ÅA-Z]{2,}\b', original)
        keywords.extend([a.lower() for a in abbrs if len(a) >= 2])

        # –ö–æ–¥—ã (–ò–ù–ù, –û–ì–†–ù –∏ —Ç.–¥.)
        codes = re.findall(r'\b\d{10,}\b', original)
        keywords.extend(codes)

        return {
            'normalized': normalized,
            'keywords': list(set(keywords)),
            'original': original,
        }

    def format_organization_name(self, name: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        return name


class PersonNameFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω –ª—é–¥–µ–π"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def format(self, name: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –§–ò–û"""
        return self.processor.format_person_name(name)


class RIDNameFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –†–ò–î"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def format(self, text: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –†–ò–î"""
        if not text or not isinstance(text, str):
            return text

        if len(text.strip()) <= 1:
            return text

        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ –¥–µ–ª–∞–µ–º –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –∑–∞–≥–ª–∞–≤–Ω–æ–π
        words = text.lower().split()
        if words:
            words[0] = words[0][0].upper() + words[0][1:]
        return ' '.join(words)


class EntityTypeDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def detect_type(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏"""
        if self.processor.is_person(text):
            return 'person'
        return 'organization'


class BaseFIPSParser:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –§–ò–ü–°"""

    def __init__(self, command):
        self.command = command
        self.stdout = command.stdout
        self.style = command.style

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
        self.processor = RussianTextProcessor()
        self.org_normalizer = OrganizationNormalizer()
        self.type_detector = EntityTypeDetector()
        self.person_formatter = PersonNameFormatter()
        self.rid_formatter = RIDNameFormatter()

        # –ö—ç—à–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.country_cache = {}
        self.person_cache = {}
        self.organization_cache = {}
        self.foiv_cache = {}
        self.rf_rep_cache = {}
        self.city_cache = {}
        self.activity_type_cache = {}
        self.ceo_position_cache = {}

    def get_ip_type(self):
        """–î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –¥–æ—á–µ—Ä–Ω–∏—Ö –∫–ª–∞—Å—Å–∞—Ö"""
        raise NotImplementedError

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
        raise NotImplementedError

    def parse_dataframe(self, df, catalogue):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame"""
        raise NotImplementedError

    def clean_string(self, value):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if pd.isna(value) or value is None:
            return ''
        value = str(value).strip()
        if value in ['', 'None', 'null', 'NULL', 'nan']:
            return ''
        return value

    def parse_date(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        if pd.isna(value) or not value:
            return None

        date_str = str(value).strip()
        if not date_str:
            return None

        for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
            try:
                return datetime.strptime(date_str, fmt).date()
            except (ValueError, TypeError):
                continue

        try:
            return pd.to_datetime(date_str).date()
        except (ValueError, TypeError):
            return None

    def parse_bool(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –±—É–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if pd.isna(value) or not value:
            return False
        value = str(value).lower().strip()
        return value in ['1', 'true', 'yes', '–¥–∞', '–¥–µ–π—Å—Ç–≤—É–µ—Ç', 't', '1.0', '–∞–∫—Ç–∏–≤–µ–Ω']

    def get_or_create_country(self, code):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –ø–æ –∫–æ–¥—É"""
        if not code or pd.isna(code):
            return None

        code = str(code).upper().strip()
        if len(code) != 2:
            return None

        if code in self.country_cache:
            return self.country_cache[code]

        try:
            country = Country.objects.filter(code=code).first()
            if country:
                self.country_cache[code] = country
                return country

            country = Country.objects.filter(code_alpha3=code).first()
            if country:
                self.country_cache[code] = country
                return country

            return None

        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω—ã {code}: {e}"))
            return None

    def parse_authors(self, authors_str):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –∞–≤—Ç–æ—Ä–∞–º–∏"""
        if pd.isna(authors_str) or not authors_str:
            return []

        authors_str = str(authors_str)
        authors_list = re.split(r'[\n,]\s*', authors_str)

        result = []
        for author in authors_list:
            author = author.strip()
            if not author or author == '""' or author == 'null':
                continue

            author = author.strip('"')
            author = re.sub(r'\s*\([A-Z]{2}\)', '', author)
            author = self.person_formatter.format(author)

            parts = author.split()

            if len(parts) >= 2:
                last_name = parts[0]
                first_name = parts[1] if len(parts) > 1 else ''
                middle_name = parts[2] if len(parts) > 2 else ''

                first_name_clean = first_name.replace('.', '')
                middle_name_clean = middle_name.replace('.', '')

                result.append({
                    'last_name': last_name,
                    'first_name': first_name_clean,
                    'middle_name': middle_name_clean,
                    'full_name': author,
                })
            else:
                result.append({
                    'last_name': author,
                    'first_name': '',
                    'middle_name': '',
                    'full_name': author,
                })

        return result

    def parse_patent_holders(self, holders_str):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—è–º–∏"""
        if pd.isna(holders_str) or not holders_str:
            return []

        holders_str = str(holders_str)
        holders_list = re.split(r'[\n]\s*', holders_str)

        result = []
        for holder in holders_list:
            holder = holder.strip().strip('"')
            if not holder or holder == 'null' or holder == 'None':
                continue

            holder = re.sub(r'\s*\([A-Z]{2}\)', '', holder)
            result.append(holder)

        return result

    def find_or_create_person(self, person_data):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞"""
        cache_key = f"{person_data['last_name']}|{person_data['first_name']}|{person_data['middle_name']}"

        if cache_key in self.person_cache:
            return self.person_cache[cache_key]

        persons = Person.objects.filter(
            last_name=person_data['last_name'],
            first_name=person_data['first_name']
        )

        if person_data['middle_name']:
            persons = persons.filter(middle_name=person_data['middle_name'])

        if persons.exists():
            person = persons.first()
            self.person_cache[cache_key] = person
            return person

        try:
            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
            new_id = max_id + 1

            if 'full_name' in person_data:
                full_name = person_data['full_name']
            else:
                full_name_parts = [person_data['last_name'], person_data['first_name']]
                if person_data['middle_name']:
                    full_name_parts.append(person_data['middle_name'])
                full_name = ' '.join(full_name_parts)
                full_name = self.person_formatter.format(full_name)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π slug
            base_slug = slugify(f"{person_data['last_name']} {person_data['first_name']} {person_data['middle_name']}".strip())
            if not base_slug:
                base_slug = 'person'

            unique_slug = base_slug
            counter = 1
            while Person.objects.filter(slug=unique_slug).exists():
                unique_slug = f"{base_slug}-{counter}"
                counter += 1

            person = Person.objects.create(
                ceo_id=new_id,
                ceo=full_name,
                last_name=person_data['last_name'],
                first_name=person_data['first_name'],
                middle_name=person_data['middle_name'],
                slug=unique_slug
            )
            self.person_cache[cache_key] = person
            return person
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Person: {e}"))
            return None

    def find_or_create_person_from_name(self, full_name):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞ –ø–æ –ø–æ–ª–Ω–æ–º—É –∏–º–µ–Ω–∏"""
        if pd.isna(full_name) or not full_name:
            return None

        full_name = str(full_name).strip().strip('"')
        full_name = self.person_formatter.format(full_name)

        if full_name in self.person_cache:
            return self.person_cache[full_name]

        parts = full_name.split()

        if len(parts) >= 2:
            last_name = parts[0]
            first_name = parts[1] if len(parts) > 1 else ''
            middle_name = parts[2] if len(parts) > 2 else ''

            first_name_clean = first_name.replace('.', '')
            middle_name_clean = middle_name.replace('.', '')

            person_data = {
                'last_name': last_name,
                'first_name': first_name_clean,
                'middle_name': middle_name_clean,
                'full_name': full_name,
            }
        else:
            person_data = {
                'last_name': full_name,
                'first_name': '',
                'middle_name': '',
                'full_name': full_name,
            }

        return self.find_or_create_person(person_data)

    def find_similar_organization(self, org_name):
        """–£—Å–∏–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        if pd.isna(org_name) or not org_name:
            return None

        org_name = str(org_name).strip().strip('"')

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        direct_match = Organization.objects.filter(
            models.Q(name=org_name) |
            models.Q(full_name=org_name) |
            models.Q(short_name=org_name)
        ).first()
        if direct_match:
            return direct_match

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
        norm_data = self.org_normalizer.normalize_for_search(org_name)
        normalized = norm_data['normalized']
        keywords = norm_data['keywords']

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in keywords:
            if len(keyword) >= 3:
                similar = Organization.objects.filter(
                    models.Q(name__icontains=keyword) |
                    models.Q(full_name__icontains=keyword) |
                    models.Q(short_name__icontains=keyword)
                ).first()
                if similar:
                    return similar

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ –ø–æ –ø–µ—Ä–≤—ã–º 30 —Å–∏–º–≤–æ–ª–∞–º
        if len(normalized) > 30:
            prefix = normalized[:30]
            similar = Organization.objects.filter(
                models.Q(name__icontains=prefix) |
                models.Q(full_name__icontains=prefix) |
                models.Q(short_name__icontains=prefix)
            ).first()
            if similar:
                return similar

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ü–æ–∏—Å–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–ª–æ–≤–∞–º
        words = org_name.split()
        for word in words:
            if len(word) > 4:
                similar = Organization.objects.filter(
                    models.Q(name__icontains=word) |
                    models.Q(full_name__icontains=word)
                ).first()
                if similar:
                    return similar

        return None

    def find_or_create_organization(self, org_name):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        if pd.isna(org_name) or not org_name:
            return None

        org_name = str(org_name).strip().strip('"')

        if not org_name or org_name == 'null' or org_name == 'None':
            return None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if org_name in self.organization_cache:
            return self.organization_cache[org_name]

        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ
        similar = self.find_similar_organization(org_name)
        if similar:
            self.organization_cache[org_name] = similar
            return similar

        # –ù–µ –Ω–∞—à–ª–∏ - —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º
        try:
            max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
            new_id = max_id + 1

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º slug –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
            base_slug = slugify(org_name[:50])
            if not base_slug:
                base_slug = 'organization'

            unique_slug = base_slug
            counter = 1
            while Organization.objects.filter(slug=unique_slug).exists():
                unique_slug = f"{base_slug}-{counter}"
                counter += 1

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            org = Organization.objects.create(
                organization_id=new_id,
                name=org_name,
                full_name=org_name,
                short_name=org_name[:500] if len(org_name) > 500 else org_name,
                slug=unique_slug,
                register_opk=False,
                strategic=False,
            )

            self.organization_cache[org_name] = org
            return org
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Organization: {e}"))
            return None


class InventionParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='invention').first()

    def get_required_columns(self):
        return ['registration number', 'invention name']

    def _has_data_changed(self, obj, new_data):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ"""
        fields_to_check = [
            ('name', obj.name, new_data['name']),
            ('application_date', obj.application_date, new_data['application_date']),
            ('registration_date', obj.registration_date, new_data['registration_date']),
            ('patent_starting_date', obj.patent_starting_date, new_data['patent_starting_date']),
            ('expiration_date', obj.expiration_date, new_data['expiration_date']),
            ('actual', obj.actual, new_data['actual']),
            ('publication_url', obj.publication_url, new_data['publication_url']),
            ('abstract', obj.abstract, new_data['abstract']),
            ('claims', obj.claims, new_data['claims']),
            ('creation_year', obj.creation_year, new_data['creation_year']),
        ]

        for field_name, old_val, new_val in fields_to_check:
            if old_val != new_val:
                return True
        return False

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π..."))

        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'invention' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats

        # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Ç–∞–ª–æ–≥–∞
        upload_date = catalogue.upload_date.date() if catalogue.upload_date else None

        # –®–ê–ì 1: –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞
        self.stdout.write("  üì• –ß—Ç–µ–Ω–∏–µ CSV...")
        all_reg_numbers = []
        reg_num_to_row = {}

        with tqdm(total=len(df), desc="     –ü—Ä–æ–≥—Ä–µ—Å—Å", unit=" –∑–∞–ø", leave=False) as pbar:
            for idx, row in df.iterrows():
                reg_num = self.clean_string(row.get('registration number'))
                if reg_num:
                    all_reg_numbers.append(reg_num)
                    reg_num_to_row[reg_num] = row
                pbar.update(1)

        self.stdout.write(f"  üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: {len(all_reg_numbers)}")

        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –ü–ê–ß–ö–ê–ú–ò
        self.stdout.write("  üîç –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î...")
        existing_objects = {}
        batch_size = 500

        with tqdm(total=len(all_reg_numbers), desc="     –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—á–∫–∞–º–∏", unit=" –∑–∞–ø") as pbar:
            for i in range(0, len(all_reg_numbers), batch_size):
                batch_numbers = all_reg_numbers[i:i+batch_size]

                for obj in IPObject.objects.filter(
                    registration_number__in=batch_numbers,
                    ip_type=ip_type
                ).select_related('ip_type'):
                    existing_objects[obj.registration_number] = obj

                pbar.update(len(batch_numbers))

                if (i // batch_size) % 10 == 0:
                    pbar.set_postfix({"–Ω–∞–π–¥–µ–Ω–æ": len(existing_objects)})

        self.stdout.write(f"  üìä –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(existing_objects)}")

        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.stdout.write("  üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        to_create = []
        to_update = []
        skipped_by_date = []
        unchanged_count = 0
        error_reg_numbers = []

        authors_cache = defaultdict(list)
        holders_cache = defaultdict(list)

        with tqdm(total=len(reg_num_to_row), desc="     –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–µ–π", unit=" –∑–∞–ø") as pbar:
            for reg_num, row in reg_num_to_row.items():
                try:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –¥–∞—Ç–µ
                    if not self.command.force and upload_date and reg_num in existing_objects:
                        existing = existing_objects[reg_num]
                        if existing.updated_at and existing.updated_at.date() >= upload_date:
                            skipped_by_date.append(reg_num)
                            pbar.update(1)
                            continue

                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                    name = self.clean_string(row.get('invention name'))
                    if name:
                        name = self.rid_formatter.format(name)
                    else:
                        name = f"–ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ ‚Ññ{reg_num}"

                    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã
                    application_date = self.parse_date(row.get('application date'))
                    registration_date = self.parse_date(row.get('registration date'))
                    patent_starting_date = self.parse_date(row.get('patent starting date'))
                    expiration_date = self.parse_date(row.get('expiration date'))
                    actual = self.parse_bool(row.get('actual'))
                    publication_url = self.clean_string(row.get('publication URL'))
                    abstract = self.clean_string(row.get('abstract'))
                    claims = self.clean_string(row.get('claims'))

                    creation_year = None
                    if application_date:
                        creation_year = application_date.year
                    elif registration_date:
                        creation_year = registration_date.year

                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                    obj_data = {
                        'registration_number': reg_num,
                        'ip_type': ip_type,
                        'name': name,
                        'application_date': application_date,
                        'registration_date': registration_date,
                        'patent_starting_date': patent_starting_date,
                        'expiration_date': expiration_date,
                        'actual': actual,
                        'publication_url': publication_url,
                        'abstract': abstract,
                        'claims': claims,
                        'creation_year': creation_year,
                    }

                    if reg_num in existing_objects:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
                        existing = existing_objects[reg_num]
                        if self._has_data_changed(existing, obj_data):
                            to_update.append(obj_data)
                        else:
                            unchanged_count += 1
                    else:
                        to_create.append(obj_data)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–≤—Ç–æ—Ä–æ–≤ –∏ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
                    authors_str = row.get('authors')
                    if not pd.isna(authors_str) and authors_str:
                        authors_cache[reg_num] = self.parse_authors(authors_str)

                    holders_str = row.get('patent holders')
                    if not pd.isna(holders_str) and holders_str:
                        holders_cache[reg_num] = self.parse_patent_holders(holders_str)

                except Exception as e:
                    error_reg_numbers.append(reg_num)
                    self.stdout.write(self.style.ERROR(f"\n  ‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    logger.error(f"Error preparing invention {reg_num}: {e}", exc_info=True)

                pbar.update(1)
                if pbar.n % 1000 == 0:
                    pbar.set_postfix({
                        "–Ω–æ–≤—ã–µ": len(to_create),
                        "–æ–±–Ω–æ–≤": len(to_update),
                        "–±–µ–∑ –∏–∑–º": unchanged_count,
                        "–ø—Ä–æ–ø—É—â": len(skipped_by_date)
                    })

        stats['skipped_by_date'] = len(skipped_by_date)
        stats['skipped'] += len(skipped_by_date)
        stats['errors'] = len(error_reg_numbers)
        stats['unchanged'] = unchanged_count

        self.stdout.write(f"     –ò—Ç–æ–≥–æ: –Ω–æ–≤—ã—Ö={len(to_create)}, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ={len(to_update)}, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={unchanged_count}")

        # –®–ê–ì 4: –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
        if to_create and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –°–æ–∑–¥–∞–Ω–∏–µ {len(to_create)} –∑–∞–ø–∏—Å–µ–π...")
            create_objects = [IPObject(**data) for data in to_create]

            batch_size = 1000
            created_count = 0

            with tqdm(total=len(create_objects), desc="     –°–æ–∑–¥–∞–Ω–∏–µ", unit=" –∑–∞–ø") as pbar:
                for i in range(0, len(create_objects), batch_size):
                    batch = create_objects[i:i+batch_size]
                    IPObject.objects.bulk_create(batch, batch_size=batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))
                    pbar.set_postfix({"—Å–æ–∑–¥–∞–Ω–æ": created_count})

            stats['created'] = created_count

            # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –Ω–æ–≤—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏
            self.stdout.write("     –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞...")

            with tqdm(total=len(to_create), desc="     –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞", unit=" –∑–∞–ø") as pbar:
                for i in range(0, len(to_create), batch_size):
                    batch_data = to_create[i:i+batch_size]
                    batch_nums = [d['registration_number'] for d in batch_data]

                    for obj in IPObject.objects.filter(
                        registration_number__in=batch_nums,
                        ip_type=ip_type
                    ):
                        existing_objects[obj.registration_number] = obj

                    pbar.update(len(batch_data))

        # –®–ê–ì 5: –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π
        if to_update and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {len(to_update)} –∑–∞–ø–∏—Å–µ–π...")
            updated_count = 0

            with tqdm(total=len(to_update), desc="     –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ", unit=" –∑–∞–ø") as pbar:
                for data in to_update:
                    obj = existing_objects[data['registration_number']]
                    update_fields = []

                    if obj.name != data['name']:
                        obj.name = data['name']
                        update_fields.append('name')

                    if obj.application_date != data['application_date']:
                        obj.application_date = data['application_date']
                        update_fields.append('application_date')

                    if obj.registration_date != data['registration_date']:
                        obj.registration_date = data['registration_date']
                        update_fields.append('registration_date')

                    if obj.patent_starting_date != data['patent_starting_date']:
                        obj.patent_starting_date = data['patent_starting_date']
                        update_fields.append('patent_starting_date')

                    if obj.expiration_date != data['expiration_date']:
                        obj.expiration_date = data['expiration_date']
                        update_fields.append('expiration_date')

                    if obj.actual != data['actual']:
                        obj.actual = data['actual']
                        update_fields.append('actual')

                    if obj.publication_url != data['publication_url']:
                        obj.publication_url = data['publication_url']
                        update_fields.append('publication_url')

                    if obj.abstract != data['abstract']:
                        obj.abstract = data['abstract']
                        update_fields.append('abstract')

                    if obj.claims != data['claims']:
                        obj.claims = data['claims']
                        update_fields.append('claims')

                    if obj.creation_year != data['creation_year']:
                        obj.creation_year = data['creation_year']
                        update_fields.append('creation_year')

                    if update_fields:
                        obj.save(update_fields=update_fields)
                        updated_count += 1

                    pbar.update(1)
                    if pbar.n % 100 == 0:
                        pbar.set_postfix({"–æ–±–Ω–æ–≤–ª–µ–Ω–æ": updated_count})

            stats['updated'] = updated_count
            self.stdout.write(f"     –†–µ–∞–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ: {updated_count} –∏–∑ {len(to_update)}")

        # –®–ê–ì 6: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤
        if authors_cache and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ ({len(authors_cache)} –∑–∞–ø–∏—Å–µ–π)...")
            self._process_authors_batch_with_progress(existing_objects, authors_cache)

        # –®–ê–ì 7: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
        if holders_cache and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π ({len(holders_cache)} –∑–∞–ø–∏—Å–µ–π)...")
            self._process_holders_batch_with_progress(existing_objects, holders_cache)

        stats['processed'] = len(df) - stats['skipped'] - stats['errors']

        self.stdout.write(self.style.SUCCESS(f"  ‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"     –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats['unchanged']}, "
                         f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –≤—Å–µ–≥–æ: {stats['skipped']} (–∏–∑ –Ω–∏—Ö –ø–æ –¥–∞—Ç–µ: {stats['skipped_by_date']}), "
                         f"–û—à–∏–±–æ–∫: {stats['errors']}")

        return stats

    def _process_authors_batch_with_progress(self, existing_objects, authors_cache):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –∏ —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ø–∞—á–∫–∏"""
        self.stdout.write(f"     ‚ö° –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤...")

        # –®–ê–ì 1: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
        self.stdout.write("        –®–∞–≥ 1/6: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤...")
        author_to_key = {}
        total_relations = 0

        for reg_num, authors_data in authors_cache.items():
            ip_object = existing_objects.get(reg_num)
            if not ip_object:
                continue

            for author_data in authors_data:
                key = f"{author_data['last_name']}|{author_data['first_name']}|{author_data['middle_name']}"
                if key not in author_to_key:
                    author_to_key[key] = {
                        'data': author_data,
                        'ip_objects': []
                    }
                author_to_key[key]['ip_objects'].append(ip_object)
                total_relations += 1

        all_keys = list(author_to_key.keys())
        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤: {len(all_keys)}, –≤—Å–µ–≥–æ —Å–≤—è–∑–µ–π: {total_relations}")

        # –®–ê–ì 2: –ü–æ–∏—Å–∫ –≤ –ë–î
        self.stdout.write("        –®–∞–≥ 2/6: –ü–æ–∏—Å–∫ –≤ –ë–î...")
        existing_people = {}
        batch_size = 50

        with tqdm(total=len(all_keys), desc="           –ü–æ–∏—Å–∫", unit=" –∫–ª—é—á") as pbar:
            for i in range(0, len(all_keys), batch_size):
                batch_keys = all_keys[i:i+batch_size]

                name_conditions = models.Q()
                for key in batch_keys:
                    last, first, middle = key.split('|')
                    if middle:
                        name_conditions |= models.Q(
                            last_name=last,
                            first_name=first,
                            middle_name=middle
                        )
                    else:
                        name_conditions |= models.Q(
                            last_name=last,
                            first_name=first,
                            middle_name__isnull=True
                        ) | models.Q(
                            last_name=last,
                            first_name=first,
                            middle_name=''
                        )

                for person in Person.objects.filter(name_conditions):
                    key = f"{person.last_name}|{person.first_name}|{person.middle_name or ''}"
                    existing_people[key] = person
                    self.person_cache[key] = person

                pbar.update(len(batch_keys))
                if (i // batch_size) % 10 == 0:
                    pbar.set_postfix({"–Ω–∞–π–¥–µ–Ω–æ": len(existing_people)})

        self.stdout.write(f"        –ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {len(existing_people)}")

        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
        self.stdout.write("        –®–∞–≥ 3/6: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤...")
        people_to_create = []
        key_to_new_person = {}

        max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
        next_id = max_id + 1
        existing_slugs = set(Person.objects.values_list('slug', flat=True))

        with tqdm(total=len(all_keys), desc="           –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞", unit=" –∫–ª—é—á") as pbar:
            for key, info in author_to_key.items():
                if key not in existing_people:
                    author_data = info['data']

                    name_parts = [author_data['last_name'], author_data['first_name']]
                    if author_data['middle_name']:
                        name_parts.append(author_data['middle_name'])

                    base_slug = slugify(' '.join(name_parts).strip())
                    if not base_slug:
                        base_slug = 'person'

                    unique_slug = base_slug
                    counter = 1
                    while unique_slug in existing_slugs or any(p.slug == unique_slug for p in people_to_create):
                        unique_slug = f"{base_slug}-{counter}"
                        counter += 1

                    person = Person(
                        ceo_id=next_id,
                        ceo=author_data['full_name'],
                        last_name=author_data['last_name'],
                        first_name=author_data['first_name'],
                        middle_name=author_data['middle_name'] or '',
                        slug=unique_slug
                    )
                    people_to_create.append(person)
                    key_to_new_person[key] = person
                    next_id += 1
                    existing_slugs.add(unique_slug)

                pbar.update(1)
                if pbar.n % 10000 == 0:
                    pbar.set_postfix({"–∫ —Å–æ–∑–¥–∞–Ω–∏—é": len(people_to_create)})

        self.stdout.write(f"        –ù–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {len(people_to_create)}")

        # –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
        if people_to_create:
            self.stdout.write(f"        –®–∞–≥ 4/6: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤...")
            batch_size = 500
            created_count = 0

            with tqdm(total=len(people_to_create), desc="           –°–æ–∑–¥–∞–Ω–∏–µ", unit=" —á–µ–ª") as pbar:
                for i in range(0, len(people_to_create), batch_size):
                    batch = people_to_create[i:i+batch_size]
                    Person.objects.bulk_create(batch, batch_size=batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))
                    pbar.set_postfix({"—Å–æ–∑–¥–∞–Ω–æ": created_count})

            for person in people_to_create:
                key = f"{person.last_name}|{person.first_name}|{person.middle_name}"
                self.person_cache[key] = person

        # –®–ê–ì 5: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π
        self.stdout.write("        –®–∞–≥ 5/6: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π...")
        unique_pairs = set()
        through_objs = []

        for key, info in author_to_key.items():
            person = existing_people.get(key) or key_to_new_person.get(key)
            if not person:
                continue

            unique_ip_objects = {ip.pk: ip for ip in info['ip_objects']}

            for ip_object in unique_ip_objects.values():
                pair = (ip_object.pk, person.pk)
                if pair not in unique_pairs:
                    unique_pairs.add(pair)
                    through_objs.append(
                        IPObject.authors.through(
                            ipobject_id=ip_object.pk,
                            person_id=person.pk
                        )
                    )

        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {len(through_objs)}")

        # –®–ê–ì 6: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π
        if through_objs:
            self.stdout.write(f"        –®–∞–≥ 6/6: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π...")

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID IP-–æ–±—ä–µ–∫—Ç–æ–≤
            ip_ids = list(set(obj.ipobject_id for obj in through_objs))
            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π –¥–ª—è {len(ip_ids)} IP-–æ–±—ä–µ–∫—Ç–æ–≤...")

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ –ü–ê–ß–ö–ê–ú–ò –ø–æ 500 ID
            delete_batch_size = 500
            deleted_total = 0

            for i in range(0, len(ip_ids), delete_batch_size):
                batch_ip_ids = ip_ids[i:i+delete_batch_size]
                deleted, _ = IPObject.authors.through.objects.filter(
                    ipobject_id__in=batch_ip_ids
                ).delete()
                deleted_total += deleted

                if (i // delete_batch_size) % 10 == 0:
                    self.stdout.write(f"              –£–¥–∞–ª–µ–Ω–æ {deleted_total} —Å–≤—è–∑–µ–π...")

            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π: {deleted_total}")

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏ –ø–∞—á–∫–∞–º–∏
            create_batch_size = 1000
            created_count = 0

            with tqdm(total=len(through_objs), desc="           –î–æ–±–∞–≤–ª–µ–Ω–∏–µ", unit=" —Å–≤—è–∑—å") as pbar:
                for i in range(0, len(through_objs), create_batch_size):
                    batch = through_objs[i:i+create_batch_size]
                    IPObject.authors.through.objects.bulk_create(batch, batch_size=create_batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))
                    pbar.set_postfix({"—Å–æ–∑–¥–∞–Ω–æ": created_count})

        self.stdout.write(f"        ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def _process_holders_batch_with_progress(self, existing_objects, holders_cache):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –∏ —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ø–∞—á–∫–∏"""
        self.stdout.write(f"     ‚ö° –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π...")

        # –®–ê–ì 1: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
        self.stdout.write("        –®–∞–≥ 1/7: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π...")
        all_holders = set()
        for holders_list in holders_cache.values():
            all_holders.update(holders_list)

        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π: {len(all_holders)}")

        # –®–ê–ì 2: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
        self.stdout.write("        –®–∞–≥ 2/7: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤...")
        person_holders = []
        org_holders = []

        with tqdm(total=len(all_holders), desc="           –ê–Ω–∞–ª–∏–∑", unit=" –æ–±") as pbar:
            for holder in all_holders:
                if self.type_detector.detect_type(holder) == 'person':
                    person_holders.append(holder)
                else:
                    org_holders.append(holder)
                pbar.update(1)

        self.stdout.write(f"        –õ—é–¥–∏: {len(person_holders)}, –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: {len(org_holders)}")

        # –®–ê–ì 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–ß–ê–°–¢–Ø–ú–ò)
        self.stdout.write("        –®–∞–≥ 3/7: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π...")
        org_map = {}

        if org_holders:
            CHUNK_SIZE = 1000
            total_orgs = len(org_holders)

            self.stdout.write(f"        –û–±—Ä–∞–±–æ—Ç–∫–∞ {total_orgs} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —á–∞—Å—Ç—è–º–∏ –ø–æ {CHUNK_SIZE}...")

            for chunk_start in range(0, total_orgs, CHUNK_SIZE):
                chunk_end = min(chunk_start + CHUNK_SIZE, total_orgs)
                chunk_holders = org_holders[chunk_start:chunk_end]

                # –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤ —ç—Ç–æ–π —á–∞—Å—Ç–∏
                existing_orgs = {}
                for org in Organization.objects.filter(name__in=chunk_holders):
                    existing_orgs[org.name] = org
                    self.organization_cache[org.name] = org

                # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤ —ç—Ç–æ–π —á–∞—Å—Ç–∏
                orgs_to_create = []
                for holder in chunk_holders:
                    if holder not in existing_orgs and holder not in self.organization_cache:
                        max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
                        new_id = max_id + len(orgs_to_create) + 1

                        base_slug = slugify(holder[:50])
                        if not base_slug:
                            base_slug = 'organization'

                        unique_slug = base_slug
                        counter = 1
                        while Organization.objects.filter(slug=unique_slug).exists() or any(o.slug == unique_slug for o in orgs_to_create):
                            unique_slug = f"{base_slug}-{counter}"
                            counter += 1

                        org = Organization(
                            organization_id=new_id,
                            name=holder,
                            full_name=holder,
                            short_name=holder[:500] if len(holder) > 500 else holder,
                            slug=unique_slug,
                            register_opk=False,
                            strategic=False,
                        )
                        orgs_to_create.append(org)
                        self.organization_cache[holder] = org

                # –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ —Å–æ–∑–¥–∞–µ–º –≤ –ë–î
                if orgs_to_create:
                    batch_size = 500
                    for i in range(0, len(orgs_to_create), batch_size):
                        batch = orgs_to_create[i:i+batch_size]
                        Organization.objects.bulk_create(batch, batch_size=batch_size)

                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                del existing_orgs
                del orgs_to_create

                progress = (chunk_end / total_orgs) * 100
                self.stdout.write(f"           –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%")

            # –§–∏–Ω–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
            for holder in org_holders:
                org_map[holder] = self.organization_cache.get(holder)

        # –®–ê–ì 4: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª—é–¥–µ–π (–ß–ê–°–¢–Ø–ú–ò)
        self.stdout.write("        –®–∞–≥ 4/7: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª—é–¥–µ–π...")
        person_map = {}

        if person_holders:
            CHUNK_SIZE = 500
            total_people = len(person_holders)

            self.stdout.write(f"        –û–±—Ä–∞–±–æ—Ç–∫–∞ {total_people} –ª—é–¥–µ–π —á–∞—Å—Ç—è–º–∏ –ø–æ {CHUNK_SIZE}...")

            for chunk_start in range(0, total_people, CHUNK_SIZE):
                chunk_end = min(chunk_start + CHUNK_SIZE, total_people)
                chunk_holders = person_holders[chunk_start:chunk_end]

                # –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
                existing_people = {}
                for holder in chunk_holders:
                    parts = holder.split()
                    if len(parts) >= 2:
                        last_name = parts[0]
                        first_name = parts[1]
                        middle_name = parts[2] if len(parts) > 2 else ''

                        persons = Person.objects.filter(
                            last_name=last_name,
                            first_name=first_name
                        )
                        if middle_name:
                            persons = persons.filter(middle_name=middle_name)

                        person = persons.first()
                        if person:
                            existing_people[holder] = person
                            self.person_cache[holder] = person

                # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö
                people_to_create = []
                for holder in chunk_holders:
                    if holder not in existing_people and holder not in self.person_cache:
                        parts = holder.split()
                        if len(parts) >= 2:
                            last_name = parts[0]
                            first_name = parts[1]
                            middle_name = parts[2] if len(parts) > 2 else ''

                            name_parts = [last_name, first_name]
                            if middle_name:
                                name_parts.append(middle_name)

                            base_slug = slugify(' '.join(name_parts))
                            if not base_slug:
                                base_slug = 'person'

                            unique_slug = base_slug
                            counter = 1
                            while Person.objects.filter(slug=unique_slug).exists() or any(p.slug == unique_slug for p in people_to_create):
                                unique_slug = f"{base_slug}-{counter}"
                                counter += 1

                            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
                            new_id = max_id + len(people_to_create) + 1

                            person = Person(
                                ceo_id=new_id,
                                ceo=holder,
                                last_name=last_name,
                                first_name=first_name,
                                middle_name=middle_name or '',
                                slug=unique_slug
                            )
                            people_to_create.append(person)
                            self.person_cache[holder] = person

                # –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ —Å–æ–∑–¥–∞–µ–º –≤ –ë–î
                if people_to_create:
                    batch_size = 500
                    for i in range(0, len(people_to_create), batch_size):
                        batch = people_to_create[i:i+batch_size]
                        Person.objects.bulk_create(batch, batch_size=batch_size)

                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                del existing_people
                del people_to_create

                progress = (chunk_end / total_people) * 100
                self.stdout.write(f"           –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%")

            # –§–∏–Ω–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
            for holder in person_holders:
                person_map[holder] = self.person_cache.get(holder)

        # –®–ê–ì 5: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π
        self.stdout.write("        –®–∞–≥ 5/7: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π...")

        org_relations = set()
        person_relations = set()

        with tqdm(total=sum(len(h) for h in holders_cache.values()), desc="           –°–±–æ—Ä —Å–≤—è–∑–µ–π", unit=" —Å–≤") as pbar:
            for reg_num, holders_list in holders_cache.items():
                ip_object = existing_objects.get(reg_num)
                if not ip_object:
                    continue

                for holder in holders_list:
                    if holder in org_map and org_map[holder]:
                        org_relations.add((ip_object.pk, org_map[holder].pk))
                    elif holder in person_map and person_map[holder]:
                        person_relations.add((ip_object.pk, person_map[holder].pk))
                    pbar.update(1)

        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏: {len(org_relations)}")
        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏: {len(person_relations)}")

        # –®–ê–ì 6: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏
        if org_relations:
            self.stdout.write("        –®–∞–≥ 6/7: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏...")

            ip_ids = list(set(ip_id for ip_id, _ in org_relations))

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ –ü–ê–ß–ö–ê–ú–ò
            delete_batch_size = 500
            deleted_total = 0
            for i in range(0, len(ip_ids), delete_batch_size):
                batch_ip_ids = ip_ids[i:i+delete_batch_size]
                deleted, _ = IPObject.owner_organizations.through.objects.filter(
                    ipobject_id__in=batch_ip_ids
                ).delete()
                deleted_total += deleted
                if (i // delete_batch_size) % 10 == 0:
                    self.stdout.write(f"              –£–¥–∞–ª–µ–Ω–æ {deleted_total} —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏...")

            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π: {deleted_total}")

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏ –ø–∞—á–∫–∞–º–∏
            through_objs = [
                IPObject.owner_organizations.through(
                    ipobject_id=ip_id,
                    organization_id=org_id
                )
                for ip_id, org_id in org_relations
            ]

            create_batch_size = 1000
            created_count = 0
            with tqdm(total=len(through_objs), desc="           –î–æ–±–∞–≤–ª–µ–Ω–∏–µ", unit=" —Å–≤") as pbar:
                for i in range(0, len(through_objs), create_batch_size):
                    batch = through_objs[i:i+create_batch_size]
                    IPObject.owner_organizations.through.objects.bulk_create(batch, batch_size=create_batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))

        # –®–ê–ì 7: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏
        if person_relations:
            self.stdout.write("        –®–∞–≥ 7/7: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏...")

            ip_ids = list(set(ip_id for ip_id, _ in person_relations))

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ –ü–ê–ß–ö–ê–ú–ò
            delete_batch_size = 500
            deleted_total = 0
            for i in range(0, len(ip_ids), delete_batch_size):
                batch_ip_ids = ip_ids[i:i+delete_batch_size]
                deleted, _ = IPObject.owner_persons.through.objects.filter(
                    ipobject_id__in=batch_ip_ids
                ).delete()
                deleted_total += deleted
                if (i // delete_batch_size) % 10 == 0:
                    self.stdout.write(f"              –£–¥–∞–ª–µ–Ω–æ {deleted_total} —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏...")

            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π: {deleted_total}")

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏ –ø–∞—á–∫–∞–º–∏
            through_objs = [
                IPObject.owner_persons.through(
                    ipobject_id=ip_id,
                    person_id=person_id
                )
                for ip_id, person_id in person_relations
            ]

            create_batch_size = 1000
            created_count = 0
            with tqdm(total=len(through_objs), desc="           –î–æ–±–∞–≤–ª–µ–Ω–∏–µ", unit=" —Å–≤") as pbar:
                for i in range(0, len(through_objs), create_batch_size):
                    batch = through_objs[i:i+create_batch_size]
                    IPObject.owner_persons.through.objects.bulk_create(batch, batch_size=create_batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))

        self.stdout.write(f"        ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


class UtilityModelParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='utility-model').first()

    def get_required_columns(self):
        return ['registration number', 'utility model name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class IndustrialDesignParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='industrial-design').first()

    def get_required_columns(self):
        return ['registration number', 'industrial design name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class IntegratedCircuitTopologyParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='integrated-circuit-topology').first()

    def get_required_columns(self):
        return ['registration number', 'microchip name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä —Ç–æ–ø–æ–ª–æ–≥–∏–π –º–∏–∫—Ä–æ—Å—Ö–µ–º –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class ComputerProgramParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='computer-program').first()

    def get_required_columns(self):
        return ['registration number', 'program name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class DatabaseParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='database').first()

    def get_required_columns(self):
        return ['registration number', 'db name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class Command(BaseCommand):
    help = '–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞'

    def add_arguments(self, parser):
        parser.add_argument('--catalogue-id', type=int, help='ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')
        parser.add_argument('--ip-type', type=str,
                        choices=['invention', 'utility-model', 'industrial-design',
                                'integrated-circuit-topology', 'computer-program', 'database'],
                        help='–¢–∏–ø –†–ò–î –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–∞—Ä—Å—è—Ç—Å—è –≤—Å–µ)')
        parser.add_argument('--dry-run', action='store_true', help='–†–µ–∂–∏–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î')
        parser.add_argument('--encoding', type=str, default='utf-8', help='–ö–æ–¥–∏—Ä–æ–≤–∫–∞ CSV —Ñ–∞–π–ª–∞')
        parser.add_argument('--delimiter', type=str, default=',', help='–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV —Ñ–∞–π–ª–µ')
        parser.add_argument('--batch-size', type=int, default=100, help='–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è bulk-–æ–ø–µ—Ä–∞—Ü–∏–π')
        parser.add_argument('--min-year', type=int, default=2000, help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏')
        parser.add_argument('--skip-filters', action='store_true', help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏)')
        parser.add_argument('--only-active', action='store_true', help='–ü–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç—ã (actual = True)')
        parser.add_argument('--max-rows', type=int, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)')
        parser.add_argument('--force', action='store_true', help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–∂–µ –µ—Å–ª–∏ –∫–∞—Ç–∞–ª–æ–≥ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω')
        parser.add_argument('--mark-processed', action='store_true',
                        help='–ü–æ–º–µ—Ç–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (–¥–∞–∂–µ –µ—Å–ª–∏ –±—ã–ª–∏ –æ—à–∏–±–∫–∏)')

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.parsers = {
            'invention': InventionParser(self),
            'utility-model': UtilityModelParser(self),
            'industrial-design': IndustrialDesignParser(self),
            'integrated-circuit-topology': IntegratedCircuitTopologyParser(self),
            'computer-program': ComputerProgramParser(self),
            'database': DatabaseParser(self),
        }

    def handle(self, *args, **options):
        self.dry_run = options['dry_run']
        self.encoding = options['encoding']
        self.delimiter = options['delimiter']
        self.batch_size = options['batch_size']
        self.min_year = options['min_year']
        self.skip_filters = options['skip_filters']
        self.only_active = options['only_active']
        self.max_rows = options.get('max_rows')
        self.force = options.get('force', False)
        self.mark_processed = options.get('mark_processed', False)

        if self.dry_run:
            self.stdout.write(self.style.WARNING("\nüîç –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î\n"))

        if self.only_active:
            self.stdout.write(self.style.WARNING("üìå –†–µ–∂–∏–º: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (actual = True)"))

        if self.force:
            self.stdout.write(self.style.WARNING("‚ö†Ô∏è  –†–µ–∂–∏–º: –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏)"))

        catalogues = self.get_catalogues(options.get('catalogue_id'), options.get('ip_type'))

        if not catalogues:
            raise CommandError('–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–∞—Ç–∞–ª–æ–≥–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')

        total_stats = {
            'catalogues': len(catalogues),
            'processed': 0,
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        for catalogue in catalogues:
            self.stdout.write(self.style.SUCCESS(f"\n{'='*60}"))
            self.stdout.write(self.style.SUCCESS(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞: {catalogue.name}"))
            self.stdout.write(self.style.SUCCESS(f"   ID: {catalogue.id}, –¢–∏–ø: {catalogue.ip_type.name if catalogue.ip_type else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'}"))
            self.stdout.write(self.style.SUCCESS(f"{'='*60}"))

            stats = self.process_catalogue(catalogue)

            for key in ['processed', 'created', 'updated', 'skipped', 'errors']:
                total_stats[key] += stats.get(key, 0)
            total_stats['skipped_by_date'] += stats.get('skipped_by_date', 0)

        self.print_final_stats(total_stats)

    def get_catalogues(self, catalogue_id=None, ip_type_slug=None):
        queryset = FipsOpenDataCatalogue.objects.all()

        if catalogue_id:
            queryset = queryset.filter(id=catalogue_id)
        elif ip_type_slug:
            queryset = queryset.filter(ip_type__slug=ip_type_slug)
        else:
            queryset = queryset.exclude(catalogue_file='')

        return queryset.order_by('ip_type__id', '-publication_date')

    def process_catalogue(self, catalogue):
        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        if not catalogue.catalogue_file:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –£ –∫–∞—Ç–∞–ª–æ–≥–∞ ID={catalogue.id} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª"))
            stats['errors'] += 1
            return stats

        if not self.force and hasattr(catalogue, 'parsed_date') and catalogue.parsed_date:
            self.stdout.write(self.style.WARNING(
                f"  ‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ —É–∂–µ –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω {catalogue.parsed_date.strftime('%d.%m.%Y %H:%M')}"
            ))
            self.stdout.write(self.style.WARNING(f"     –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --force –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"))
            stats['skipped'] += 1
            return stats

        ip_type_slug = catalogue.ip_type.slug if catalogue.ip_type else None

        if ip_type_slug not in self.parsers:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –ù–µ—Ç –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è —Ç–∏–ø–∞ –†–ò–î: {ip_type_slug}"))
            stats['errors'] += 1
            return stats

        parser = self.parsers[ip_type_slug]
        df = self.load_csv(catalogue)

        if df is None or df.empty:
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å"))
            stats['skipped'] += 1
            return stats

        self.stdout.write(f"  üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")

        missing_columns = self.check_required_columns(df, parser.get_required_columns())
        if missing_columns:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}"))
            stats['errors'] += 1
            return stats

        if not self.skip_filters:
            df = self.apply_filters(df)

        if df.empty:
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"))
            stats['skipped'] += 1
            return stats

        self.stdout.write(f"  üìä –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df)} –∑–∞–ø–∏—Å–µ–π")

        if self.max_rows and len(df) > self.max_rows:
            df = df.head(self.max_rows)
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {self.max_rows} –∑–∞–ø–∏—Å–µ–π"))

        try:
            parser_stats = parser.parse_dataframe(df, catalogue)
            stats.update(parser_stats)

            if not self.dry_run and hasattr(catalogue, 'parsed_date'):
                if stats['errors'] == 0 or self.mark_processed:
                    catalogue.parsed_date = timezone.now()
                    catalogue.save(update_fields=['parsed_date'])
                    self.stdout.write(self.style.SUCCESS(f"  ‚úÖ –ö–∞—Ç–∞–ª–æ–≥ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π"))
                else:
                    self.stdout.write(self.style.WARNING(
                        f"  ‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ –Ω–µ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫"
                    ))

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}"))
            logger.error(f"Error parsing catalogue {catalogue.id}: {e}", exc_info=True)
            stats['errors'] += 1

        return stats

    def load_csv(self, catalogue):
        file_path = catalogue.catalogue_file.path

        if not os.path.exists(file_path):
            self.stdout.write(self.style.ERROR(f"  ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"))
            return None

        try:
            strategies = [
                {'encoding': self.encoding, 'delimiter': self.delimiter, 'skipinitialspace': True},
                {'encoding': 'cp1251', 'delimiter': self.delimiter, 'skipinitialspace': True},
                {'encoding': 'utf-8', 'delimiter': ';', 'skipinitialspace': True},
                {'encoding': 'cp1251', 'delimiter': ';', 'skipinitialspace': True},
                {'encoding': 'utf-8', 'delimiter': '\t', 'skipinitialspace': True},
            ]

            for strategy in strategies:
                try:
                    df = pd.read_csv(file_path, **strategy, dtype=str, keep_default_na=False)
                    self.stdout.write(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {strategy}")

                    df.columns = [col.strip().strip('\ufeff').strip('"') for col in df.columns]

                    return df
                except Exception as e:
                    continue

            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π")

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}"))
            return None

    def check_required_columns(self, df, required_columns):
        missing = [col for col in required_columns if col not in df.columns]
        return missing

    def apply_filters(self, df):
        original_count = len(df)

        if 'registration date' in df.columns:
            df = self.filter_by_registration_year(df)

        if self.only_active and 'actual' in df.columns:
            df = self.filter_by_actual(df)

        filtered_count = len(df)
        if filtered_count < original_count:
            self.stdout.write(f"  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {original_count} ‚Üí {filtered_count} –∑–∞–ø–∏—Å–µ–π")

        return df

    def filter_by_registration_year(self, df):
        def extract_year(date_str):
            try:
                if pd.isna(date_str) or not date_str:
                    return None

                date_str = str(date_str).strip()
                if not date_str:
                    return None

                for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
                    try:
                        return datetime.strptime(date_str, fmt).year
                    except (ValueError, TypeError):
                        continue

                try:
                    return pd.to_datetime(date_str).year
                except (ValueError, TypeError):
                    return None
            except:
                return None

        self.stdout.write("  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≥–æ–¥—É —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏...")
        df['_year'] = df['registration date'].apply(extract_year)

        years_dist = df['_year'].value_counts().sort_index()
        years_list = list(years_dist.items())
        if len(years_list) > 0:
            self.stdout.write(f"     –î–∏–∞–ø–∞–∑–æ–Ω –≥–æ–¥–æ–≤: {years_list[0][0]:.0f} - {years_list[-1][0]:.0f}")
            self.stdout.write(f"     –ü–µ—Ä–≤—ã–µ 5: {years_list[:5]}")
            self.stdout.write(f"     –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5: {years_list[-5:]}")

        filtered_df = df[df['_year'] >= self.min_year].copy()
        filtered_df.drop('_year', axis=1, inplace=True)

        return filtered_df

    def filter_by_actual(self, df):
        def parse_actual(value):
            if pd.isna(value) or not value:
                return False
            value = str(value).lower().strip()
            return value in ['1', 'true', 'yes', '–¥–∞', '–¥–µ–π—Å—Ç–≤—É–µ—Ç', 't', '1.0', '–∞–∫—Ç–∏–≤–µ–Ω']

        df['_actual'] = df['actual'].apply(parse_actual)
        filtered_df = df[df['_actual'] == True].copy()
        filtered_df.drop('_actual', axis=1, inplace=True)

        return filtered_df

    def print_final_stats(self, stats):
        self.stdout.write(self.style.SUCCESS(f"\n{'='*60}"))
        self.stdout.write(self.style.SUCCESS("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê"))
        self.stdout.write(self.style.SUCCESS(f"{'='*60}"))
        self.stdout.write(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞—Ç–∞–ª–æ–≥–æ–≤: {stats['catalogues']}")
        self.stdout.write(f"üìù –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}")
        self.stdout.write(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ: {stats['created']}")
        self.stdout.write(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}")
        self.stdout.write(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ –≤—Å–µ–≥–æ: {stats['skipped']}")
        self.stdout.write(f"   ‚îî‚îÄ –ø–æ –¥–∞—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {stats.get('skipped_by_date', 0)}")

        if stats['errors'] > 0:
            self.stdout.write(self.style.ERROR(f"‚ùå –û—à–∏–±–æ–∫: {stats['errors']}"))
        else:
            self.stdout.write(self.style.SUCCESS(f"‚úÖ –û—à–∏–±–æ–∫: {stats['errors']}"))

        if self.dry_run:
            self.stdout.write(self.style.WARNING("\nüîç –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î"))

        self.stdout.write(self.style.SUCCESS(f"{'='*60}"))

```


-----

# –§–∞–π–ª: management\commands\__init__.py

```

```


-----

# –§–∞–π–ª: management\parsers\base.py

```
"""
–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –§–ò–ü–°
"""

import logging
import re
from datetime import datetime
from typing import Optional, List, Dict, Any
import gc

from django.db import models
from django.utils.text import slugify
import pandas as pd

from intellectual_property.models import IPObject, IPType
from core.models import Person, Organization, Country

# –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –ø–∞–∫–µ—Ç–∞ (.processors)
from .processors import (
    RussianTextProcessor,
    OrganizationNormalizer,
    PersonNameFormatter,
    RIDNameFormatter,
    EntityTypeDetector
)

# –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É—Ç–∏–ª–∏—Ç—ã –∏–∑ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø–∞–∫–µ—Ç–∞ (..utils)
from ..utils.progress import batch_iterator

logger = logging.getLogger(__name__)


class BaseFIPSParser:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –§–ò–ü–°"""

    def __init__(self, command):
        self.command = command
        self.stdout = command.stdout
        self.style = command.style

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
        self.processor = RussianTextProcessor()
        self.org_normalizer = OrganizationNormalizer()
        self.type_detector = EntityTypeDetector()
        self.person_formatter = PersonNameFormatter()
        self.rid_formatter = RIDNameFormatter()

        # –ö—ç—à–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.country_cache = {}
        self.person_cache = {}
        self.organization_cache = {}
        self.foiv_cache = {}
        self.rf_rep_cache = {}
        self.city_cache = {}
        self.activity_type_cache = {}
        self.ceo_position_cache = {}

    def get_ip_type(self):
        """–î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –¥–æ—á–µ—Ä–Ω–∏—Ö –∫–ª–∞—Å—Å–∞—Ö"""
        raise NotImplementedError

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
        raise NotImplementedError

    def parse_dataframe(self, df, catalogue):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame"""
        raise NotImplementedError

    def clean_string(self, value):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if pd.isna(value) or value is None:
            return ''
        value = str(value).strip()
        if value in ['', 'None', 'null', 'NULL', 'nan']:
            return ''
        return value

    def parse_date(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        if pd.isna(value) or not value:
            return None

        date_str = str(value).strip()
        if not date_str:
            return None

        for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
            try:
                return datetime.strptime(date_str, fmt).date()
            except (ValueError, TypeError):
                continue

        try:
            return pd.to_datetime(date_str).date()
        except (ValueError, TypeError):
            return None

    def parse_bool(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –±—É–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if pd.isna(value) or not value:
            return False
        value = str(value).lower().strip()
        return value in ['1', 'true', 'yes', '–¥–∞', '–¥–µ–π—Å—Ç–≤—É–µ—Ç', 't', '1.0', '–∞–∫—Ç–∏–≤–µ–Ω']

    def get_or_create_country(self, code):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –ø–æ –∫–æ–¥—É"""
        if not code or pd.isna(code):
            return None

        code = str(code).upper().strip()
        if len(code) != 2:
            return None

        if code in self.country_cache:
            return self.country_cache[code]

        try:
            country = Country.objects.filter(code=code).first()
            if country:
                self.country_cache[code] = country
                return country

            country = Country.objects.filter(code_alpha3=code).first()
            if country:
                self.country_cache[code] = country
                return country

            return None

        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω—ã {code}: {e}"))
            return None

    def parse_authors(self, authors_str):
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –∞–≤—Ç–æ—Ä–∞–º–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –∞–≤—Ç–æ—Ä–æ–≤
        """
        if pd.isna(authors_str) or not authors_str:
            return []

        authors_str = str(authors_str)
        authors_list = re.split(r'[\n,]\s*', authors_str)

        result = []
        for author in authors_list:
            author = author.strip()
            if not author or author == '""' or author == 'null':
                continue

            author = author.strip('"')
            author = re.sub(r'\s*\([A-Z]{2}\)', '', author)
            author = self.person_formatter.format(author)

            parts = author.split()

            if len(parts) >= 2:
                last_name = parts[0]
                first_name = parts[1] if len(parts) > 1 else ''
                middle_name = parts[2] if len(parts) > 2 else ''

                first_name_clean = first_name.replace('.', '')
                middle_name_clean = middle_name.replace('.', '')

                result.append({
                    'last_name': last_name,
                    'first_name': first_name_clean,
                    'middle_name': middle_name_clean,
                    'full_name': author,
                })
            else:
                result.append({
                    'last_name': author,
                    'first_name': '',
                    'middle_name': '',
                    'full_name': author,
                })

        return result

    def parse_patent_holders(self, holders_str):
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—è–º–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π
        """
        if pd.isna(holders_str) or not holders_str:
            return []

        holders_str = str(holders_str)
        holders_list = re.split(r'[\n]\s*', holders_str)

        result = []
        for holder in holders_list:
            holder = holder.strip().strip('"')
            if not holder or holder == 'null' or holder == 'None':
                continue

            holder = re.sub(r'\s*\([A-Z]{2}\)', '', holder)
            result.append(holder)

        return result

    def find_or_create_person(self, person_data):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞"""
        cache_key = f"{person_data['last_name']}|{person_data['first_name']}|{person_data['middle_name']}"

        if cache_key in self.person_cache:
            return self.person_cache[cache_key]

        persons = Person.objects.filter(
            last_name=person_data['last_name'],
            first_name=person_data['first_name']
        )

        if person_data['middle_name']:
            persons = persons.filter(middle_name=person_data['middle_name'])

        if persons.exists():
            person = persons.first()
            self.person_cache[cache_key] = person
            return person

        try:
            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
            new_id = max_id + 1

            if 'full_name' in person_data:
                full_name = person_data['full_name']
            else:
                full_name_parts = [person_data['last_name'], person_data['first_name']]
                if person_data['middle_name']:
                    full_name_parts.append(person_data['middle_name'])
                full_name = ' '.join(full_name_parts)
                full_name = self.person_formatter.format(full_name)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π slug
            base_slug = slugify(f"{person_data['last_name']} {person_data['first_name']} {person_data['middle_name']}".strip())
            if not base_slug:
                base_slug = 'person'

            unique_slug = base_slug
            counter = 1
            while Person.objects.filter(slug=unique_slug).exists():
                unique_slug = f"{base_slug}-{counter}"
                counter += 1

            person = Person.objects.create(
                ceo_id=new_id,
                ceo=full_name,
                last_name=person_data['last_name'],
                first_name=person_data['first_name'],
                middle_name=person_data['middle_name'],
                slug=unique_slug
            )
            self.person_cache[cache_key] = person
            return person
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Person: {e}"))
            return None

    def find_or_create_person_from_name(self, full_name):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞ –ø–æ –ø–æ–ª–Ω–æ–º—É –∏–º–µ–Ω–∏"""
        if pd.isna(full_name) or not full_name:
            return None

        full_name = str(full_name).strip().strip('"')
        full_name = self.person_formatter.format(full_name)

        if full_name in self.person_cache:
            return self.person_cache[full_name]

        parts = full_name.split()

        if len(parts) >= 2:
            last_name = parts[0]
            first_name = parts[1] if len(parts) > 1 else ''
            middle_name = parts[2] if len(parts) > 2 else ''

            first_name_clean = first_name.replace('.', '')
            middle_name_clean = middle_name.replace('.', '')

            person_data = {
                'last_name': last_name,
                'first_name': first_name_clean,
                'middle_name': middle_name_clean,
                'full_name': full_name,
            }
        else:
            person_data = {
                'last_name': full_name,
                'first_name': '',
                'middle_name': '',
                'full_name': full_name,
            }

        return self.find_or_create_person(person_data)

    def find_similar_organization(self, org_name):
        """–£—Å–∏–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        if pd.isna(org_name) or not org_name:
            return None

        org_name = str(org_name).strip().strip('"')

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        direct_match = Organization.objects.filter(
            models.Q(name=org_name) |
            models.Q(full_name=org_name) |
            models.Q(short_name=org_name)
        ).first()
        if direct_match:
            return direct_match

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
        norm_data = self.org_normalizer.normalize_for_search(org_name)
        normalized = norm_data['normalized']
        keywords = norm_data['keywords']

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in keywords:
            if len(keyword) >= 3:
                similar = Organization.objects.filter(
                    models.Q(name__icontains=keyword) |
                    models.Q(full_name__icontains=keyword) |
                    models.Q(short_name__icontains=keyword)
                ).first()
                if similar:
                    return similar

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ –ø–æ –ø–µ—Ä–≤—ã–º 30 —Å–∏–º–≤–æ–ª–∞–º
        if len(normalized) > 30:
            prefix = normalized[:30]
            similar = Organization.objects.filter(
                models.Q(name__icontains=prefix) |
                models.Q(full_name__icontains=prefix) |
                models.Q(short_name__icontains=prefix)
            ).first()
            if similar:
                return similar

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ü–æ–∏—Å–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–ª–æ–≤–∞–º
        words = org_name.split()
        for word in words:
            if len(word) > 4:
                similar = Organization.objects.filter(
                    models.Q(name__icontains=word) |
                    models.Q(full_name__icontains=word)
                ).first()
                if similar:
                    return similar

        return None

    def find_or_create_organization(self, org_name):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        if pd.isna(org_name) or not org_name:
            return None

        org_name = str(org_name).strip().strip('"')

        if not org_name or org_name == 'null' or org_name == 'None':
            return None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if org_name in self.organization_cache:
            return self.organization_cache[org_name]

        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ
        similar = self.find_similar_organization(org_name)
        if similar:
            self.organization_cache[org_name] = similar
            return similar

        # –ù–µ –Ω–∞—à–ª–∏ - —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º
        try:
            max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
            new_id = max_id + 1

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º slug –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
            base_slug = slugify(org_name[:50])
            if not base_slug:
                base_slug = 'organization'

            unique_slug = base_slug
            counter = 1
            while Organization.objects.filter(slug=unique_slug).exists():
                unique_slug = f"{base_slug}-{counter}"
                counter += 1

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            org = Organization.objects.create(
                organization_id=new_id,
                name=org_name,
                full_name=org_name,
                short_name=org_name[:500] if len(org_name) > 500 else org_name,
                slug=unique_slug,
                register_opk=False,
                strategic=False,
            )

            self.organization_cache[org_name] = org
            return org
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Organization: {e}"))
            return None
```


-----

# –§–∞–π–ª: management\parsers\computer_program.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú
"""

from intellectual_property.models import IPType
from .base import BaseFIPSParser


class ComputerProgramParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='computer-program').first()

    def get_required_columns(self):
        return ['registration number', 'program name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}

```


-----

# –§–∞–π–ª: management\parsers\database.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
"""

from intellectual_property.models import IPType
from .base import BaseFIPSParser


class DatabaseParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='database').first()

    def get_required_columns(self):
        return ['registration number', 'db name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}

```


-----

# –§–∞–π–ª: management\parsers\industrial_design.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
"""

from intellectual_property.models import IPType
from .base import BaseFIPSParser


class IndustrialDesignParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='industrial-design').first()

    def get_required_columns(self):
        return ['registration number', 'industrial design name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}

```


-----

# –§–∞–π–ª: management\parsers\integrated_circuit.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º
"""

from intellectual_property.models import IPType
from .base import BaseFIPSParser


class IntegratedCircuitTopologyParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='integrated-circuit-topology').first()

    def get_required_columns(self):
        return ['registration number', 'microchip name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä —Ç–æ–ø–æ–ª–æ–≥–∏–π –º–∏–∫—Ä–æ—Å—Ö–µ–º –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}

```


-----

# –§–∞–π–ª: management\parsers\invention.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –µ–¥–∏–Ω–æ–≥–æ DataFrame –¥–ª—è —Å–≤—è–∑–µ–π
"""

import logging
import gc
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict

import pandas as pd
from django.db import models, transaction
from django.utils.text import slugify
from tqdm import tqdm

from intellectual_property.models import IPObject, IPType, Person
from core.models import Organization

from .base import BaseFIPSParser
from ..utils.progress import batch_iterator

logger = logging.getLogger(__name__)


class InventionParser(BaseFIPSParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–≤—è–∑–µ–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–¥–∏–Ω—ã–π DataFrame –¥–ª—è –≤—Å–µ—Ö —Å–≤—è–∑–µ–π (–∞–≤—Ç–æ—Ä—ã + –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏)
    """

    def get_ip_type(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø –†–ò–î 'invention'"""
        return IPType.objects.filter(slug='invention').first()

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è CSV"""
        return ['registration number', 'invention name']

    def _has_data_changed(self, obj, new_data):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç–∞
        """
        fields_to_check = [
            ('name', obj.name, new_data['name']),
            ('application_date', obj.application_date, new_data['application_date']),
            ('registration_date', obj.registration_date, new_data['registration_date']),
            ('patent_starting_date', obj.patent_starting_date, new_data['patent_starting_date']),
            ('expiration_date', obj.expiration_date, new_data['expiration_date']),
            ('actual', obj.actual, new_data['actual']),
            ('publication_url', obj.publication_url, new_data['publication_url']),
            ('abstract', obj.abstract, new_data['abstract']),
            ('claims', obj.claims, new_data['claims']),
            ('creation_year', obj.creation_year, new_data['creation_year']),
        ]

        for field_name, old_val, new_val in fields_to_check:
            if old_val != new_val:
                return True
        return False

    def parse_dataframe(self, df, catalogue):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame
        """
        self.stdout.write("\nüîπ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π")

        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –†–ò–î
        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'invention' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats

        upload_date = catalogue.upload_date.date() if catalogue.upload_date else None

        # =====================================================================
        # –®–ê–ì 1: –°–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
        # =====================================================================
        self.stdout.write("üîπ –ß—Ç–µ–Ω–∏–µ CSV –∏ —Å–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        reg_num_to_row = {}
        skipped_empty = 0
        
        for idx, row in df.iterrows():
            reg_num = self.clean_string(row.get('registration number'))
            if reg_num:
                reg_num_to_row[reg_num] = row
            else:
                skipped_empty += 1

        self.stdout.write(f"üîπ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: {len(reg_num_to_row)} (–ø—Ä–æ–ø—É—â–µ–Ω–æ –ø—É—Å—Ç—ã—Ö: {skipped_empty})")

        # =====================================================================
        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î
        # =====================================================================
        self.stdout.write("üîπ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î")
        
        existing_objects = {}
        batch_size = 500
        reg_numbers = list(reg_num_to_row.keys())
        
        with tqdm(total=len(reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—á–∫–∞–º–∏", unit="–∑–∞–ø") as pbar:
            for i in range(0, len(reg_numbers), batch_size):
                batch_numbers = reg_numbers[i:i+batch_size]
                
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_numbers,
                    ip_type=ip_type
                ).select_related('ip_type'):
                    existing_objects[obj.registration_number] = obj
                
                pbar.update(len(batch_numbers))

        self.stdout.write(f"üîπ –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(existing_objects)}")

        # =====================================================================
        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è IPObject
        # =====================================================================
        self.stdout.write("üîπ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject")
        
        to_create = []
        to_update = []
        skipped_by_date = []
        unchanged_count = 0
        error_reg_numbers = []

        relations_data = []
        
        with tqdm(total=len(reg_num_to_row), desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject", unit="–∑–∞–ø") as pbar:
            for reg_num, row in reg_num_to_row.items():
                try:
                    if not self.command.force and upload_date and reg_num in existing_objects:
                        existing = existing_objects[reg_num]
                        if existing.updated_at and existing.updated_at.date() >= upload_date:
                            skipped_by_date.append(reg_num)
                            pbar.update(1)
                            continue

                    name = self.clean_string(row.get('invention name'))
                    if name:
                        name = self.rid_formatter.format(name)
                    else:
                        name = f"–ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ ‚Ññ{reg_num}"

                    application_date = self.parse_date(row.get('application date'))
                    registration_date = self.parse_date(row.get('registration date'))
                    patent_starting_date = self.parse_date(row.get('patent starting date'))
                    expiration_date = self.parse_date(row.get('expiration date'))
                    actual = self.parse_bool(row.get('actual'))
                    publication_url = self.clean_string(row.get('publication URL'))
                    abstract = self.clean_string(row.get('abstract'))
                    claims = self.clean_string(row.get('claims'))

                    creation_year = None
                    if application_date:
                        creation_year = application_date.year
                    elif registration_date:
                        creation_year = registration_date.year

                    obj_data = {
                        'registration_number': reg_num,
                        'ip_type_id': ip_type.id,
                        'name': name,
                        'application_date': application_date,
                        'registration_date': registration_date,
                        'patent_starting_date': patent_starting_date,
                        'expiration_date': expiration_date,
                        'actual': actual,
                        'publication_url': publication_url,
                        'abstract': abstract,
                        'claims': claims,
                        'creation_year': creation_year,
                    }

                    if reg_num in existing_objects:
                        if self._has_data_changed(existing_objects[reg_num], obj_data):
                            to_update.append(obj_data)
                        else:
                            unchanged_count += 1
                    else:
                        to_create.append(obj_data)

                    # –ê–≤—Ç–æ—Ä—ã
                    authors_str = row.get('authors')
                    if not pd.isna(authors_str) and authors_str:
                        authors = self.parse_authors(authors_str)
                        for author in authors:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': author['full_name'],
                                'entity_type': 'person',
                                'relation_type': 'author',
                                'entity_data': author
                            })

                    # –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏
                    holders_str = row.get('patent holders')
                    if not pd.isna(holders_str) and holders_str:
                        holders = self.parse_patent_holders(holders_str)
                        for holder in holders:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': holder,
                                'entity_type': None,
                                'relation_type': 'holder',
                                'entity_data': {'full_name': holder}
                            })

                except Exception as e:
                    error_reg_numbers.append(reg_num)
                    if len(error_reg_numbers) < 10:
                        self.stdout.write(self.style.ERROR(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    elif len(error_reg_numbers) == 10:
                        self.stdout.write(self.style.WARNING("\n‚ö†Ô∏è ... –∏ –¥–∞–ª–µ–µ –æ—à–∏–±–∫–∏ –ø–æ–¥–∞–≤–ª—è—é—Ç—Å—è"))
                    
                    logger.error(f"Error preparing invention {reg_num}: {e}", exc_info=True)

                pbar.update(1)

        self.stdout.write(f"üîπ –ò—Ç–æ–≥–æ: –Ω–æ–≤—ã—Ö={len(to_create)}, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ={len(to_update)}, "
                         f"–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={unchanged_count}, –æ—à–∏–±–æ–∫={len(error_reg_numbers)}")

        stats['skipped_by_date'] = len(skipped_by_date)
        stats['skipped'] += len(skipped_by_date)
        stats['errors'] = len(error_reg_numbers)
        stats['unchanged'] = unchanged_count

        # =====================================================================
        # –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ IPObject
        # =====================================================================
        if to_create and not self.command.dry_run:
            self.stdout.write(f"üîπ –°–æ–∑–¥–∞–Ω–∏–µ {len(to_create)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_create), desc="–°–æ–∑–¥–∞–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['created'] = self._bulk_create_objects(to_create, pbar)

        if to_update and not self.command.dry_run:
            self.stdout.write(f"üîπ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {len(to_update)} –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_update), desc="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['updated'] = self._bulk_update_objects(to_update, existing_objects, pbar)

        # =====================================================================
        # –®–ê–ì 5: –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ reg_number -> ip_id
        # =====================================================================
        self.stdout.write("üîπ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        all_reg_numbers = list(set(
            list(existing_objects.keys()) + 
            [data['registration_number'] for data in to_create]
        ))
        
        reg_to_ip = {}
        with tqdm(total=len(all_reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ ID –æ–±—ä–µ–∫—Ç–æ–≤", unit="–∑–∞–ø") as pbar:
            batch_size = 1000
            for i in range(0, len(all_reg_numbers), batch_size):
                batch_nums = all_reg_numbers[i:i+batch_size]
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_nums,
                    ip_type=ip_type
                ).only('id', 'registration_number'):
                    reg_to_ip[obj.registration_number] = obj.id
                pbar.update(len(batch_nums))

        self.stdout.write(f"üîπ –ó–∞–≥—Ä—É–∂–µ–Ω–æ ID –¥–ª—è {len(reg_to_ip)} –æ–±—ä–µ–∫—Ç–æ–≤")

        # =====================================================================
        # –®–ê–ì 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π DataFrame
        # =====================================================================
        if relations_data and not self.command.dry_run:
            self.stdout.write("üîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π")
            self._process_relations_dataframe(relations_data, reg_to_ip)

        gc.collect()

        stats['processed'] = len(df) - stats['skipped'] - stats['errors']

        self.stdout.write(self.style.SUCCESS("\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"   –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats['unchanged']}")
        self.stdout.write(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']} (–∏–∑ –Ω–∏—Ö –ø–æ –¥–∞—Ç–µ: {stats['skipped_by_date']})")
        self.stdout.write(f"   –û—à–∏–±–æ–∫: {stats['errors']}")

        return stats

    def _bulk_create_objects(self, to_create: List[Dict], pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        created_count = 0
        batch_size = 1000

        for batch in batch_iterator(to_create, batch_size):
            create_objects = [IPObject(**data) for data in batch]
            IPObject.objects.bulk_create(create_objects, batch_size=batch_size)
            created_count += len(batch)
            pbar.update(len(batch))

        return created_count

    def _bulk_update_objects(self, to_update: List[Dict], existing_objects: Dict, pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        updated_count = 0
        BATCH_UPDATE_SIZE = 500

        for batch in batch_iterator(to_update, BATCH_UPDATE_SIZE):
            with transaction.atomic():
                for data in batch:
                    obj = existing_objects[data['registration_number']]
                    update_fields = []
                    for field, value in data.items():
                        if field != 'registration_number' and getattr(obj, field) != value:
                            setattr(obj, field, value)
                            update_fields.append(field)
                    if update_fields:
                        obj.save(update_fields=update_fields)
                        updated_count += 1
            pbar.update(len(batch))

        return updated_count

    def _process_relations_dataframe(self, relations_data: List[Dict], reg_to_ip: Dict):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Å–≤—è–∑–µ–π —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π DataFrame"""
        if not relations_data:
            self.stdout.write("   –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–≤—è–∑–µ–π")
            return

        self.stdout.write("   –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å–≤—è–∑–µ–π")
        df_relations = pd.DataFrame(relations_data)
        
        self.stdout.write(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π —Å–≤—è–∑–µ–π: {len(df_relations)}")
        self.stdout.write(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤: {df_relations['reg_number'].nunique()}")

        self.stdout.write("   –î–æ–±–∞–≤–ª–µ–Ω–∏–µ ID –æ–±—ä–µ–∫—Ç–æ–≤")
        df_relations['ip_id'] = df_relations['reg_number'].map(reg_to_ip)

        missing_ip = df_relations['ip_id'].isna().sum()
        if missing_ip > 0:
            self.stdout.write(self.style.WARNING(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ {missing_ip} —Å–≤—è–∑–µ–π —Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º–∏ ID –æ–±—ä–µ–∫—Ç–æ–≤"))
            df_relations = df_relations.dropna(subset=['ip_id']).copy()
        
        df_relations['ip_id'] = df_relations['ip_id'].astype(int)

        # =====================================================================
        # –®–ê–ì 6.1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
        # =====================================================================
        self.stdout.write("   –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Natasha")
        
        unique_entities = df_relations[['entity_name', 'entity_type']].drop_duplicates()
        holders_to_check = unique_entities[unique_entities['entity_type'].isna()]['entity_name'].tolist()

        if holders_to_check:
            self.stdout.write(f"   –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è {len(holders_to_check)} –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π")
            entity_type_map = self.type_detector.detect_type_batch(holders_to_check)

            mask = df_relations['entity_type'].isna()
            df_relations.loc[mask, 'entity_type'] = \
                df_relations.loc[mask, 'entity_name'].map(entity_type_map)

        type_stats = df_relations['entity_type'].value_counts().to_dict()
        self.stdout.write(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤: –ª—é–¥–∏={type_stats.get('person', 0)}, "
                         f"–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏={type_stats.get('organization', 0)}")

        # =====================================================================
        # –®–ê–ì 6.2: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û)
        # =====================================================================
        unique_entities = df_relations[['entity_name', 'entity_type']].drop_duplicates()
        
        persons_df = unique_entities[unique_entities['entity_type'] == 'person']
        orgs_df = unique_entities[unique_entities['entity_type'] == 'organization']

        person_map = {}
        if not persons_df.empty:
            self.stdout.write(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(persons_df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π")
            person_map = self._create_persons_bulk(persons_df)

        org_map = {}
        if not orgs_df.empty:
            self.stdout.write(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(orgs_df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
            org_map = self._create_organizations_bulk(orgs_df)

        # =====================================================================
        # –®–ê–ì 6.3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π
        # =====================================================================
        self.stdout.write("   –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ –ë–î")

        authors_df = df_relations[df_relations['relation_type'] == 'author'].copy()
        holders_df = df_relations[df_relations['relation_type'] == 'holder'].copy()

        author_relations = []
        if not authors_df.empty:
            # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∏–º–µ–Ω –≤ ID
            person_id_map = {name: p.ceo_id for name, p in person_map.items()}
            authors_df['person_id'] = authors_df['entity_name'].map(person_id_map)
            
            # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º person_id (NaN)
            authors_df = authors_df.dropna(subset=['person_id'])
            authors_df['person_id'] = authors_df['person_id'].astype(int)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            authors_unique = authors_df[['ip_id', 'person_id']].drop_duplicates()
            author_relations = [(row['ip_id'], row['person_id']) 
                            for _, row in authors_unique.iterrows()]
            self.stdout.write(f"   –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(author_relations)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤")

        holder_person_relations = []
        holder_org_relations = []

        if not holders_df.empty:
            holders_persons = holders_df[holders_df['entity_type'] == 'person'].copy()
            if not holders_persons.empty:
                person_id_map = {name: p.ceo_id for name, p in person_map.items()}
                holders_persons['person_id'] = holders_persons['entity_name'].map(person_id_map)
                
                # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º person_id
                holders_persons = holders_persons.dropna(subset=['person_id'])
                holders_persons['person_id'] = holders_persons['person_id'].astype(int)
                
                holders_persons_unique = holders_persons[['ip_id', 'person_id']].drop_duplicates()
                holder_person_relations = [(row['ip_id'], row['person_id']) 
                                        for _, row in holders_persons_unique.iterrows()]
                self.stdout.write(f"   –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(holder_person_relations)} —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–ª—é–¥–µ–π")

            holders_orgs = holders_df[holders_df['entity_type'] == 'organization'].copy()
            if not holders_orgs.empty:
                org_id_map = {name: o.organization_id for name, o in org_map.items()}
                holders_orgs['org_id'] = holders_orgs['entity_name'].map(org_id_map)
                
                # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º org_id
                holders_orgs = holders_orgs.dropna(subset=['org_id'])
                holders_orgs['org_id'] = holders_orgs['org_id'].astype(int)
                
                holders_orgs_unique = holders_orgs[['ip_id', 'org_id']].drop_duplicates()
                holder_org_relations = [(row['ip_id'], row['org_id']) 
                                    for _, row in holders_orgs_unique.iterrows()]
                self.stdout.write(f"   –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(holder_org_relations)} —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")

        # =====================================================================
        # –®–ê–ì 6.4: –ú–∞—Å—Å–æ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π
        # =====================================================================
        if author_relations:
            self.stdout.write("   –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤")
            ip_ids = list(set(ip_id for ip_id, _ in author_relations))
            with tqdm(total=len(ip_ids), desc="   –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤", unit="ip") as pbar:
                self._delete_author_relations(ip_ids, pbar)
            
            with tqdm(total=len(author_relations), desc="   –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤", unit="—Å–≤") as pbar:
                self._create_author_relations(author_relations, pbar)

        if holder_person_relations:
            self.stdout.write("   –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π (–ª—é–¥–∏)")
            ip_ids = list(set(ip_id for ip_id, _ in holder_person_relations))
            with tqdm(total=len(ip_ids), desc="   –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π", unit="ip") as pbar:
                self._delete_holder_person_relations(ip_ids, pbar)
            
            with tqdm(total=len(holder_person_relations), desc="   –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π", unit="—Å–≤") as pbar:
                self._create_holder_person_relations(holder_person_relations, pbar)

        if holder_org_relations:
            self.stdout.write("   –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π (–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)")
            ip_ids = list(set(ip_id for ip_id, _ in holder_org_relations))
            with tqdm(total=len(ip_ids), desc="   –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π", unit="ip") as pbar:
                self._delete_holder_org_relations(ip_ids, pbar)
            
            with tqdm(total=len(holder_org_relations), desc="   –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π", unit="—Å–≤") as pbar:
                self._create_holder_org_relations(holder_org_relations, pbar)

        self.stdout.write(self.style.SUCCESS("   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Å–≤—è–∑–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞"))

    def _create_persons_bulk(self, persons_df: pd.DataFrame) -> Dict:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ª—é–¥–µ–π –∏–∑ DataFrame —Å –∏–Ω–¥–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        """
        person_map = {}
        all_names = persons_df['entity_name'].tolist()
        total_names = len(all_names)
        
        self.stdout.write(f"      –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_names}")
        
        # –®–ê–ì 1: –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ª—é–¥–µ–π (–ø–∞—á–∫–∞–º–∏ –ø–æ 100 –∏–º–µ–Ω)
        self.stdout.write(f"      –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ª—é–¥–µ–π –≤ –ë–î...")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –∏–º–µ–Ω–∞ –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
        name_to_parts = {}
        for name in all_names:
            parts = name.split()
            if len(parts) >= 2:
                last = parts[0]
                first = parts[1]
                middle = parts[2] if len(parts) > 2 else ''
                name_to_parts[name] = (last, first, middle)
        
        # –ò—â–µ–º –ª—é–¥–µ–π –ø–∞—á–∫–∞–º–∏
        existing_persons = {}
        found_count = 0
        batch_size = 100  # SQLite –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å ~100 OR —É—Å–ª–æ–≤–∏–π
        
        all_names_list = list(name_to_parts.keys())
        
        for i in range(0, len(all_names_list), batch_size):
            batch_names = all_names_list[i:i+batch_size]
            
            # –°—Ç—Ä–æ–∏–º –∑–∞–ø—Ä–æ—Å –¥–ª—è —Ç–µ–∫—É—â–µ–π –ø–∞—á–∫–∏
            name_conditions = models.Q()
            batch_name_to_parts = {}
            
            for name in batch_names:
                last, first, middle = name_to_parts[name]
                batch_name_to_parts[name] = (last, first, middle)
                
                if middle:
                    name_conditions |= models.Q(
                        last_name=last, 
                        first_name=first, 
                        middle_name=middle
                    )
                else:
                    name_conditions |= models.Q(
                        last_name=last, 
                        first_name=first
                    ) & (models.Q(middle_name='') | models.Q(middle_name__isnull=True))
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –¥–ª—è –ø–∞—á–∫–∏
            for person in Person.objects.filter(name_conditions).only(
                'ceo_id', 'last_name', 'first_name', 'middle_name', 'ceo'
            ):
                # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤ —Ç–µ–∫—É—â–µ–π –ø–∞—á–∫–µ
                for name, (last, first, middle) in batch_name_to_parts.items():
                    if (person.last_name == last and 
                        person.first_name == first and 
                        (not middle or person.middle_name == middle)):
                        existing_persons[name] = person
                        self.person_cache[name] = person
                        found_count += 1
                        break
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ–∏—Å–∫–∞
            if (i + len(batch_names)) % 500 == 0 or (i + len(batch_names)) >= len(all_names_list):
                self.stdout.write(f"         –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + len(batch_names)}/{len(all_names_list)} –∏–º–µ–Ω")
        
        self.stdout.write(f"      –ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {found_count}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–≤—ã—Ö –ª—é–¥–µ–π
        new_names = [name for name in all_names if name not in existing_persons]
        new_count = len(new_names)
        
        self.stdout.write(f"      –ù–æ–≤—ã—Ö –ª—é–¥–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {new_count}")
        
        if new_names:
            # –®–ê–ì 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è
            self.stdout.write(f"      –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è...")
            
            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
            existing_slugs = set(Person.objects.values_list('slug', flat=True)[:100000])
            
            people_to_create = []
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –¥–ª—è bulk_create
            for name in new_names:
                parts = name.split()
                if len(parts) >= 2:
                    last_name = parts[0]
                    first_name = parts[1]
                    middle_name = parts[2] if len(parts) > 2 else ''
                    
                    name_parts_list = [last_name, first_name]
                    if middle_name:
                        name_parts_list.append(middle_name)
                    
                    base_slug = slugify(' '.join(name_parts_list)) or 'person'
                    unique_slug = base_slug
                    counter = 1
                    while unique_slug in existing_slugs:
                        unique_slug = f"{base_slug}-{counter}"
                        counter += 1
                    existing_slugs.add(unique_slug)
                    
                    person = Person(
                        ceo_id=max_id + len(people_to_create) + 1,
                        ceo=name,
                        last_name=last_name,
                        first_name=first_name,
                        middle_name=middle_name or '',
                        slug=unique_slug
                    )
                    people_to_create.append(person)
            
            # –®–ê–ì 3: –ú–∞—Å—Å–æ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Å –∏–Ω–¥–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            self.stdout.write(f"      –°–æ–∑–¥–∞–Ω–∏–µ –ª—é–¥–µ–π –ø–∞—á–∫–∞–º–∏ –ø–æ 500...")
            
            batch_size = 500
            created_count = 0
            
            for batch in batch_iterator(people_to_create, batch_size):
                Person.objects.bulk_create(batch, batch_size=batch_size)
                created_count += len(batch)
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 5000 –∑–∞–ø–∏—Å–µ–π
                if created_count % 5000 == 0 or created_count == new_count:
                    percent = (created_count / new_count) * 100
                    self.stdout.write(f"         –°–æ–∑–¥–∞–Ω–æ {created_count}/{new_count} ({percent:.1f}%)")
            
            # –®–ê–ì 4: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ª—é–¥–µ–π –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞
            self.stdout.write(f"      –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ª—é–¥–µ–π –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞...")
            
            created_names = [p.ceo for p in people_to_create]
            for batch in batch_iterator(created_names, 1000):
                for person in Person.objects.filter(ceo__in=batch):
                    person_map[person.ceo] = person
                    self.person_cache[person.ceo] = person

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ª—é–¥–µ–π –≤ –º–∞–ø–ø–∏–Ω–≥
        person_map.update(existing_persons)
        
        self.stdout.write(f"      ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ª—é–¥–µ–π: {len(person_map)}")
        
        return person_map

    def _create_organizations_bulk(self, orgs_df: pd.DataFrame) -> Dict:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏–∑ DataFrame —Å –∏–Ω–¥–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        """
        org_map = {}
        all_names = orgs_df['entity_name'].tolist()
        total_names = len(all_names)
        
        self.stdout.write(f"      –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_names}")
        
        # –®–ê–ì 1: –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–ø–∞—á–∫–∞–º–∏)
        self.stdout.write(f"      –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –≤ –ë–î...")
        
        existing_orgs = {}
        batch_size = 100  # SQLite –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å ~100 —É—Å–ª–æ–≤–∏–π IN
        
        for i in range(0, len(all_names), batch_size):
            batch_names = all_names[i:i+batch_size]
            
            for org in Organization.objects.filter(name__in=batch_names).only('organization_id', 'name'):
                existing_orgs[org.name] = org
                self.organization_cache[org.name] = org
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ–∏—Å–∫–∞
            if (i + len(batch_names)) % 500 == 0 or (i + len(batch_names)) >= len(all_names):
                self.stdout.write(f"         –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + len(batch_names)}/{len(all_names)} –Ω–∞–∑–≤–∞–Ω–∏–π")
        
        self.stdout.write(f"      –ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {len(existing_orgs)}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–≤—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        new_names = [name for name in all_names if name not in existing_orgs]
        new_count = len(new_names)
        
        self.stdout.write(f"      –ù–æ–≤—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {new_count}")
        
        if new_names:
            # –®–ê–ì 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è
            self.stdout.write(f"      –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è...")
            
            max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
            existing_slugs = set(Organization.objects.values_list('slug', flat=True)[:50000])
            
            orgs_to_create = []
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –¥–ª—è bulk_create
            for name in new_names:
                base_slug = slugify(name[:50]) or 'organization'
                unique_slug = base_slug
                counter = 1
                while unique_slug in existing_slugs:
                    unique_slug = f"{base_slug}-{counter}"
                    counter += 1
                existing_slugs.add(unique_slug)
                
                org = Organization(
                    organization_id=max_id + len(orgs_to_create) + 1,
                    name=name,
                    full_name=name,
                    short_name=name[:500] if len(name) > 500 else name,
                    slug=unique_slug,
                    register_opk=False,
                    strategic=False,
                )
                orgs_to_create.append(org)
            
            # –®–ê–ì 3: –ú–∞—Å—Å–æ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Å –∏–Ω–¥–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            self.stdout.write(f"      –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –ø–∞—á–∫–∞–º–∏ –ø–æ 500...")
            
            batch_size = 500
            created_count = 0
            
            for batch in batch_iterator(orgs_to_create, batch_size):
                Organization.objects.bulk_create(batch, batch_size=batch_size)
                created_count += len(batch)
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 5000 –∑–∞–ø–∏—Å–µ–π
                if created_count % 5000 == 0 or created_count == new_count:
                    percent = (created_count / new_count) * 100
                    self.stdout.write(f"         –°–æ–∑–¥–∞–Ω–æ {created_count}/{new_count} ({percent:.1f}%)")
            
            # –®–ê–ì 4: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞
            self.stdout.write(f"      –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞...")
            
            created_names = [o.name for o in orgs_to_create]
            for batch in batch_iterator(created_names, 1000):
                for org in Organization.objects.filter(name__in=batch):
                    org_map[org.name] = org
                    self.organization_cache[org.name] = org

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –≤ –º–∞–ø–ø–∏–Ω–≥
        org_map.update(existing_orgs)
        
        self.stdout.write(f"      ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π: {len(org_map)}")
        
        return org_map

    def _create_persons_from_dataframe(self, persons_df: pd.DataFrame, pbar) -> Dict:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ª—é–¥–µ–π –∏–∑ DataFrame (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥, –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        """
        return self._create_persons_bulk(persons_df)

    def _create_organizations_from_dataframe(self, orgs_df: pd.DataFrame, pbar) -> Dict:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏–∑ DataFrame (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥, –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        """
        return self._create_organizations_bulk(orgs_df)

    def _delete_author_relations(self, ip_ids: List[int], pbar):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤"""
        delete_batch_size = 500
        for i in range(0, len(ip_ids), delete_batch_size):
            batch_ids = ip_ids[i:i+delete_batch_size]
            IPObject.authors.through.objects.filter(
                ipobject_id__in=batch_ids
            ).delete()
            pbar.update(len(batch_ids))

    def _delete_holder_person_relations(self, ip_ids: List[int], pbar):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–ª—é–¥–µ–π"""
        delete_batch_size = 500
        for i in range(0, len(ip_ids), delete_batch_size):
            batch_ids = ip_ids[i:i+delete_batch_size]
            IPObject.owner_persons.through.objects.filter(
                ipobject_id__in=batch_ids
            ).delete()
            pbar.update(len(batch_ids))

    def _delete_holder_org_relations(self, ip_ids: List[int], pbar):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        delete_batch_size = 500
        for i in range(0, len(ip_ids), delete_batch_size):
            batch_ids = ip_ids[i:i+delete_batch_size]
            IPObject.owner_organizations.through.objects.filter(
                ipobject_id__in=batch_ids
            ).delete()
            pbar.update(len(batch_ids))

    def _create_author_relations(self, relations: List[Tuple[int, int]], pbar):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤"""
        create_batch_size = 2000
        for batch in batch_iterator(relations, create_batch_size):
            through_objs = [
                IPObject.authors.through(
                    ipobject_id=ip_id,
                    person_id=person_id
                )
                for ip_id, person_id in batch
            ]
            IPObject.authors.through.objects.bulk_create(
                through_objs, batch_size=2000, ignore_conflicts=True
            )
            pbar.update(len(batch))

    def _create_holder_person_relations(self, relations: List[Tuple[int, int]], pbar):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–ª—é–¥–µ–π"""
        create_batch_size = 2000
        for batch in batch_iterator(relations, create_batch_size):
            through_objs = [
                IPObject.owner_persons.through(
                    ipobject_id=ip_id,
                    person_id=person_id
                )
                for ip_id, person_id in batch
            ]
            IPObject.owner_persons.through.objects.bulk_create(
                through_objs, batch_size=2000, ignore_conflicts=True
            )
            pbar.update(len(batch))

    def _create_holder_org_relations(self, relations: List[Tuple[int, int]], pbar):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        create_batch_size = 2000
        for batch in batch_iterator(relations, create_batch_size):
            through_objs = [
                IPObject.owner_organizations.through(
                    ipobject_id=ip_id,
                    organization_id=org_id
                )
                for ip_id, org_id in batch
            ]
            IPObject.owner_organizations.through.objects.bulk_create(
                through_objs, batch_size=2000, ignore_conflicts=True
            )
            pbar.update(len(batch))
```


-----

# –§–∞–π–ª: management\parsers\utility_model.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
"""

from intellectual_property.models import IPType
from .base import BaseFIPSParser


class UtilityModelParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='utility-model').first()

    def get_required_columns(self):
        return ['registration number', 'utility model name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}

```


-----

# –§–∞–π–ª: management\parsers\__init__.py

```
"""
–ü–∞–∫–µ—Ç —Å –ø–∞—Ä—Å–µ—Ä–∞–º–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –†–ò–î
"""

from .invention import InventionParser
from .utility_model import UtilityModelParser
from .industrial_design import IndustrialDesignParser
from .integrated_circuit import IntegratedCircuitTopologyParser
from .computer_program import ComputerProgramParser
from .database import DatabaseParser

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –∏–∑ –ø–æ–¥–ø–∞–∫–µ—Ç–∞ processors
from .processors import (
    RussianTextProcessor,
    OrganizationNormalizer,
    PersonNameFormatter,
    RIDNameFormatter,
    EntityTypeDetector
)

__all__ = [
    'InventionParser',
    'UtilityModelParser',
    'IndustrialDesignParser',
    'IntegratedCircuitTopologyParser',
    'ComputerProgramParser',
    'DatabaseParser',
    'RussianTextProcessor',
    'OrganizationNormalizer',
    'PersonNameFormatter',
    'RIDNameFormatter',
    'EntityTypeDetector',
]
```


-----

# –§–∞–π–ª: management\parsers\processors\entity_detector.py

```
"""
–î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
"""

# –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –ø–∞–∫–µ—Ç–∞ (.text_processor)
from .text_processor import RussianTextProcessor


class EntityTypeDetector:
    """
    –î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –∏–º–µ–Ω–µ–º —á–µ–ª–æ–≤–µ–∫–∞ –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
    """

    def __init__(self, cache_size: int = 50000):
        self.processor = RussianTextProcessor()
        # –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —á—Ç–æ–±—ã –Ω–µ –≤—ã–∑—ã–≤–∞—Ç—å Natasha –ø–æ–≤—Ç–æ—Ä–Ω–æ
        self.cache = {}
        self.cache_size = cache_size
        self.cache_hits = 0
        self.cache_misses = 0

    def detect_type(self, text: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha

        Args:
            text: –ù–∞–∑–≤–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–§–ò–û –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)

        Returns:
            'person' –∏–ª–∏ 'organization'
        """
        if not text or len(text) < 2:
            return 'organization'

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if text in self.cache:
            self.cache_hits += 1
            return self.cache[text]

        self.cache_misses += 1

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å Natasha –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        # is_person() –≤–Ω—É—Ç—Ä–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç NER –∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã Natasha
        if self.processor.is_person(text):
            result = 'person'
        else:
            result = 'organization'

        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞–∑–º–µ—Ä–∞
        self._add_to_cache(text, result)

        return result

    def detect_type_batch(self, texts: list) -> dict:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤

        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å {—Ç–µ–∫—Å—Ç: —Ç–∏–ø}
        """
        result = {}

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        to_process = []
        for text in texts:
            if text in self.cache:
                result[text] = self.cache[text]
                self.cache_hits += 1
            else:
                to_process.append(text)
                self.cache_misses += 1

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
        for text in to_process:
            if self.processor.is_person(text):
                result[text] = 'person'
            else:
                result[text] = 'organization'
            self._add_to_cache(text, result[text])

        return result

    def _add_to_cache(self, text: str, result: str):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫—ç—à —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞–∑–º–µ—Ä–∞
        """
        if len(self.cache) >= self.cache_size:
            # –û—á–∏—â–∞–µ–º 20% —Å–∞–º—ã—Ö —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
            items = list(self.cache.items())
            self.cache = dict(items[-int(self.cache_size * 0.8):])

        self.cache[text] = result

    def get_cache_stats(self) -> dict:
        """
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        """
        total = self.cache_hits + self.cache_misses
        return {
            'size': len(self.cache),
            'hits': self.cache_hits,
            'misses': self.cache_misses,
            'hit_ratio': self.cache_hits / total if total > 0 else 0
        }

    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        self.cache.clear()
        self.cache_hits = 0
        self.cache_misses = 0
```


-----

# –§–∞–π–ª: management\parsers\processors\organization.py

```
"""
–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)
"""

import re
from typing import Dict, Any

import pandas as pd

from core.models import OrganizationNormalizationRule

# –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –ø–∞–∫–µ—Ç–∞ (.text_processor)
from .text_processor import RussianTextProcessor


class OrganizationNormalizer:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)"""

    def __init__(self):
        self.rules_cache = None
        self.processor = RussianTextProcessor()
        self.load_rules()

    def load_rules(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∏–∑ –ë–î"""
        try:
            rules = OrganizationNormalizationRule.objects.all().order_by('priority')
            self.rules_cache = [
                {
                    'original': rule.original_text.lower(),
                    'replacement': rule.replacement_text.lower(),
                    'type': rule.rule_type,
                    'priority': rule.priority
                }
                for rule in rules
            ]
        except Exception as e:
            self.rules_cache = []
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏, –Ω–æ –Ω–µ –ø–∞–¥–∞–µ–º

    def normalize_for_search(self, name: str) -> Dict[str, Any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –¢–û–õ–¨–ö–û –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        –°–∞–º–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –≤ CSV
        """
        if pd.isna(name) or not name:
            return {'normalized': '', 'keywords': [], 'original': name}

        original = str(name).strip()
        name_lower = original.lower()

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ –ë–î –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        normalized = name_lower
        if self.rules_cache:
            for rule in self.rules_cache:
                try:
                    if rule['type'] == 'ignore':
                        pattern = r'\b' + re.escape(rule['original']) + r'\b'
                        normalized = re.sub(pattern, '', normalized)
                    else:
                        pattern = r'\b' + re.escape(rule['original']) + r'\b'
                        normalized = re.sub(pattern, rule['replacement'], normalized)
                except Exception:
                    continue

        # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞
        normalized = re.sub(r'["\'¬´¬ª‚Äû‚Äú‚Äù]', '', normalized)
        normalized = re.sub(r'[^\w\s-]', ' ', normalized)
        normalized = ' '.join(normalized.split())

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        keywords = []

        # –°–ª–æ–≤–∞ –≤ –∫–∞–≤—ã—á–∫–∞—Ö
        quoted = re.findall(r'"([^"]+)"', original)
        for q in quoted:
            words = q.lower().split()
            keywords.extend([w for w in words if len(w) > 3])

        # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        abbrs = re.findall(r'\b[–ê-–Ø–ÅA-Z]{2,}\b', original)
        keywords.extend([a.lower() for a in abbrs if len(a) >= 2])

        # –ö–æ–¥—ã (–ò–ù–ù, –û–ì–†–ù –∏ —Ç.–¥.)
        codes = re.findall(r'\b\d{10,}\b', original)
        keywords.extend(codes)

        return {
            'normalized': normalized,
            'keywords': list(set(keywords)),
            'original': original,
        }

    def format_organization_name(self, name: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        return name

```


-----

# –§–∞–π–ª: management\parsers\processors\person.py

```
"""
–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω –ª—é–¥–µ–π
"""
from .text_processor import RussianTextProcessor


class PersonNameFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω –ª—é–¥–µ–π"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def format(self, name: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –§–ò–û"""
        return self.processor.format_person_name(name)

```


-----

# –§–∞–π–ª: management\parsers\processors\rid.py

```
"""
–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –†–ò–î
"""

from .text_processor import RussianTextProcessor


class RIDNameFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –†–ò–î"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def format(self, text: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –†–ò–î"""
        if not text or not isinstance(text, str):
            return text

        if len(text.strip()) <= 1:
            return text

        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ –¥–µ–ª–∞–µ–º –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –∑–∞–≥–ª–∞–≤–Ω–æ–π
        words = text.lower().split()
        if words:
            words[0] = words[0][0].upper() + words[0][1:]
        return ' '.join(words)

```


-----

# –§–∞–π–ª: management\parsers\processors\text_processor.py

```
"""
–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º natasha
"""

from natasha import (
    Segmenter,
    MorphVocab,
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    NewsNERTagger,
    Doc,
    NamesExtractor
)


class RussianTextProcessor:
    """
    –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º natasha
    """

    # –°–ø–∏—Å–æ–∫ —Ä–∏–º—Å–∫–∏—Ö —Ü–∏—Ñ—Ä
    ROMAN_NUMERALS = {
        'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X',
        'XI', 'XII', 'XIII', 'XIV', 'XV', 'XVI', 'XVII', 'XVIII', 'XIX', 'XX',
        'XXI', 'XXII', 'XXIII', 'XXIV', 'XXV', 'XXX', 'XL', 'L', 'LX', 'XC',
        'C', 'CD', 'D', 'DC', 'CM', 'M'
    }

    # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
    ORG_ABBR = {
        '–û–û–û', '–ó–ê–û', '–û–ê–û', '–ê–û', '–ü–ê–û', '–ù–ê–û',
        '–§–ì–£–ü', '–§–ì–ë–£', '–§–ì–ê–û–£', '–§–ì–ê–£', '–§–ì–ö–£',
        '–ù–ò–ò', '–ö–ë', '–û–ö–ë', '–°–ö–ë', '–¶–ö–ë', '–ü–ö–ë',
        '–ù–ü–û', '–ù–ü–ü', '–ù–ü–§', '–ù–ü–¶', '–ù–ò–¶',
        '–ú–£–ü', '–ì–£–ü', '–ò–ß–ü', '–¢–û–û', '–ê–û–ó–¢', '–ê–û–û–¢',
        '–†–§', '–†–ê–ù', '–°–û –†–ê–ù', '–£—Ä–û –†–ê–ù', '–î–í–û –†–ê–ù',
        '–ú–ì–£', '–°–ü–±–ì–£', '–ú–§–¢–ò', '–ú–ò–§–ò', '–ú–ì–¢–£', '–ú–ê–ò',
        '–õ–¢–î', '–ò–ù–ö', '–ö–û', '–ì–ú–ë–•', '–ê–ì', '–°–ê', '–ù–í', '–ë–í', '–°–ï',
        '–ö–æ', 'Ltd', 'Inc', 'GmbH', 'AG', 'SA', 'NV', 'BV', 'SE',
    }

    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ natasha
        self.segmenter = Segmenter()
        self.morph_vocab = MorphVocab()
        self.emb = NewsEmbedding()
        self.morph_tagger = NewsMorphTagger(self.emb)
        self.syntax_parser = NewsSyntaxParser(self.emb)
        self.ner_tagger = NewsNERTagger(self.emb)
        self.names_extractor = NamesExtractor(self.morph_vocab)

        # –ö—ç—à–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.doc_cache = {}
        self.morph_cache = {}

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã –≤ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        self.ORG_ABBR.update(self.ROMAN_NUMERALS)

    def get_doc(self, text: str) -> Doc:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not text:
            return None

        if text in self.doc_cache:
            return self.doc_cache[text]

        doc = Doc(text)
        doc.segment(self.segmenter)
        doc.tag_morph(self.morph_tagger)
        doc.parse_syntax(self.syntax_parser)
        doc.tag_ner(self.ner_tagger)

        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
        for token in doc.tokens:
            token.lemmatize(self.morph_vocab)

        for span in doc.spans:
            span.normalize(self.morph_vocab)

        self.doc_cache[text] = doc
        return doc

    def is_roman_numeral(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∏–º—Å–∫—É—é —Ü–∏—Ñ—Ä—É"""
        if not text:
            return False
        clean_text = text.strip('.,;:!?()').upper()
        return clean_text in self.ROMAN_NUMERALS

    def is_abbr(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        if not text:
            return False
        clean_text = text.strip('.,;:!?()').upper()
        return clean_text in self.ORG_ABBR

    def is_person(self, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –§–ò–û —á–µ–ª–æ–≤–µ–∫–∞"""
        if not text or len(text) < 6:
            return False

        # –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        if any(ind in text for ind in self.ORG_ABBR if len(ind) > 2):
            return False

        org_indicators = ['–û–±—â–µ—Å—Ç–≤–æ', '–ö–æ–º–ø–∞–Ω–∏—è', '–ö–æ—Ä–ø–æ—Ä–∞—Ü–∏—è', '–ó–∞–≤–æ–¥',
                         '–ò–Ω—Å—Ç–∏—Ç—É—Ç', '–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '–ê–∫–∞–¥–µ–º–∏—è', '–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è',
                         '–§–∏—Ä–º–∞', '–¶–µ–Ω—Ç—Ä']

        if any(ind.lower() in text.lower() for ind in org_indicators):
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ NER
        doc = self.get_doc(text)
        if doc and doc.spans:
            for span in doc.spans:
                if span.type == 'PER':
                    return True

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –§–ò–û
        words = text.split()
        if 2 <= len(words) <= 4:
            name_like = 0
            for word in words:
                clean = word.rstrip('.,')
                if clean and clean[0].isupper() and len(clean) > 1:
                    name_like += 1
            return name_like >= len(words) - 1

        return False

    def extract_person_parts(self, text: str) -> dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π –§–ò–û —Å –ø–æ–º–æ—â—å—é natasha"""
        matches = list(self.names_extractor(text))
        if matches:
            fact = matches[0].fact
            parts = []
            if fact.last:
                parts.append(fact.last)
            if fact.first:
                parts.append(fact.first)
            if fact.middle:
                parts.append(fact.middle)

            return {
                'last': fact.last or '',
                'first': fact.first or '',
                'middle': fact.middle or '',
                'full': ' '.join(parts)
            }

        # Fallback: —Ä—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥
        return self._parse_name_manually(text)

    def _parse_name_manually(self, text: str) -> dict:
        """–†—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –∏–º–µ–Ω–∏"""
        words = text.split()

        if len(words) == 3:
            return {
                'last': words[0],
                'first': words[1],
                'middle': words[2],
                'full': text
            }
        elif len(words) == 2:
            return {
                'last': words[0],
                'first': words[1],
                'middle': '',
                'full': text
            }
        else:
            return {
                'last': text,
                'first': '',
                'middle': '',
                'full': text
            }

    def format_person_name(self, name: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –§–ò–û —á–µ–ª–æ–≤–µ–∫–∞"""
        if not name:
            return name

        parts = self.extract_person_parts(name)
        if parts.get('full'):
            return parts['full']

        return name

```


-----

# –§–∞–π–ª: management\parsers\processors\__init__.py

```
from .text_processor import RussianTextProcessor
from .organization import OrganizationNormalizer
from .person import PersonNameFormatter
from .rid import RIDNameFormatter
from .entity_detector import EntityTypeDetector

__all__ = [
    'RussianTextProcessor',
    'OrganizationNormalizer',
    'PersonNameFormatter',
    'RIDNameFormatter',
    'EntityTypeDetector',
]
```


-----

# –§–∞–π–ª: management\utils\csv_loader.py

```
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ CSV —Ñ–∞–π–ª–æ–≤
"""

import pandas as pd


def load_csv_with_strategies(file_path, encoding, delimiter, stdout=None):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏
    """
    strategies = [
        {'encoding': encoding, 'delimiter': delimiter, 'skipinitialspace': True},
        {'encoding': 'cp1251', 'delimiter': delimiter, 'skipinitialspace': True},
        {'encoding': 'utf-8', 'delimiter': ';', 'skipinitialspace': True},
        {'encoding': 'cp1251', 'delimiter': ';', 'skipinitialspace': True},
        {'encoding': 'utf-8', 'delimiter': '\t', 'skipinitialspace': True},
    ]

    for strategy in strategies:
        try:
            df = pd.read_csv(file_path, **strategy, dtype=str, keep_default_na=False)
            if stdout:
                stdout.write(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {strategy}")

            df.columns = [col.strip().strip('\ufeff').strip('"') for col in df.columns]
            return df
        except Exception:
            continue

    raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π")

```


-----

# –§–∞–π–ª: management\utils\filters.py

```
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ DataFrame
"""

from datetime import datetime
import pandas as pd


def filter_by_registration_year(df, min_year, stdout=None):
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è DataFrame –ø–æ –≥–æ–¥—É —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
    """
    def extract_year(date_str):
        try:
            if pd.isna(date_str) or not date_str:
                return None

            date_str = str(date_str).strip()
            if not date_str:
                return None

            for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
                try:
                    return datetime.strptime(date_str, fmt).year
                except (ValueError, TypeError):
                    continue

            try:
                return pd.to_datetime(date_str).year
            except (ValueError, TypeError):
                return None
        except:
            return None

    if stdout:
        stdout.write("  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≥–æ–¥—É —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏...")

    if 'registration date' not in df.columns:
        if stdout:
            stdout.write("  ‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'registration date' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –≥–æ–¥—É")
        return df

    df['_year'] = df['registration date'].apply(extract_year)

    if stdout:
        # –§–∏–ª—å—Ç—Ä—É–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        valid_years = df['_year'].dropna()
        if not valid_years.empty:
            years_dist = valid_years.value_counts().sort_index()
            years_list = list(years_dist.items())
            if len(years_list) > 0:
                stdout.write(f"     –î–∏–∞–ø–∞–∑–æ–Ω –≥–æ–¥–æ–≤: {years_list[0][0]:.0f} - {years_list[-1][0]:.0f}")
                stdout.write(f"     –ü–µ—Ä–≤—ã–µ 5: {years_list[:5]}")
                stdout.write(f"     –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5: {years_list[-5:]}")

    filtered_df = df[df['_year'] >= min_year].copy() if '_year' in df.columns else df.copy()
    if '_year' in filtered_df.columns:
        filtered_df.drop('_year', axis=1, inplace=True)

    return filtered_df


def filter_by_actual(df, stdout=None):
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è DataFrame –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (actual = True)
    """
    def parse_actual(value):
        if pd.isna(value) or not value:
            return False
        value = str(value).lower().strip()
        return value in ['1', 'true', 'yes', '–¥–∞', '–¥–µ–π—Å—Ç–≤—É–µ—Ç', 't', '1.0', '–∞–∫—Ç–∏–≤–µ–Ω']

    if 'actual' not in df.columns:
        if stdout:
            stdout.write("  ‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'actual' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏")
        return df

    df['_actual'] = df['actual'].apply(parse_actual)
    filtered_df = df[df['_actual'] == True].copy()
    filtered_df.drop('_actual', axis=1, inplace=True)

    return filtered_df


def apply_filters(df, min_year, only_active, stdout=None):
    """
    –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∫ DataFrame
    """
    original_count = len(df)

    if min_year is not None:
        df = filter_by_registration_year(df, min_year, stdout)

    if only_active:
        df = filter_by_actual(df, stdout)

    filtered_count = len(df)
    if stdout and filtered_count < original_count:
        stdout.write(f"  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {original_count} ‚Üí {filtered_count} –∑–∞–ø–∏—Å–µ–π")

    return df

```


-----

# –§–∞–π–ª: management\utils\progress.py

```
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–≤
–í—ã–Ω–µ—Å–µ–Ω—ã –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –ø–∞—Ä—Å–µ—Ä–∞—Ö
"""

import sys
from tqdm import tqdm
from contextlib import contextmanager
from typing import Optional, Iterable, Iterator, Any


class ProgressManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞–º–∏
    –í—Å–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º tqdm)
    """
    
    def __init__(self, enabled: bool = True, file=sys.stdout):
        self.enabled = enabled
        self.file = file
        self._current_bar = None  # –¢–µ–∫—É—â–∏–π –∞–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
    
    @contextmanager
    def task(self, description: str, total: Optional[int] = None, unit: str = "—ç–ª–µ–º"):
        """
        –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∑–∞–¥–∞—á–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        –í—Å–µ –∑–∞–¥–∞—á–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É (–ø—Ä–µ–¥—ã–¥—É—â–∞—è –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è)
        """
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π –±–∞—Ä, –∑–∞–∫—Ä—ã–≤–∞–µ–º –µ–≥–æ
        if self._current_bar is not None:
            self._current_bar.close()
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        bar = tqdm(
            total=total,
            desc=description,
            unit=unit,
            file=self.file,
            leave=False,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
        )
        
        self._current_bar = bar
        
        try:
            yield bar
        finally:
            bar.close()
            self._current_bar = None
            print(file=self.file)
    
    @contextmanager
    def subtask(self, description: str, total: Optional[int] = None, unit: str = "—ç–ª–µ–º"):
        """–ê–ª–∏–∞—Å –¥–ª—è task (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        with self.task(description, total, unit) as bar:
            yield bar
    
    def step(self, message: str):
        """–í—ã–≤–æ–¥ —Å–æ–æ–±—â–µ–Ω–∏—è –æ —à–∞–≥–µ (–≤—Å–µ–≥–¥–∞ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)"""
        if self._current_bar is not None:
            self._current_bar.write(f"üîπ {message}")
        else:
            print(f"üîπ {message}", file=self.file)

    def success(self, message: str):
        """–í—ã–≤–æ–¥ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± —É—Å–ø–µ—Ö–µ"""
        if self._current_bar is not None:
            self._current_bar.write(f"‚úÖ {message}")
        else:
            print(f"‚úÖ {message}", file=self.file)

    def warning(self, message: str):
        """–í—ã–≤–æ–¥ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è"""
        if self._current_bar is not None:
            self._current_bar.write(f"‚ö†Ô∏è {message}")
        else:
            print(f"‚ö†Ô∏è {message}", file=self.file)

    def error(self, message: str):
        """–í—ã–≤–æ–¥ –æ—à–∏–±–∫–∏"""
        if self._current_bar is not None:
            self._current_bar.write(f"‚ùå {message}")
        else:
            print(f"‚ùå {message}", file=self.file)


def batch_iterator(iterable, batch_size: int):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π –æ–±—ä–µ–∫—Ç –Ω–∞ –±–∞—Ç—á–∏"""
    batch = []
    for item in iterable:
        batch.append(item)
        if len(batch) >= batch_size:
            yield batch
            batch = []
    if batch:
        yield batch
```


-----

# –§–∞–π–ª: management\utils\__init__.py

```
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –ø–∞—Ä—Å–µ—Ä–æ–≤
"""

from .csv_loader import load_csv_with_strategies
from .filters import apply_filters
from .progress import ProgressManager, batch_iterator

__all__ = [
    'load_csv_with_strategies',
    'apply_filters',
    'ProgressManager',
    'batch_iterator',
]
```


-----

# –§–∞–π–ª: models\models_fips_catalogue.py

```
from django.db import models

from intellectual_property.models.models_ip import IPType


class FipsOpenDataCatalogue(models.Model):
    """
    –ú–æ–¥–µ–ª—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞.
    """
    
    ip_type = models.ForeignKey(
        IPType,
        on_delete=models.PROTECT,
        verbose_name='–¢–∏–ø –†–ò–î',
        help_text='–¢–∏–ø –†–ò–î, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫–∞—Ç–∞–ª–æ–≥',
        related_name='catalogues',
        db_index=True
    )
    
    name = models.CharField(
        max_length=255,
        verbose_name='–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞',
        help_text='–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞)',
        blank=False,
        null=False
    )
    
    catalogue_file = models.FileField(
        upload_to='ip_catalogue/%Y/%m/',
        verbose_name='CSV —Ñ–∞–π–ª –∫–∞—Ç–∞–ª–æ–≥–∞',
        help_text='CSV —Ñ–∞–π–ª –∫–∞—Ç–∞–ª–æ–≥–∞ —Å —Å–∞–π—Ç–∞ –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞',
        blank=True,
        null=True
    )
    
    publication_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∫–∞—Ç–∞–ª–æ–≥–∞',
        help_text='–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∫–∞—Ç–∞–ª–æ–≥–∞ –Ω–∞ —Å–∞–π—Ç–µ –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞',
        blank=True,
        null=True,
        db_index=True
    )
    
    upload_date = models.DateTimeField(
        verbose_name='–î–∞—Ç–∞ –∑–∞–≥—Ä—É–∑–∫–∏',
        help_text='–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ –≤ —Å–∏—Å—Ç–µ–º—É',
        auto_now_add=True
    )
    
    last_modified = models.DateTimeField(
        verbose_name='–î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è',
        help_text='–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏',
        auto_now=True
    )
    
    description = models.TextField(
        verbose_name='–û–ø–∏—Å–∞–Ω–∏–µ',
        help_text='–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–∞—Ç–∞–ª–æ–≥–µ',
        blank=True
    )
    
    parsed_date = models.DateTimeField(
        verbose_name='–î–∞—Ç–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞',
        help_text='–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è, –∫–æ–≥–¥–∞ –∫–∞—Ç–∞–ª–æ–≥ –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ø–∞—Ä—Å–µ—Ä–æ–º',
        blank=True,
        null=True,
        db_index=True
    )
    
    def __str__(self):
        if self.name:
            return self.name
        date_str = self.publication_date.strftime('%d.%m.%Y') if self.publication_date else '–î–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞'
        return f"–ö–∞—Ç–∞–ª–æ–≥ {self.ip_type.name} –æ—Ç {date_str}"
    
    class Meta:
        verbose_name = '–ö–∞—Ç–∞–ª–æ–≥ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–°'
        verbose_name_plural = '–ö–∞—Ç–∞–ª–æ–≥–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–°'
        ordering = ['-publication_date', '-upload_date']
        indexes = [
            models.Index(fields=['ip_type', 'publication_date']),
            models.Index(fields=['publication_date']),
            models.Index(fields=['name']),
        ]
        unique_together = ['id', 'publication_date']

```


-----

# –§–∞–π–ª: models\models_ip.py

```
from django.db import models
from common.utils.text import TextUtils
from core.models.models_foiv import FOIV, RFRepresentative
from core.models.models_geo import Country
from core.models.models_it import DBMS, OperatingSystem, ProgrammingLanguage
from core.models.models_organization import Organization
from core.models.models_person import Person

class ProtectionDocumentType(models.Model):
    """
    –ú–æ–¥–µ–ª—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∏–¥–æ–≤ –æ—Ö—Ä–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    """
    
    name = models.CharField(
        max_length=255,
        verbose_name='–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ',
        help_text='–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –≤–∏–¥–∞ –æ—Ö—Ä–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞',
        unique=True,
        db_index=True
    )
    
    description = models.TextField(
        verbose_name='–û–ø–∏—Å–∞–Ω–∏–µ',
        help_text='–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∏–¥–∞ –æ—Ö—Ä–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞',
        blank=True
    )
    
    slug = models.SlugField(
        max_length=255,
        verbose_name='URL-–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä',
        help_text='–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ URL',
        unique=True,
        db_index=True
    )
    
    class Meta:
        verbose_name = '–í–∏–¥ –æ—Ö—Ä–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞'
        verbose_name_plural = '–í–∏–¥—ã –æ—Ö—Ä–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤'
        ordering = ['name']
        indexes = [
            models.Index(fields=['name']),
        ]
    
    def __str__(self):
        return self.name
    
    def save(self, *args, **kwargs):
        if not self.slug and self.name:
            self.slug = TextUtils.generate_slug(
                self,
                slug_field_name='slug'
            )[:520]
        super().save(*args, **kwargs)


class IPType(models.Model):
    """
    –ú–æ–¥–µ–ª—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–∏–ø–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–†–ò–î).
    """
    
    name = models.CharField(
        max_length=255,
        verbose_name='–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ',
        help_text='–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –†–ò–î',
        unique=True,
        db_index=True
    )
    
    description = models.TextField(
        verbose_name='–û–ø–∏—Å–∞–Ω–∏–µ',
        help_text='–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–∏–ø–∞ –†–ò–î –∏ –µ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫',
        blank=True
    )
    
    protection_document_type = models.ForeignKey(
        ProtectionDocumentType,
        on_delete=models.PROTECT,
        verbose_name='–í–∏–¥ –æ—Ö—Ä–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞',
        help_text='–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞, —É–¥–æ—Å—Ç–æ–≤–µ—Ä—è—é—â–µ–≥–æ –æ—Ö—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∞ –Ω–∞ –¥–∞–Ω–Ω—ã–π –†–ò–î',
        related_name='ip_types',
        db_index=True
    )
    
    validity_duration = models.PositiveIntegerField(
        verbose_name='–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è, –ª–µ—Ç',
        help_text='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è –æ—Ö—Ä–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞',
        blank=True,
        null=True
    )
    
    slug = models.SlugField(
        max_length=255,
        verbose_name='URL-–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä',
        help_text='–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ URL',
        unique=True,
        db_index=True
    )
    
    class Meta:
        verbose_name = '–¢–∏–ø –†–ò–î'
        verbose_name_plural = '–¢–∏–ø—ã –†–ò–î'
        ordering = ['id']
        indexes = [
            models.Index(fields=['name', 'protection_document_type']),
        ]
    
    def __str__(self):
        return self.name
    
    def save(self, *args, **kwargs):
        if not self.slug and self.name:
            self.slug = TextUtils.generate_slug(
                self,
                slug_field_name='slug'
            )[:520]
        super().save(*args, **kwargs)
    
    @property
    def protection_document_name(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ö—Ä–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        return self.protection_document_type.name if self.protection_document_type else None


class AdditionalPatent(models.Model):
    """
    –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç—ã, –≤—ã–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–¥–ª–µ–Ω–∏—è —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è.
    """
    patent_number = models.CharField(
        max_length=50,
        verbose_name='–ù–æ–º–µ—Ä –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ç–µ–Ω—Ç–∞',
        null=True,
        blank=True,
        db_index=True
    )
    patent_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –≤—ã–¥–∞—á–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ç–µ–Ω—Ç–∞',
        null=True,
        blank=True
    )
    description = models.TextField(
        verbose_name='–û–ø–∏—Å–∞–Ω–∏–µ',
        blank=True
    )
    
    class Meta:
        verbose_name = '–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ç–µ–Ω—Ç'
        verbose_name_plural = '–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç—ã'
    
    def __str__(self):
        return self.patent_number


class IPImage(models.Model):
    """
    –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –†–ò–î (—á–µ—Ä—Ç–µ–∂–∏, —Ä–µ–Ω–¥–µ—Ä—ã, —Å–∫—Ä–∏–Ω—à–æ—Ç—ã).
    """
    image = models.ImageField(
        upload_to='ip_images/%Y/%m/',
        verbose_name='–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'
    )
    title = models.CharField(
        max_length=255,
        verbose_name='–ù–∞–∑–≤–∞–Ω–∏–µ',
        blank=True
    )
    description = models.TextField(
        verbose_name='–û–ø–∏—Å–∞–Ω–∏–µ',
        blank=True
    )
    is_main = models.BooleanField(
        default=False,
        verbose_name='–ì–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'
    )
    sort_order = models.PositiveSmallIntegerField(
        default=0,
        verbose_name='–ü–æ—Ä—è–¥–æ–∫ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏'
    )
    
    class Meta:
        verbose_name = '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –†–ò–î'
        verbose_name_plural = '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –†–ò–î'
        ordering = ['sort_order', 'id']
    
    def __str__(self):
        return self.title or f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {self.id}"


class IPObject(models.Model):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –†–ò–î.
    """
    
    # === –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ===
    name = models.CharField(
        max_length=500,
        verbose_name='–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –†–ò–î',
        db_index=True
    )
    
    ip_type = models.ForeignKey(
        IPType,
        on_delete=models.PROTECT,
        verbose_name='–í–∏–¥ –†–ò–î',
        related_name='ip_objects',
        null=True,
        blank=True,
        db_index=True
    )
    
    actual = models.BooleanField(
        default=True,
        verbose_name='–ü—Ä–∏–∑–Ω–∞–∫ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–æ–≤–æ–π –æ—Ö—Ä–∞–Ω—ã',
        help_text='–û—Ç–º–µ—Ç—å—Ç–µ, –µ—Å–ª–∏ –ø—Ä–∞–≤–æ–≤–∞—è –æ—Ö—Ä–∞–Ω–∞ –¥–µ–π—Å—Ç–≤—É–µ—Ç',
        db_index=True
    )
    
    # === –ê–≤—Ç–æ—Ä—ã –∏ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏ ===
    authors = models.ManyToManyField(
        Person,
        related_name='authored_ip_objects',
        verbose_name='–ê–≤—Ç–æ—Ä—ã',
        blank=True
    )
    
    owner_organizations = models.ManyToManyField(
        Organization,
        related_name='owned_ip_objects_organization',
        verbose_name='–ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏ (–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)',
        blank=True
    )
    owner_persons = models.ManyToManyField(
        Person,
        related_name='owned_ip_objects_person',
        verbose_name='–ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏ (—Ñ–∏–∑. –ª–∏—Ü–∞)',
        blank=True
    )
    
    # === –î–∞—Ç—ã –∏ —Å—Ä–æ–∫–∏ ===
    creation_year = models.PositiveSmallIntegerField(
        verbose_name='–ì–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è',
        null=True,
        blank=True
    )
    
    publication_year = models.PositiveSmallIntegerField(
        verbose_name='–ì–æ–¥ –æ–±–Ω–∞—Ä–æ–¥–æ–≤–∞–Ω–∏—è',
        null=True,
        blank=True
    )
    
    update_year = models.PositiveSmallIntegerField(
        verbose_name='–ì–æ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è',
        null=True,
        blank=True
    )
    
    application_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏ –Ω–∞ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—É—é —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é',
        null=True,
        blank=True
    )
    
    registration_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏',
        null=True,
        blank=True
    )
    
    patent_starting_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –æ—Ç—Å—á–µ—Ç–∞ —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–∞—Ç–µ–Ω—Ç–∞',
        null=True,
        blank=True
    )
    
    expiration_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –∏—Å—Ç–µ—á–µ–Ω–∏—è —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–∞—Ç–µ–Ω—Ç–∞ / –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∞–≤–∞',
        null=True,
        blank=True
    )
    
    # === –ù–æ–º–µ—Ä–∞ –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã ===
    registration_number = models.CharField(
        max_length=50,
        verbose_name='–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–æ–º–µ—Ä –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è (–Ω–æ–º–µ—Ä –ø–∞—Ç–µ–Ω—Ç–∞)',
        blank=True,
        db_index=True
    )
    
    revoked_patent_number = models.CharField(
        max_length=50,
        verbose_name='–ù–æ–º–µ—Ä –∞–Ω–Ω—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ç–µ–Ω—Ç–∞, –ø—Ä–∏–∑–Ω–∞–Ω–Ω–æ–≥–æ –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º —á–∞—Å—Ç–∏—á–Ω–æ',
        blank=True
    )
    
    publication_url = models.URLField(
        max_length=500,
        verbose_name='URL –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ä–µ–µ—Å—Ç—Ä–∞—Ö —Å–∞–π—Ç–∞ –§–ò–ü–°',
        blank=True
    )
    
    # === –ï–≤—Ä–∞–∑–∏–π—Å–∫–∞—è –∑–∞—è–≤–∫–∞ ===
    ea_application_number = models.CharField(
        max_length=50,
        verbose_name='–ù–æ–º–µ—Ä –µ–≤—Ä–∞–∑–∏–π—Å–∫–æ–π –∑–∞—è–≤–∫–∏',
        blank=True
    )
    
    ea_application_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –µ–≤—Ä–∞–∑–∏–π—Å–∫–æ–π –∑–∞—è–≤–∫–∏',
        null=True,
        blank=True
    )
    
    ea_application_publish_number = models.CharField(
        max_length=50,
        verbose_name='–ù–æ–º–µ—Ä –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –µ–≤—Ä–∞–∑–∏–π—Å–∫–æ–π –∑–∞—è–≤–∫–∏',
        blank=True
    )
    
    ea_application_publish_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –µ–≤—Ä–∞–∑–∏–π—Å–∫–æ–π –∑–∞—è–≤–∫–∏',
        null=True,
        blank=True
    )
    
    # === PCT –∑–∞—è–≤–∫–∞ ===
    pct_application_number = models.CharField(
        max_length=50,
        verbose_name='–ù–æ–º–µ—Ä PCT –∑–∞—è–≤–∫–∏',
        blank=True
    )
    
    pct_application_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ PCT –∑–∞—è–≤–∫–∏',
        null=True,
        blank=True
    )
    
    pct_application_publish_number = models.CharField(
        max_length=50,
        verbose_name='–ù–æ–º–µ—Ä –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ PCT –∑–∞—è–≤–∫–∏',
        blank=True
    )
    
    pct_application_publish_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ PCT –∑–∞—è–≤–∫–∏',
        null=True,
        blank=True
    )
    
    pct_application_examination_start_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è –∑–∞—è–≤–∫–∏ –†–°–¢ –Ω–∞ –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ñ–∞–∑–µ',
        null=True,
        blank=True
    )
    
    # === –ü–∞—Ä–∏–∂—Å–∫–∞—è –∫–æ–Ω–≤–µ–Ω—Ü–∏—è ===
    paris_convention_priority_number = models.CharField(
        max_length=50,
        verbose_name='–ù–æ–º–µ—Ä –ø–µ—Ä–≤–æ–π –∑–∞—è–≤–∫–∏ –≤ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ - —É—á–∞—Å—Ç–Ω–∏–∫–µ –ü–∞—Ä–∏–∂—Å–∫–æ–π –∫–æ–Ω–≤–µ–Ω—Ü–∏–∏',
        blank=True
    )
    
    paris_convention_priority_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –ø–µ—Ä–≤–æ–π –∑–∞—è–≤–∫–∏ –≤ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ - —É—á–∞—Å—Ç–Ω–∏–∫–µ –ü–∞—Ä–∏–∂—Å–∫–æ–π –∫–æ–Ω–≤–µ–Ω—Ü–∏–∏',
        null=True,
        blank=True
    )
    
    paris_convention_priority_country = models.ForeignKey(
        Country,
        on_delete=models.PROTECT,
        verbose_name='–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã –ø–æ–¥–∞—á–∏ –ø–µ—Ä–≤–æ–π –∑–∞—è–≤–∫–∏',
        related_name='paris_priority_ip_objects',
        null=True,
        blank=True
    )
    
    # === –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–ø–æ–ª–æ–≥–∏–∏ ===
    first_usage_date = models.DateField(
        verbose_name='–î–∞—Ç–∞ –ø–µ—Ä–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–ø–æ–ª–æ–≥–∏–∏',
        null=True,
        blank=True
    )
    
    first_usage_countries = models.ManyToManyField(
        Country,
        related_name='first_usage_ip_objects',
        verbose_name='–°—Ç—Ä–∞–Ω–∞ (—Å—Ç—Ä–∞–Ω—ã) –ø–µ—Ä–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–ø–æ–ª–æ–≥–∏–∏',
        blank=True
    )
    
    # === –î–æ–∫—É–º–µ–Ω—Ç—ã –∏ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ ===
    information_about_the_obligation_to_conclude_contract_of_alienation = models.TextField(
        verbose_name='–°–≤–µ–¥–µ–Ω–∏—è –æ –ø–æ–¥–∞–Ω–Ω–æ–º –∑–∞—è–≤–ª–µ–Ω–∏–∏ –æ–± –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–µ –∑–∞–∫–ª—é—á–∏—Ç—å –¥–æ–≥–æ–≤–æ—Ä –æ–± –æ—Ç—á—É–∂–¥–µ–Ω–∏–∏ –ø–∞—Ç–µ–Ω—Ç–∞',
        blank=True
    )
    
    # === –¢–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è ===
    abstract = models.TextField(
        verbose_name='–†–µ—Ñ–µ—Ä–∞—Ç',
        blank=True
    )
    
    claims = models.TextField(
        verbose_name='–§–æ—Ä–º—É–ª–∞',
        blank=True
    )
    
    description = models.TextField(
        verbose_name='–û–ø–∏—Å–∞–Ω–∏–µ',
        blank=True
    )
    
    source_code_deposit = models.TextField(
        verbose_name='–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ (–¥–µ–ø–æ–Ω–∏—Ä—É–µ–º—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã)',
        blank=True
    )
    
    # === –°–≤—è–∑–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (One-to-Many) ===
    additional_patents = models.ManyToManyField(
        AdditionalPatent,
        related_name='ip_objects',
        verbose_name='–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–¥–ª–µ–Ω–∏—è —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è',
        blank=True
    )
    
    images = models.ManyToManyField(
        IPImage,
        related_name='ip_objects',
        verbose_name='–§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏–ª–∏ —Ä–µ–Ω–¥–µ—Ä—ã –∏–∑–¥–µ–ª–∏—è',
        blank=True
    )
    
    # === IT-—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞ (–¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º) ===
    programming_languages = models.ManyToManyField(
        ProgrammingLanguage,
        related_name='ip_objects',
        verbose_name='–Ø–∑—ã–∫(–∏) –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è',
        blank=True
    )
    
    dbms = models.ManyToManyField(
        DBMS,
        related_name='ip_objects',
        verbose_name='–°–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö',
        blank=True
    )
    
    operating_systems = models.ManyToManyField(
        OperatingSystem,
        related_name='ip_objects',
        verbose_name='–¶–µ–ª–µ–≤–∞—è –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞',
        blank=True
    )
    
    # === –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø–æ–ª—è ===
    created_at = models.DateTimeField(
        auto_now_add=True,
        verbose_name='–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è',
        null=True,
        blank=True
    )
    updated_at = models.DateTimeField(
        auto_now=True,
        verbose_name='–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è'
    )
    
    class Meta:
        verbose_name = '–û–±—ä–µ–∫—Ç –†–ò–î'
        verbose_name_plural = '–û–±—ä–µ–∫—Ç—ã –†–ò–î'
        indexes = [
            models.Index(fields=['name']),
            models.Index(fields=['registration_number']),
            models.Index(fields=['application_date']),
            models.Index(fields=['registration_date']),
            models.Index(fields=['expiration_date']),
            models.Index(fields=['ip_type', 'actual']),
        ]
    
    def __str__(self):
        return self.name
    
    @property
    def all_owners(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π (–≤—Å–µ—Ö —Ç–∏–ø–æ–≤)"""
        from itertools import chain
        return list(chain(
            self.owner_persons.all(),
            self.owner_organizations.all(),
            self.owner_foivs.all()
        ))
    
    @property
    def is_expired(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏—Å—Ç–µ–∫ –ª–∏ —Å—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è"""
        from django.utils import timezone
        if self.expiration_date:
            return self.expiration_date < timezone.now().date()
        return False
```


-----

# –§–∞–π–ª: models\__init__.py

```
import os
import glob

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ
model_files = glob.glob(os.path.dirname(__file__) + "/*.py")
for module in model_files:
    if not module.endswith('__init__.py'):
        module_name = os.path.basename(module)[:-3]
        exec(f"from .{module_name} import *")
```


-----

# –§–∞–π–ª: views\__init__.py

```

```
