
# –§–∞–π–ª: management\help.txt

```
# –†–µ–∂–∏–º ONLY-ACTIVE: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (actual = True)
# –†–µ–∂–∏–º MIN-YEAR 2020: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π 2020 –≥–æ–¥–∞ –∏ –ø–æ–∑–∂–µ
# –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î

–î–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ —Ç–∏–ø–∞–º –†–ò–î:
--ip-type invention ‚Äî –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è
--ip-type utility-model ‚Äî –ø–æ–ª–µ–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏
--ip-type industrial-design ‚Äî –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã
--ip-type integrated-circuit-topology ‚Äî —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º
--ip-type computer-program ‚Äî –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –≠–í–ú
--ip-type database ‚Äî –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

python manage.py pars_fips_catalogue --only-active --min-year 2020 --ip-type invention --dry-run

# –¢–µ—Å—Ç –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π
python manage.py pars_fips_catalogue --only-active --min-year 2020 --ip-type invention --max-rows 10

# –í—Å–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è
python manage.py pars_fips_catalogue --only-active --ip-type invention --force

============
    HELP
============

usage: manage.py pars_fips_catalogue [-h] [--catalogue-id CATALOGUE_ID] [--ip-type {invention,utility-model,industrial-design,integrated-circuit-topology,computer-program,database}] [--dry-run] [--encoding ENCODING]
                                     [--delimiter DELIMITER] [--batch-size BATCH_SIZE] [--min-year MIN_YEAR] [--skip-filters] [--only-active] [--max-rows MAX_ROWS] [--version] [-v {0,1,2,3}] [--settings SETTINGS]      
                                     [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color] [--skip-checks]

–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞

options:
  -h, --help            show this help message and exit
  --catalogue-id CATALOGUE_ID
                        ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
  --ip-type {invention,utility-model,industrial-design,integrated-circuit-topology,computer-program,database}
                        –¢–∏–ø –†–ò–î –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–∞—Ä—Å—è—Ç—Å—è –≤—Å–µ)
  --dry-run             –†–µ–∂–∏–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
  --encoding ENCODING   –ö–æ–¥–∏—Ä–æ–≤–∫–∞ CSV —Ñ–∞–π–ª–∞
  --delimiter DELIMITER
                        –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV —Ñ–∞–π–ª–µ
  --batch-size BATCH_SIZE
                        –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è bulk-–æ–ø–µ—Ä–∞—Ü–∏–π
  --min-year MIN_YEAR   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
  --skip-filters        –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏)
  --only-active         –ü–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç—ã (actual = True)
  --max-rows MAX_ROWS   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
  --version             Show program's version number and exit.
  -v, --verbosity {0,1,2,3}
                        Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output
  --settings SETTINGS   The Python path to a settings module, e.g. "myproject.settings.main". If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable will be used.
  --pythonpath PYTHONPATH
                        A directory to add to the Python path, e.g. "/home/djangoprojects/myproject".
  --traceback           Display a full stack trace on CommandError exceptions.
  --no-color            Don't colorize the command output.
  --force-color         Force colorization of the command output.
  --skip-checks         Skip system checks.

```


-----

# –§–∞–π–ª: management\__init__.py

```

```


-----

# –§–∞–π–ª: management\commands\pars_fips_catalogue.py

```
"""
–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞.
–û–±–µ—Ä—Ç–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–ª–µ–≥–∏—Ä—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –ø–∞—Ä—Å–µ—Ä–∞–º.
"""

import logging
import os

from django.core.management.base import BaseCommand, CommandError
from django.utils import timezone
import pandas as pd

from intellectual_property.models import FipsOpenDataCatalogue

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä—ã –∏–∑ –Ω–æ–≤–æ–≥–æ –ø–∞–∫–µ—Ç–∞
from ..parsers import (
    InventionParser, UtilityModelParser, IndustrialDesignParser,
    IntegratedCircuitTopologyParser, ComputerProgramParser, DatabaseParser
)
from ..utils.csv_loader import load_csv_with_strategies
from ..utils.filters import apply_filters

logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = '–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞'

    def add_arguments(self, parser):
        parser.add_argument('--catalogue-id', type=int, help='ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')
        parser.add_argument('--ip-type', type=str,
                        choices=['invention', 'utility-model', 'industrial-design',
                                'integrated-circuit-topology', 'computer-program', 'database'],
                        help='–¢–∏–ø –†–ò–î –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–∞—Ä—Å—è—Ç—Å—è –≤—Å–µ)')
        parser.add_argument('--dry-run', action='store_true', help='–†–µ–∂–∏–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î')
        parser.add_argument('--encoding', type=str, default='utf-8', help='–ö–æ–¥–∏—Ä–æ–≤–∫–∞ CSV —Ñ–∞–π–ª–∞')
        parser.add_argument('--delimiter', type=str, default=',', help='–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV —Ñ–∞–π–ª–µ')
        parser.add_argument('--batch-size', type=int, default=100, help='–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è bulk-–æ–ø–µ—Ä–∞—Ü–∏–π')
        parser.add_argument('--min-year', type=int, default=2000, help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏')
        parser.add_argument('--skip-filters', action='store_true', help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏)')
        parser.add_argument('--only-active', action='store_true', help='–ü–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç—ã (actual = True)')
        parser.add_argument('--max-rows', type=int, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)')
        parser.add_argument('--force', action='store_true', help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–∂–µ –µ—Å–ª–∏ –∫–∞—Ç–∞–ª–æ–≥ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω')
        parser.add_argument('--mark-processed', action='store_true',
                        help='–ü–æ–º–µ—Ç–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (–¥–∞–∂–µ –µ—Å–ª–∏ –±—ã–ª–∏ –æ—à–∏–±–∫–∏)')

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.parsers = {
            'invention': InventionParser(self),
            'utility-model': UtilityModelParser(self),
            'industrial-design': IndustrialDesignParser(self),
            'integrated-circuit-topology': IntegratedCircuitTopologyParser(self),
            'computer-program': ComputerProgramParser(self),
            'database': DatabaseParser(self),
        }

    def handle(self, *args, **options):
        self.dry_run = options['dry_run']
        self.encoding = options['encoding']
        self.delimiter = options['delimiter']
        self.batch_size = options['batch_size']
        self.min_year = options['min_year']
        self.skip_filters = options['skip_filters']
        self.only_active = options['only_active']
        self.max_rows = options.get('max_rows')
        self.force = options.get('force', False)
        self.mark_processed = options.get('mark_processed', False)

        if self.dry_run:
            self.stdout.write(self.style.WARNING("\nüîç –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î\n"))

        if self.only_active:
            self.stdout.write(self.style.WARNING("üìå –†–µ–∂–∏–º: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (actual = True)"))

        if self.force:
            self.stdout.write(self.style.WARNING("‚ö†Ô∏è  –†–µ–∂–∏–º: –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏)"))

        catalogues = self.get_catalogues(options.get('catalogue_id'), options.get('ip_type'))

        if not catalogues:
            raise CommandError('–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–∞—Ç–∞–ª–æ–≥–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')

        total_stats = {
            'catalogues': len(catalogues),
            'processed': 0,
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        for catalogue in catalogues:
            self.stdout.write(self.style.SUCCESS(f"\n{'='*60}"))
            self.stdout.write(self.style.SUCCESS(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞: {catalogue.name}"))
            self.stdout.write(self.style.SUCCESS(f"   ID: {catalogue.id}, –¢–∏–ø: {catalogue.ip_type.name if catalogue.ip_type else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'}"))
            self.stdout.write(self.style.SUCCESS(f"{'='*60}"))

            stats = self.process_catalogue(catalogue)

            for key in ['processed', 'created', 'updated', 'skipped', 'errors']:
                total_stats[key] += stats.get(key, 0)
            total_stats['skipped_by_date'] += stats.get('skipped_by_date', 0)

        self.print_final_stats(total_stats)

    def get_catalogues(self, catalogue_id=None, ip_type_slug=None):
        queryset = FipsOpenDataCatalogue.objects.all()

        if catalogue_id:
            queryset = queryset.filter(id=catalogue_id)
        elif ip_type_slug:
            queryset = queryset.filter(ip_type__slug=ip_type_slug)
        else:
            queryset = queryset.exclude(catalogue_file='')

        return queryset.order_by('ip_type__id', '-publication_date')

    def process_catalogue(self, catalogue):
        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        if not catalogue.catalogue_file:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –£ –∫–∞—Ç–∞–ª–æ–≥–∞ ID={catalogue.id} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª"))
            stats['errors'] += 1
            return stats

        if not self.force and hasattr(catalogue, 'parsed_date') and catalogue.parsed_date:
            self.stdout.write(self.style.WARNING(
                f"  ‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ —É–∂–µ –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω {catalogue.parsed_date.strftime('%d.%m.%Y %H:%M')}"
            ))
            self.stdout.write(self.style.WARNING(f"     –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --force –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"))
            stats['skipped'] += 1
            return stats

        ip_type_slug = catalogue.ip_type.slug if catalogue.ip_type else None

        if ip_type_slug not in self.parsers:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –ù–µ—Ç –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è —Ç–∏–ø–∞ –†–ò–î: {ip_type_slug}"))
            stats['errors'] += 1
            return stats

        parser = self.parsers[ip_type_slug]
        df = self.load_csv(catalogue)

        if df is None or df.empty:
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å"))
            stats['skipped'] += 1
            return stats

        self.stdout.write(f"  üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")

        missing_columns = self.check_required_columns(df, parser.get_required_columns())
        if missing_columns:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}"))
            stats['errors'] += 1
            return stats

        if not self.skip_filters:
            df = apply_filters(df, self.min_year, self.only_active, self.stdout)

        if df.empty:
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"))
            stats['skipped'] += 1
            return stats

        self.stdout.write(f"  üìä –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df)} –∑–∞–ø–∏—Å–µ–π")

        if self.max_rows and len(df) > self.max_rows:
            df = df.head(self.max_rows)
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {self.max_rows} –∑–∞–ø–∏—Å–µ–π"))

        try:
            parser_stats = parser.parse_dataframe(df, catalogue)
            stats.update(parser_stats)

            if not self.dry_run and hasattr(catalogue, 'parsed_date'):
                if stats['errors'] == 0 or self.mark_processed:
                    catalogue.parsed_date = timezone.now()
                    catalogue.save(update_fields=['parsed_date'])
                    self.stdout.write(self.style.SUCCESS(f"  ‚úÖ –ö–∞—Ç–∞–ª–æ–≥ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π"))
                else:
                    self.stdout.write(self.style.WARNING(
                        f"  ‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ –Ω–µ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫"
                    ))

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}"))
            logger.error(f"Error parsing catalogue {catalogue.id}: {e}", exc_info=True)
            stats['errors'] += 1

        return stats

    def load_csv(self, catalogue):
        file_path = catalogue.catalogue_file.path

        if not os.path.exists(file_path):
            self.stdout.write(self.style.ERROR(f"  ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"))
            return None

        df = load_csv_with_strategies(file_path, self.encoding, self.delimiter, self.stdout)
        return df

    def check_required_columns(self, df, required_columns):
        missing = [col for col in required_columns if col not in df.columns]
        return missing

    def print_final_stats(self, stats):
        self.stdout.write(self.style.SUCCESS(f"\n{'='*60}"))
        self.stdout.write(self.style.SUCCESS("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê"))
        self.stdout.write(self.style.SUCCESS(f"{'='*60}"))
        self.stdout.write(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞—Ç–∞–ª–æ–≥–æ–≤: {stats['catalogues']}")
        self.stdout.write(f"üìù –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}")
        self.stdout.write(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ: {stats['created']}")
        self.stdout.write(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}")
        self.stdout.write(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ –≤—Å–µ–≥–æ: {stats['skipped']}")
        self.stdout.write(f"   ‚îî‚îÄ –ø–æ –¥–∞—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {stats.get('skipped_by_date', 0)}")

        if stats['errors'] > 0:
            self.stdout.write(self.style.ERROR(f"‚ùå –û—à–∏–±–æ–∫: {stats['errors']}"))
        else:
            self.stdout.write(self.style.SUCCESS(f"‚úÖ –û—à–∏–±–æ–∫: {stats['errors']}"))

        if self.dry_run:
            self.stdout.write(self.style.WARNING("\nüîç –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î"))

        self.stdout.write(self.style.SUCCESS(f"{'='*60}"))

```


-----

# –§–∞–π–ª: management\commands\pars_fips_catalogue_archive.py

```
"""
–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ —Ç–∏–ø—ã –†–ò–î: –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è, –ø–æ–ª–µ–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏, –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã,
—Ç–æ–ø–æ–ª–æ–≥–∏–∏ –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º, –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –≠–í–ú –∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.
"""

import logging
import re
from datetime import datetime
from typing import Optional, Tuple, List, Dict, Any, Set
from collections import defaultdict

from django.db import models
from django.core.management.base import BaseCommand, CommandError
from django.utils.text import slugify
from django.utils import timezone
from tqdm import tqdm
import pandas as pd
import os

# –ò–º–ø–æ—Ä—Ç—ã natasha
from natasha import (
    Segmenter,
    MorphVocab,
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    NewsNERTagger,
    Doc,
    NamesExtractor
)

from intellectual_property.models import (
    FipsOpenDataCatalogue, IPType, ProtectionDocumentType,
    IPObject, AdditionalPatent, IPImage
)
from core.models import (
    City, Region, District, Person, Organization,
    FOIV, Country, RFRepresentative,
    OrganizationNormalizationRule, ActivityType, CeoPosition
)
from common.utils.text import TextUtils
from common.utils.dates import DateUtils

logger = logging.getLogger(__name__)


class RussianTextProcessor:
    """
    –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º natasha
    """

    # –°–ø–∏—Å–æ–∫ —Ä–∏–º—Å–∫–∏—Ö —Ü–∏—Ñ—Ä
    ROMAN_NUMERALS = {
        'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X',
        'XI', 'XII', 'XIII', 'XIV', 'XV', 'XVI', 'XVII', 'XVIII', 'XIX', 'XX',
        'XXI', 'XXII', 'XXIII', 'XXIV', 'XXV', 'XXX', 'XL', 'L', 'LX', 'XC',
        'C', 'CD', 'D', 'DC', 'CM', 'M'
    }

    # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
    ORG_ABBR = {
        '–û–û–û', '–ó–ê–û', '–û–ê–û', '–ê–û', '–ü–ê–û', '–ù–ê–û',
        '–§–ì–£–ü', '–§–ì–ë–£', '–§–ì–ê–û–£', '–§–ì–ê–£', '–§–ì–ö–£',
        '–ù–ò–ò', '–ö–ë', '–û–ö–ë', '–°–ö–ë', '–¶–ö–ë', '–ü–ö–ë',
        '–ù–ü–û', '–ù–ü–ü', '–ù–ü–§', '–ù–ü–¶', '–ù–ò–¶',
        '–ú–£–ü', '–ì–£–ü', '–ò–ß–ü', '–¢–û–û', '–ê–û–ó–¢', '–ê–û–û–¢',
        '–†–§', '–†–ê–ù', '–°–û –†–ê–ù', '–£—Ä–û –†–ê–ù', '–î–í–û –†–ê–ù',
        '–ú–ì–£', '–°–ü–±–ì–£', '–ú–§–¢–ò', '–ú–ò–§–ò', '–ú–ì–¢–£', '–ú–ê–ò',
        '–õ–¢–î', '–ò–ù–ö', '–ö–û', '–ì–ú–ë–•', '–ê–ì', '–°–ê', '–ù–í', '–ë–í', '–°–ï',
        '–ö–æ', 'Ltd', 'Inc', 'GmbH', 'AG', 'SA', 'NV', 'BV', 'SE',
    }

    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ natasha
        self.segmenter = Segmenter()
        self.morph_vocab = MorphVocab()
        self.emb = NewsEmbedding()
        self.morph_tagger = NewsMorphTagger(self.emb)
        self.syntax_parser = NewsSyntaxParser(self.emb)
        self.ner_tagger = NewsNERTagger(self.emb)
        self.names_extractor = NamesExtractor(self.morph_vocab)

        # –ö—ç—à–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.doc_cache = {}
        self.morph_cache = {}

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã –≤ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        self.ORG_ABBR.update(self.ROMAN_NUMERALS)

    def get_doc(self, text: str) -> Optional[Doc]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not text:
            return None

        if text in self.doc_cache:
            return self.doc_cache[text]

        doc = Doc(text)
        doc.segment(self.segmenter)
        doc.tag_morph(self.morph_tagger)
        doc.parse_syntax(self.syntax_parser)
        doc.tag_ner(self.ner_tagger)

        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
        for token in doc.tokens:
            token.lemmatize(self.morph_vocab)

        for span in doc.spans:
            span.normalize(self.morph_vocab)

        self.doc_cache[text] = doc
        return doc

    def is_roman_numeral(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∏–º—Å–∫—É—é —Ü–∏—Ñ—Ä—É"""
        if not text:
            return False
        clean_text = text.strip('.,;:!?()').upper()
        return clean_text in self.ROMAN_NUMERALS

    def is_abbr(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        if not text:
            return False
        clean_text = text.strip('.,;:!?()').upper()
        return clean_text in self.ORG_ABBR

    def is_person(self, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –§–ò–û —á–µ–ª–æ–≤–µ–∫–∞"""
        if not text or len(text) < 6:
            return False

        # –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        if any(ind in text for ind in self.ORG_ABBR if len(ind) > 2):
            return False

        org_indicators = ['–û–±—â–µ—Å—Ç–≤–æ', '–ö–æ–º–ø–∞–Ω–∏—è', '–ö–æ—Ä–ø–æ—Ä–∞—Ü–∏—è', '–ó–∞–≤–æ–¥',
                         '–ò–Ω—Å—Ç–∏—Ç—É—Ç', '–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '–ê–∫–∞–¥–µ–º–∏—è', '–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è',
                         '–§–∏—Ä–º–∞', '–¶–µ–Ω—Ç—Ä']

        if any(ind.lower() in text.lower() for ind in org_indicators):
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ NER
        doc = self.get_doc(text)
        if doc and doc.spans:
            for span in doc.spans:
                if span.type == 'PER':
                    return True

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –§–ò–û
        words = text.split()
        if 2 <= len(words) <= 4:
            name_like = 0
            for word in words:
                clean = word.rstrip('.,')
                if clean and clean[0].isupper() and len(clean) > 1:
                    name_like += 1
            return name_like >= len(words) - 1

        return False

    def extract_person_parts(self, text: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π –§–ò–û —Å –ø–æ–º–æ—â—å—é natasha"""
        matches = list(self.names_extractor(text))
        if matches:
            fact = matches[0].fact
            parts = []
            if fact.last:
                parts.append(fact.last)
            if fact.first:
                parts.append(fact.first)
            if fact.middle:
                parts.append(fact.middle)

            return {
                'last': fact.last or '',
                'first': fact.first or '',
                'middle': fact.middle or '',
                'full': ' '.join(parts)
            }

        # Fallback: —Ä—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥
        return self._parse_name_manually(text)

    def _parse_name_manually(self, text: str) -> Dict[str, str]:
        """–†—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –∏–º–µ–Ω–∏"""
        words = text.split()

        if len(words) == 3:
            return {
                'last': words[0],
                'first': words[1],
                'middle': words[2],
                'full': text
            }
        elif len(words) == 2:
            return {
                'last': words[0],
                'first': words[1],
                'middle': '',
                'full': text
            }
        else:
            return {
                'last': text,
                'first': '',
                'middle': '',
                'full': text
            }

    def format_person_name(self, name: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –§–ò–û —á–µ–ª–æ–≤–µ–∫–∞"""
        if not name:
            return name

        parts = self.extract_person_parts(name)
        if parts.get('full'):
            return parts['full']

        return name


class OrganizationNormalizer:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)"""

    def __init__(self):
        self.rules_cache = None
        self.processor = RussianTextProcessor()
        self.load_rules()

    def load_rules(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∏–∑ –ë–î"""
        try:
            rules = OrganizationNormalizationRule.objects.all().order_by('priority')
            self.rules_cache = [
                {
                    'original': rule.original_text.lower(),
                    'replacement': rule.replacement_text.lower(),
                    'type': rule.rule_type,
                    'priority': rule.priority
                }
                for rule in rules
            ]
        except Exception as e:
            self.rules_cache = []
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")

    def normalize_for_search(self, name: str) -> Dict[str, Any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –¢–û–õ–¨–ö–û –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        –°–∞–º–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –≤ CSV
        """
        if pd.isna(name) or not name:
            return {'normalized': '', 'keywords': [], 'original': name}

        original = str(name).strip()
        name_lower = original.lower()

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ –ë–î –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        normalized = name_lower
        if self.rules_cache:
            for rule in self.rules_cache:
                try:
                    if rule['type'] == 'ignore':
                        pattern = r'\b' + re.escape(rule['original']) + r'\b'
                        normalized = re.sub(pattern, '', normalized)
                    else:
                        pattern = r'\b' + re.escape(rule['original']) + r'\b'
                        normalized = re.sub(pattern, rule['replacement'], normalized)
                except Exception:
                    continue

        # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞
        normalized = re.sub(r'["\'¬´¬ª‚Äû‚Äú‚Äù]', '', normalized)
        normalized = re.sub(r'[^\w\s-]', ' ', normalized)
        normalized = ' '.join(normalized.split())

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        keywords = []

        # –°–ª–æ–≤–∞ –≤ –∫–∞–≤—ã—á–∫–∞—Ö
        quoted = re.findall(r'"([^"]+)"', original)
        for q in quoted:
            words = q.lower().split()
            keywords.extend([w for w in words if len(w) > 3])

        # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        abbrs = re.findall(r'\b[–ê-–Ø–ÅA-Z]{2,}\b', original)
        keywords.extend([a.lower() for a in abbrs if len(a) >= 2])

        # –ö–æ–¥—ã (–ò–ù–ù, –û–ì–†–ù –∏ —Ç.–¥.)
        codes = re.findall(r'\b\d{10,}\b', original)
        keywords.extend(codes)

        return {
            'normalized': normalized,
            'keywords': list(set(keywords)),
            'original': original,
        }

    def format_organization_name(self, name: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        return name


class PersonNameFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω –ª—é–¥–µ–π"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def format(self, name: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –§–ò–û"""
        return self.processor.format_person_name(name)


class RIDNameFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –†–ò–î"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def format(self, text: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –†–ò–î"""
        if not text or not isinstance(text, str):
            return text

        if len(text.strip()) <= 1:
            return text

        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ –¥–µ–ª–∞–µ–º –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –∑–∞–≥–ª–∞–≤–Ω–æ–π
        words = text.lower().split()
        if words:
            words[0] = words[0][0].upper() + words[0][1:]
        return ' '.join(words)


class EntityTypeDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def detect_type(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏"""
        if self.processor.is_person(text):
            return 'person'
        return 'organization'


class BaseFIPSParser:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –§–ò–ü–°"""

    def __init__(self, command):
        self.command = command
        self.stdout = command.stdout
        self.style = command.style

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
        self.processor = RussianTextProcessor()
        self.org_normalizer = OrganizationNormalizer()
        self.type_detector = EntityTypeDetector()
        self.person_formatter = PersonNameFormatter()
        self.rid_formatter = RIDNameFormatter()

        # –ö—ç—à–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.country_cache = {}
        self.person_cache = {}
        self.organization_cache = {}
        self.foiv_cache = {}
        self.rf_rep_cache = {}
        self.city_cache = {}
        self.activity_type_cache = {}
        self.ceo_position_cache = {}

    def get_ip_type(self):
        """–î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –¥–æ—á–µ—Ä–Ω–∏—Ö –∫–ª–∞—Å—Å–∞—Ö"""
        raise NotImplementedError

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
        raise NotImplementedError

    def parse_dataframe(self, df, catalogue):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame"""
        raise NotImplementedError

    def clean_string(self, value):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if pd.isna(value) or value is None:
            return ''
        value = str(value).strip()
        if value in ['', 'None', 'null', 'NULL', 'nan']:
            return ''
        return value

    def parse_date(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        if pd.isna(value) or not value:
            return None

        date_str = str(value).strip()
        if not date_str:
            return None

        for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
            try:
                return datetime.strptime(date_str, fmt).date()
            except (ValueError, TypeError):
                continue

        try:
            return pd.to_datetime(date_str).date()
        except (ValueError, TypeError):
            return None

    def parse_bool(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –±—É–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if pd.isna(value) or not value:
            return False
        value = str(value).lower().strip()
        return value in ['1', 'true', 'yes', '–¥–∞', '–¥–µ–π—Å—Ç–≤—É–µ—Ç', 't', '1.0', '–∞–∫—Ç–∏–≤–µ–Ω']

    def get_or_create_country(self, code):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –ø–æ –∫–æ–¥—É"""
        if not code or pd.isna(code):
            return None

        code = str(code).upper().strip()
        if len(code) != 2:
            return None

        if code in self.country_cache:
            return self.country_cache[code]

        try:
            country = Country.objects.filter(code=code).first()
            if country:
                self.country_cache[code] = country
                return country

            country = Country.objects.filter(code_alpha3=code).first()
            if country:
                self.country_cache[code] = country
                return country

            return None

        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω—ã {code}: {e}"))
            return None

    def parse_authors(self, authors_str):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –∞–≤—Ç–æ—Ä–∞–º–∏"""
        if pd.isna(authors_str) or not authors_str:
            return []

        authors_str = str(authors_str)
        authors_list = re.split(r'[\n,]\s*', authors_str)

        result = []
        for author in authors_list:
            author = author.strip()
            if not author or author == '""' or author == 'null':
                continue

            author = author.strip('"')
            author = re.sub(r'\s*\([A-Z]{2}\)', '', author)
            author = self.person_formatter.format(author)

            parts = author.split()

            if len(parts) >= 2:
                last_name = parts[0]
                first_name = parts[1] if len(parts) > 1 else ''
                middle_name = parts[2] if len(parts) > 2 else ''

                first_name_clean = first_name.replace('.', '')
                middle_name_clean = middle_name.replace('.', '')

                result.append({
                    'last_name': last_name,
                    'first_name': first_name_clean,
                    'middle_name': middle_name_clean,
                    'full_name': author,
                })
            else:
                result.append({
                    'last_name': author,
                    'first_name': '',
                    'middle_name': '',
                    'full_name': author,
                })

        return result

    def parse_patent_holders(self, holders_str):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—è–º–∏"""
        if pd.isna(holders_str) or not holders_str:
            return []

        holders_str = str(holders_str)
        holders_list = re.split(r'[\n]\s*', holders_str)

        result = []
        for holder in holders_list:
            holder = holder.strip().strip('"')
            if not holder or holder == 'null' or holder == 'None':
                continue

            holder = re.sub(r'\s*\([A-Z]{2}\)', '', holder)
            result.append(holder)

        return result

    def find_or_create_person(self, person_data):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞"""
        cache_key = f"{person_data['last_name']}|{person_data['first_name']}|{person_data['middle_name']}"

        if cache_key in self.person_cache:
            return self.person_cache[cache_key]

        persons = Person.objects.filter(
            last_name=person_data['last_name'],
            first_name=person_data['first_name']
        )

        if person_data['middle_name']:
            persons = persons.filter(middle_name=person_data['middle_name'])

        if persons.exists():
            person = persons.first()
            self.person_cache[cache_key] = person
            return person

        try:
            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
            new_id = max_id + 1

            if 'full_name' in person_data:
                full_name = person_data['full_name']
            else:
                full_name_parts = [person_data['last_name'], person_data['first_name']]
                if person_data['middle_name']:
                    full_name_parts.append(person_data['middle_name'])
                full_name = ' '.join(full_name_parts)
                full_name = self.person_formatter.format(full_name)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π slug
            base_slug = slugify(f"{person_data['last_name']} {person_data['first_name']} {person_data['middle_name']}".strip())
            if not base_slug:
                base_slug = 'person'

            unique_slug = base_slug
            counter = 1
            while Person.objects.filter(slug=unique_slug).exists():
                unique_slug = f"{base_slug}-{counter}"
                counter += 1

            person = Person.objects.create(
                ceo_id=new_id,
                ceo=full_name,
                last_name=person_data['last_name'],
                first_name=person_data['first_name'],
                middle_name=person_data['middle_name'],
                slug=unique_slug
            )
            self.person_cache[cache_key] = person
            return person
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Person: {e}"))
            return None

    def find_or_create_person_from_name(self, full_name):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞ –ø–æ –ø–æ–ª–Ω–æ–º—É –∏–º–µ–Ω–∏"""
        if pd.isna(full_name) or not full_name:
            return None

        full_name = str(full_name).strip().strip('"')
        full_name = self.person_formatter.format(full_name)

        if full_name in self.person_cache:
            return self.person_cache[full_name]

        parts = full_name.split()

        if len(parts) >= 2:
            last_name = parts[0]
            first_name = parts[1] if len(parts) > 1 else ''
            middle_name = parts[2] if len(parts) > 2 else ''

            first_name_clean = first_name.replace('.', '')
            middle_name_clean = middle_name.replace('.', '')

            person_data = {
                'last_name': last_name,
                'first_name': first_name_clean,
                'middle_name': middle_name_clean,
                'full_name': full_name,
            }
        else:
            person_data = {
                'last_name': full_name,
                'first_name': '',
                'middle_name': '',
                'full_name': full_name,
            }

        return self.find_or_create_person(person_data)

    def find_similar_organization(self, org_name):
        """–£—Å–∏–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        if pd.isna(org_name) or not org_name:
            return None

        org_name = str(org_name).strip().strip('"')

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        direct_match = Organization.objects.filter(
            models.Q(name=org_name) |
            models.Q(full_name=org_name) |
            models.Q(short_name=org_name)
        ).first()
        if direct_match:
            return direct_match

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
        norm_data = self.org_normalizer.normalize_for_search(org_name)
        normalized = norm_data['normalized']
        keywords = norm_data['keywords']

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in keywords:
            if len(keyword) >= 3:
                similar = Organization.objects.filter(
                    models.Q(name__icontains=keyword) |
                    models.Q(full_name__icontains=keyword) |
                    models.Q(short_name__icontains=keyword)
                ).first()
                if similar:
                    return similar

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ –ø–æ –ø–µ—Ä–≤—ã–º 30 —Å–∏–º–≤–æ–ª–∞–º
        if len(normalized) > 30:
            prefix = normalized[:30]
            similar = Organization.objects.filter(
                models.Q(name__icontains=prefix) |
                models.Q(full_name__icontains=prefix) |
                models.Q(short_name__icontains=prefix)
            ).first()
            if similar:
                return similar

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ü–æ–∏—Å–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–ª–æ–≤–∞–º
        words = org_name.split()
        for word in words:
            if len(word) > 4:
                similar = Organization.objects.filter(
                    models.Q(name__icontains=word) |
                    models.Q(full_name__icontains=word)
                ).first()
                if similar:
                    return similar

        return None

    def find_or_create_organization(self, org_name):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        if pd.isna(org_name) or not org_name:
            return None

        org_name = str(org_name).strip().strip('"')

        if not org_name or org_name == 'null' or org_name == 'None':
            return None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if org_name in self.organization_cache:
            return self.organization_cache[org_name]

        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ
        similar = self.find_similar_organization(org_name)
        if similar:
            self.organization_cache[org_name] = similar
            return similar

        # –ù–µ –Ω–∞—à–ª–∏ - —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º
        try:
            max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
            new_id = max_id + 1

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º slug –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
            base_slug = slugify(org_name[:50])
            if not base_slug:
                base_slug = 'organization'

            unique_slug = base_slug
            counter = 1
            while Organization.objects.filter(slug=unique_slug).exists():
                unique_slug = f"{base_slug}-{counter}"
                counter += 1

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            org = Organization.objects.create(
                organization_id=new_id,
                name=org_name,
                full_name=org_name,
                short_name=org_name[:500] if len(org_name) > 500 else org_name,
                slug=unique_slug,
                register_opk=False,
                strategic=False,
            )

            self.organization_cache[org_name] = org
            return org
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Organization: {e}"))
            return None


class InventionParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='invention').first()

    def get_required_columns(self):
        return ['registration number', 'invention name']

    def _has_data_changed(self, obj, new_data):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ"""
        fields_to_check = [
            ('name', obj.name, new_data['name']),
            ('application_date', obj.application_date, new_data['application_date']),
            ('registration_date', obj.registration_date, new_data['registration_date']),
            ('patent_starting_date', obj.patent_starting_date, new_data['patent_starting_date']),
            ('expiration_date', obj.expiration_date, new_data['expiration_date']),
            ('actual', obj.actual, new_data['actual']),
            ('publication_url', obj.publication_url, new_data['publication_url']),
            ('abstract', obj.abstract, new_data['abstract']),
            ('claims', obj.claims, new_data['claims']),
            ('creation_year', obj.creation_year, new_data['creation_year']),
        ]

        for field_name, old_val, new_val in fields_to_check:
            if old_val != new_val:
                return True
        return False

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π..."))

        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'invention' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats

        # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Ç–∞–ª–æ–≥–∞
        upload_date = catalogue.upload_date.date() if catalogue.upload_date else None

        # –®–ê–ì 1: –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞
        self.stdout.write("  üì• –ß—Ç–µ–Ω–∏–µ CSV...")
        all_reg_numbers = []
        reg_num_to_row = {}

        with tqdm(total=len(df), desc="     –ü—Ä–æ–≥—Ä–µ—Å—Å", unit=" –∑–∞–ø", leave=False) as pbar:
            for idx, row in df.iterrows():
                reg_num = self.clean_string(row.get('registration number'))
                if reg_num:
                    all_reg_numbers.append(reg_num)
                    reg_num_to_row[reg_num] = row
                pbar.update(1)

        self.stdout.write(f"  üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: {len(all_reg_numbers)}")

        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –ü–ê–ß–ö–ê–ú–ò
        self.stdout.write("  üîç –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î...")
        existing_objects = {}
        batch_size = 500

        with tqdm(total=len(all_reg_numbers), desc="     –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—á–∫–∞–º–∏", unit=" –∑–∞–ø") as pbar:
            for i in range(0, len(all_reg_numbers), batch_size):
                batch_numbers = all_reg_numbers[i:i+batch_size]

                for obj in IPObject.objects.filter(
                    registration_number__in=batch_numbers,
                    ip_type=ip_type
                ).select_related('ip_type'):
                    existing_objects[obj.registration_number] = obj

                pbar.update(len(batch_numbers))

                if (i // batch_size) % 10 == 0:
                    pbar.set_postfix({"–Ω–∞–π–¥–µ–Ω–æ": len(existing_objects)})

        self.stdout.write(f"  üìä –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(existing_objects)}")

        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.stdout.write("  üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        to_create = []
        to_update = []
        skipped_by_date = []
        unchanged_count = 0
        error_reg_numbers = []

        authors_cache = defaultdict(list)
        holders_cache = defaultdict(list)

        with tqdm(total=len(reg_num_to_row), desc="     –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–µ–π", unit=" –∑–∞–ø") as pbar:
            for reg_num, row in reg_num_to_row.items():
                try:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –¥–∞—Ç–µ
                    if not self.command.force and upload_date and reg_num in existing_objects:
                        existing = existing_objects[reg_num]
                        if existing.updated_at and existing.updated_at.date() >= upload_date:
                            skipped_by_date.append(reg_num)
                            pbar.update(1)
                            continue

                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                    name = self.clean_string(row.get('invention name'))
                    if name:
                        name = self.rid_formatter.format(name)
                    else:
                        name = f"–ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ ‚Ññ{reg_num}"

                    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã
                    application_date = self.parse_date(row.get('application date'))
                    registration_date = self.parse_date(row.get('registration date'))
                    patent_starting_date = self.parse_date(row.get('patent starting date'))
                    expiration_date = self.parse_date(row.get('expiration date'))
                    actual = self.parse_bool(row.get('actual'))
                    publication_url = self.clean_string(row.get('publication URL'))
                    abstract = self.clean_string(row.get('abstract'))
                    claims = self.clean_string(row.get('claims'))

                    creation_year = None
                    if application_date:
                        creation_year = application_date.year
                    elif registration_date:
                        creation_year = registration_date.year

                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                    obj_data = {
                        'registration_number': reg_num,
                        'ip_type': ip_type,
                        'name': name,
                        'application_date': application_date,
                        'registration_date': registration_date,
                        'patent_starting_date': patent_starting_date,
                        'expiration_date': expiration_date,
                        'actual': actual,
                        'publication_url': publication_url,
                        'abstract': abstract,
                        'claims': claims,
                        'creation_year': creation_year,
                    }

                    if reg_num in existing_objects:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
                        existing = existing_objects[reg_num]
                        if self._has_data_changed(existing, obj_data):
                            to_update.append(obj_data)
                        else:
                            unchanged_count += 1
                    else:
                        to_create.append(obj_data)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–≤—Ç–æ—Ä–æ–≤ –∏ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
                    authors_str = row.get('authors')
                    if not pd.isna(authors_str) and authors_str:
                        authors_cache[reg_num] = self.parse_authors(authors_str)

                    holders_str = row.get('patent holders')
                    if not pd.isna(holders_str) and holders_str:
                        holders_cache[reg_num] = self.parse_patent_holders(holders_str)

                except Exception as e:
                    error_reg_numbers.append(reg_num)
                    self.stdout.write(self.style.ERROR(f"\n  ‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    logger.error(f"Error preparing invention {reg_num}: {e}", exc_info=True)

                pbar.update(1)
                if pbar.n % 1000 == 0:
                    pbar.set_postfix({
                        "–Ω–æ–≤—ã–µ": len(to_create),
                        "–æ–±–Ω–æ–≤": len(to_update),
                        "–±–µ–∑ –∏–∑–º": unchanged_count,
                        "–ø—Ä–æ–ø—É—â": len(skipped_by_date)
                    })

        stats['skipped_by_date'] = len(skipped_by_date)
        stats['skipped'] += len(skipped_by_date)
        stats['errors'] = len(error_reg_numbers)
        stats['unchanged'] = unchanged_count

        self.stdout.write(f"     –ò—Ç–æ–≥–æ: –Ω–æ–≤—ã—Ö={len(to_create)}, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ={len(to_update)}, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={unchanged_count}")

        # –®–ê–ì 4: –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
        if to_create and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –°–æ–∑–¥–∞–Ω–∏–µ {len(to_create)} –∑–∞–ø–∏—Å–µ–π...")
            create_objects = [IPObject(**data) for data in to_create]

            batch_size = 1000
            created_count = 0

            with tqdm(total=len(create_objects), desc="     –°–æ–∑–¥–∞–Ω–∏–µ", unit=" –∑–∞–ø") as pbar:
                for i in range(0, len(create_objects), batch_size):
                    batch = create_objects[i:i+batch_size]
                    IPObject.objects.bulk_create(batch, batch_size=batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))
                    pbar.set_postfix({"—Å–æ–∑–¥–∞–Ω–æ": created_count})

            stats['created'] = created_count

            # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –Ω–æ–≤—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏
            self.stdout.write("     –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞...")

            with tqdm(total=len(to_create), desc="     –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞", unit=" –∑–∞–ø") as pbar:
                for i in range(0, len(to_create), batch_size):
                    batch_data = to_create[i:i+batch_size]
                    batch_nums = [d['registration_number'] for d in batch_data]

                    for obj in IPObject.objects.filter(
                        registration_number__in=batch_nums,
                        ip_type=ip_type
                    ):
                        existing_objects[obj.registration_number] = obj

                    pbar.update(len(batch_data))

        # –®–ê–ì 5: –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π
        if to_update and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {len(to_update)} –∑–∞–ø–∏—Å–µ–π...")
            updated_count = 0

            with tqdm(total=len(to_update), desc="     –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ", unit=" –∑–∞–ø") as pbar:
                for data in to_update:
                    obj = existing_objects[data['registration_number']]
                    update_fields = []

                    if obj.name != data['name']:
                        obj.name = data['name']
                        update_fields.append('name')

                    if obj.application_date != data['application_date']:
                        obj.application_date = data['application_date']
                        update_fields.append('application_date')

                    if obj.registration_date != data['registration_date']:
                        obj.registration_date = data['registration_date']
                        update_fields.append('registration_date')

                    if obj.patent_starting_date != data['patent_starting_date']:
                        obj.patent_starting_date = data['patent_starting_date']
                        update_fields.append('patent_starting_date')

                    if obj.expiration_date != data['expiration_date']:
                        obj.expiration_date = data['expiration_date']
                        update_fields.append('expiration_date')

                    if obj.actual != data['actual']:
                        obj.actual = data['actual']
                        update_fields.append('actual')

                    if obj.publication_url != data['publication_url']:
                        obj.publication_url = data['publication_url']
                        update_fields.append('publication_url')

                    if obj.abstract != data['abstract']:
                        obj.abstract = data['abstract']
                        update_fields.append('abstract')

                    if obj.claims != data['claims']:
                        obj.claims = data['claims']
                        update_fields.append('claims')

                    if obj.creation_year != data['creation_year']:
                        obj.creation_year = data['creation_year']
                        update_fields.append('creation_year')

                    if update_fields:
                        obj.save(update_fields=update_fields)
                        updated_count += 1

                    pbar.update(1)
                    if pbar.n % 100 == 0:
                        pbar.set_postfix({"–æ–±–Ω–æ–≤–ª–µ–Ω–æ": updated_count})

            stats['updated'] = updated_count
            self.stdout.write(f"     –†–µ–∞–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ: {updated_count} –∏–∑ {len(to_update)}")

        # –®–ê–ì 6: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤
        if authors_cache and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ ({len(authors_cache)} –∑–∞–ø–∏—Å–µ–π)...")
            self._process_authors_batch_with_progress(existing_objects, authors_cache)

        # –®–ê–ì 7: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
        if holders_cache and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π ({len(holders_cache)} –∑–∞–ø–∏—Å–µ–π)...")
            self._process_holders_batch_with_progress(existing_objects, holders_cache)

        stats['processed'] = len(df) - stats['skipped'] - stats['errors']

        self.stdout.write(self.style.SUCCESS(f"  ‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"     –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats['unchanged']}, "
                         f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –≤—Å–µ–≥–æ: {stats['skipped']} (–∏–∑ –Ω–∏—Ö –ø–æ –¥–∞—Ç–µ: {stats['skipped_by_date']}), "
                         f"–û—à–∏–±–æ–∫: {stats['errors']}")

        return stats

    def _process_authors_batch_with_progress(self, existing_objects, authors_cache):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –∏ —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ø–∞—á–∫–∏"""
        self.stdout.write(f"     ‚ö° –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤...")

        # –®–ê–ì 1: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
        self.stdout.write("        –®–∞–≥ 1/6: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤...")
        author_to_key = {}
        total_relations = 0

        for reg_num, authors_data in authors_cache.items():
            ip_object = existing_objects.get(reg_num)
            if not ip_object:
                continue

            for author_data in authors_data:
                key = f"{author_data['last_name']}|{author_data['first_name']}|{author_data['middle_name']}"
                if key not in author_to_key:
                    author_to_key[key] = {
                        'data': author_data,
                        'ip_objects': []
                    }
                author_to_key[key]['ip_objects'].append(ip_object)
                total_relations += 1

        all_keys = list(author_to_key.keys())
        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤: {len(all_keys)}, –≤—Å–µ–≥–æ —Å–≤—è–∑–µ–π: {total_relations}")

        # –®–ê–ì 2: –ü–æ–∏—Å–∫ –≤ –ë–î
        self.stdout.write("        –®–∞–≥ 2/6: –ü–æ–∏—Å–∫ –≤ –ë–î...")
        existing_people = {}
        batch_size = 50

        with tqdm(total=len(all_keys), desc="           –ü–æ–∏—Å–∫", unit=" –∫–ª—é—á") as pbar:
            for i in range(0, len(all_keys), batch_size):
                batch_keys = all_keys[i:i+batch_size]

                name_conditions = models.Q()
                for key in batch_keys:
                    last, first, middle = key.split('|')
                    if middle:
                        name_conditions |= models.Q(
                            last_name=last,
                            first_name=first,
                            middle_name=middle
                        )
                    else:
                        name_conditions |= models.Q(
                            last_name=last,
                            first_name=first,
                            middle_name__isnull=True
                        ) | models.Q(
                            last_name=last,
                            first_name=first,
                            middle_name=''
                        )

                for person in Person.objects.filter(name_conditions):
                    key = f"{person.last_name}|{person.first_name}|{person.middle_name or ''}"
                    existing_people[key] = person
                    self.person_cache[key] = person

                pbar.update(len(batch_keys))
                if (i // batch_size) % 10 == 0:
                    pbar.set_postfix({"–Ω–∞–π–¥–µ–Ω–æ": len(existing_people)})

        self.stdout.write(f"        –ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {len(existing_people)}")

        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
        self.stdout.write("        –®–∞–≥ 3/6: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤...")
        people_to_create = []
        key_to_new_person = {}

        max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
        next_id = max_id + 1
        existing_slugs = set(Person.objects.values_list('slug', flat=True))

        with tqdm(total=len(all_keys), desc="           –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞", unit=" –∫–ª—é—á") as pbar:
            for key, info in author_to_key.items():
                if key not in existing_people:
                    author_data = info['data']

                    name_parts = [author_data['last_name'], author_data['first_name']]
                    if author_data['middle_name']:
                        name_parts.append(author_data['middle_name'])

                    base_slug = slugify(' '.join(name_parts).strip())
                    if not base_slug:
                        base_slug = 'person'

                    unique_slug = base_slug
                    counter = 1
                    while unique_slug in existing_slugs or any(p.slug == unique_slug for p in people_to_create):
                        unique_slug = f"{base_slug}-{counter}"
                        counter += 1

                    person = Person(
                        ceo_id=next_id,
                        ceo=author_data['full_name'],
                        last_name=author_data['last_name'],
                        first_name=author_data['first_name'],
                        middle_name=author_data['middle_name'] or '',
                        slug=unique_slug
                    )
                    people_to_create.append(person)
                    key_to_new_person[key] = person
                    next_id += 1
                    existing_slugs.add(unique_slug)

                pbar.update(1)
                if pbar.n % 10000 == 0:
                    pbar.set_postfix({"–∫ —Å–æ–∑–¥–∞–Ω–∏—é": len(people_to_create)})

        self.stdout.write(f"        –ù–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {len(people_to_create)}")

        # –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
        if people_to_create:
            self.stdout.write(f"        –®–∞–≥ 4/6: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤...")
            batch_size = 500
            created_count = 0

            with tqdm(total=len(people_to_create), desc="           –°–æ–∑–¥–∞–Ω–∏–µ", unit=" —á–µ–ª") as pbar:
                for i in range(0, len(people_to_create), batch_size):
                    batch = people_to_create[i:i+batch_size]
                    Person.objects.bulk_create(batch, batch_size=batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))
                    pbar.set_postfix({"—Å–æ–∑–¥–∞–Ω–æ": created_count})

            for person in people_to_create:
                key = f"{person.last_name}|{person.first_name}|{person.middle_name}"
                self.person_cache[key] = person

        # –®–ê–ì 5: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π
        self.stdout.write("        –®–∞–≥ 5/6: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π...")
        unique_pairs = set()
        through_objs = []

        for key, info in author_to_key.items():
            person = existing_people.get(key) or key_to_new_person.get(key)
            if not person:
                continue

            unique_ip_objects = {ip.pk: ip for ip in info['ip_objects']}

            for ip_object in unique_ip_objects.values():
                pair = (ip_object.pk, person.pk)
                if pair not in unique_pairs:
                    unique_pairs.add(pair)
                    through_objs.append(
                        IPObject.authors.through(
                            ipobject_id=ip_object.pk,
                            person_id=person.pk
                        )
                    )

        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {len(through_objs)}")

        # –®–ê–ì 6: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π
        if through_objs:
            self.stdout.write(f"        –®–∞–≥ 6/6: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π...")

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID IP-–æ–±—ä–µ–∫—Ç–æ–≤
            ip_ids = list(set(obj.ipobject_id for obj in through_objs))
            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π –¥–ª—è {len(ip_ids)} IP-–æ–±—ä–µ–∫—Ç–æ–≤...")

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ –ü–ê–ß–ö–ê–ú–ò –ø–æ 500 ID
            delete_batch_size = 500
            deleted_total = 0

            for i in range(0, len(ip_ids), delete_batch_size):
                batch_ip_ids = ip_ids[i:i+delete_batch_size]
                deleted, _ = IPObject.authors.through.objects.filter(
                    ipobject_id__in=batch_ip_ids
                ).delete()
                deleted_total += deleted

                if (i // delete_batch_size) % 10 == 0:
                    self.stdout.write(f"              –£–¥–∞–ª–µ–Ω–æ {deleted_total} —Å–≤—è–∑–µ–π...")

            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π: {deleted_total}")

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏ –ø–∞—á–∫–∞–º–∏
            create_batch_size = 1000
            created_count = 0

            with tqdm(total=len(through_objs), desc="           –î–æ–±–∞–≤–ª–µ–Ω–∏–µ", unit=" —Å–≤—è–∑—å") as pbar:
                for i in range(0, len(through_objs), create_batch_size):
                    batch = through_objs[i:i+create_batch_size]
                    IPObject.authors.through.objects.bulk_create(batch, batch_size=create_batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))
                    pbar.set_postfix({"—Å–æ–∑–¥–∞–Ω–æ": created_count})

        self.stdout.write(f"        ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def _process_holders_batch_with_progress(self, existing_objects, holders_cache):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –∏ —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ø–∞—á–∫–∏"""
        self.stdout.write(f"     ‚ö° –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π...")

        # –®–ê–ì 1: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
        self.stdout.write("        –®–∞–≥ 1/7: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π...")
        all_holders = set()
        for holders_list in holders_cache.values():
            all_holders.update(holders_list)

        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π: {len(all_holders)}")

        # –®–ê–ì 2: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
        self.stdout.write("        –®–∞–≥ 2/7: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤...")
        person_holders = []
        org_holders = []

        with tqdm(total=len(all_holders), desc="           –ê–Ω–∞–ª–∏–∑", unit=" –æ–±") as pbar:
            for holder in all_holders:
                if self.type_detector.detect_type(holder) == 'person':
                    person_holders.append(holder)
                else:
                    org_holders.append(holder)
                pbar.update(1)

        self.stdout.write(f"        –õ—é–¥–∏: {len(person_holders)}, –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: {len(org_holders)}")

        # –®–ê–ì 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–ß–ê–°–¢–Ø–ú–ò)
        self.stdout.write("        –®–∞–≥ 3/7: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π...")
        org_map = {}

        if org_holders:
            CHUNK_SIZE = 1000
            total_orgs = len(org_holders)

            self.stdout.write(f"        –û–±—Ä–∞–±–æ—Ç–∫–∞ {total_orgs} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —á–∞—Å—Ç—è–º–∏ –ø–æ {CHUNK_SIZE}...")

            for chunk_start in range(0, total_orgs, CHUNK_SIZE):
                chunk_end = min(chunk_start + CHUNK_SIZE, total_orgs)
                chunk_holders = org_holders[chunk_start:chunk_end]

                # –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤ —ç—Ç–æ–π —á–∞—Å—Ç–∏
                existing_orgs = {}
                for org in Organization.objects.filter(name__in=chunk_holders):
                    existing_orgs[org.name] = org
                    self.organization_cache[org.name] = org

                # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤ —ç—Ç–æ–π —á–∞—Å—Ç–∏
                orgs_to_create = []
                for holder in chunk_holders:
                    if holder not in existing_orgs and holder not in self.organization_cache:
                        max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
                        new_id = max_id + len(orgs_to_create) + 1

                        base_slug = slugify(holder[:50])
                        if not base_slug:
                            base_slug = 'organization'

                        unique_slug = base_slug
                        counter = 1
                        while Organization.objects.filter(slug=unique_slug).exists() or any(o.slug == unique_slug for o in orgs_to_create):
                            unique_slug = f"{base_slug}-{counter}"
                            counter += 1

                        org = Organization(
                            organization_id=new_id,
                            name=holder,
                            full_name=holder,
                            short_name=holder[:500] if len(holder) > 500 else holder,
                            slug=unique_slug,
                            register_opk=False,
                            strategic=False,
                        )
                        orgs_to_create.append(org)
                        self.organization_cache[holder] = org

                # –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ —Å–æ–∑–¥–∞–µ–º –≤ –ë–î
                if orgs_to_create:
                    batch_size = 500
                    for i in range(0, len(orgs_to_create), batch_size):
                        batch = orgs_to_create[i:i+batch_size]
                        Organization.objects.bulk_create(batch, batch_size=batch_size)

                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                del existing_orgs
                del orgs_to_create

                progress = (chunk_end / total_orgs) * 100
                self.stdout.write(f"           –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%")

            # –§–∏–Ω–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
            for holder in org_holders:
                org_map[holder] = self.organization_cache.get(holder)

        # –®–ê–ì 4: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª—é–¥–µ–π (–ß–ê–°–¢–Ø–ú–ò)
        self.stdout.write("        –®–∞–≥ 4/7: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª—é–¥–µ–π...")
        person_map = {}

        if person_holders:
            CHUNK_SIZE = 500
            total_people = len(person_holders)

            self.stdout.write(f"        –û–±—Ä–∞–±–æ—Ç–∫–∞ {total_people} –ª—é–¥–µ–π —á–∞—Å—Ç—è–º–∏ –ø–æ {CHUNK_SIZE}...")

            for chunk_start in range(0, total_people, CHUNK_SIZE):
                chunk_end = min(chunk_start + CHUNK_SIZE, total_people)
                chunk_holders = person_holders[chunk_start:chunk_end]

                # –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
                existing_people = {}
                for holder in chunk_holders:
                    parts = holder.split()
                    if len(parts) >= 2:
                        last_name = parts[0]
                        first_name = parts[1]
                        middle_name = parts[2] if len(parts) > 2 else ''

                        persons = Person.objects.filter(
                            last_name=last_name,
                            first_name=first_name
                        )
                        if middle_name:
                            persons = persons.filter(middle_name=middle_name)

                        person = persons.first()
                        if person:
                            existing_people[holder] = person
                            self.person_cache[holder] = person

                # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö
                people_to_create = []
                for holder in chunk_holders:
                    if holder not in existing_people and holder not in self.person_cache:
                        parts = holder.split()
                        if len(parts) >= 2:
                            last_name = parts[0]
                            first_name = parts[1]
                            middle_name = parts[2] if len(parts) > 2 else ''

                            name_parts = [last_name, first_name]
                            if middle_name:
                                name_parts.append(middle_name)

                            base_slug = slugify(' '.join(name_parts))
                            if not base_slug:
                                base_slug = 'person'

                            unique_slug = base_slug
                            counter = 1
                            while Person.objects.filter(slug=unique_slug).exists() or any(p.slug == unique_slug for p in people_to_create):
                                unique_slug = f"{base_slug}-{counter}"
                                counter += 1

                            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
                            new_id = max_id + len(people_to_create) + 1

                            person = Person(
                                ceo_id=new_id,
                                ceo=holder,
                                last_name=last_name,
                                first_name=first_name,
                                middle_name=middle_name or '',
                                slug=unique_slug
                            )
                            people_to_create.append(person)
                            self.person_cache[holder] = person

                # –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ —Å–æ–∑–¥–∞–µ–º –≤ –ë–î
                if people_to_create:
                    batch_size = 500
                    for i in range(0, len(people_to_create), batch_size):
                        batch = people_to_create[i:i+batch_size]
                        Person.objects.bulk_create(batch, batch_size=batch_size)

                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                del existing_people
                del people_to_create

                progress = (chunk_end / total_people) * 100
                self.stdout.write(f"           –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%")

            # –§–∏–Ω–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
            for holder in person_holders:
                person_map[holder] = self.person_cache.get(holder)

        # –®–ê–ì 5: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π
        self.stdout.write("        –®–∞–≥ 5/7: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π...")

        org_relations = set()
        person_relations = set()

        with tqdm(total=sum(len(h) for h in holders_cache.values()), desc="           –°–±–æ—Ä —Å–≤—è–∑–µ–π", unit=" —Å–≤") as pbar:
            for reg_num, holders_list in holders_cache.items():
                ip_object = existing_objects.get(reg_num)
                if not ip_object:
                    continue

                for holder in holders_list:
                    if holder in org_map and org_map[holder]:
                        org_relations.add((ip_object.pk, org_map[holder].pk))
                    elif holder in person_map and person_map[holder]:
                        person_relations.add((ip_object.pk, person_map[holder].pk))
                    pbar.update(1)

        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏: {len(org_relations)}")
        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏: {len(person_relations)}")

        # –®–ê–ì 6: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏
        if org_relations:
            self.stdout.write("        –®–∞–≥ 6/7: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏...")

            ip_ids = list(set(ip_id for ip_id, _ in org_relations))

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ –ü–ê–ß–ö–ê–ú–ò
            delete_batch_size = 500
            deleted_total = 0
            for i in range(0, len(ip_ids), delete_batch_size):
                batch_ip_ids = ip_ids[i:i+delete_batch_size]
                deleted, _ = IPObject.owner_organizations.through.objects.filter(
                    ipobject_id__in=batch_ip_ids
                ).delete()
                deleted_total += deleted
                if (i // delete_batch_size) % 10 == 0:
                    self.stdout.write(f"              –£–¥–∞–ª–µ–Ω–æ {deleted_total} —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏...")

            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π: {deleted_total}")

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏ –ø–∞—á–∫–∞–º–∏
            through_objs = [
                IPObject.owner_organizations.through(
                    ipobject_id=ip_id,
                    organization_id=org_id
                )
                for ip_id, org_id in org_relations
            ]

            create_batch_size = 1000
            created_count = 0
            with tqdm(total=len(through_objs), desc="           –î–æ–±–∞–≤–ª–µ–Ω–∏–µ", unit=" —Å–≤") as pbar:
                for i in range(0, len(through_objs), create_batch_size):
                    batch = through_objs[i:i+create_batch_size]
                    IPObject.owner_organizations.through.objects.bulk_create(batch, batch_size=create_batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))

        # –®–ê–ì 7: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏
        if person_relations:
            self.stdout.write("        –®–∞–≥ 7/7: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏...")

            ip_ids = list(set(ip_id for ip_id, _ in person_relations))

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ –ü–ê–ß–ö–ê–ú–ò
            delete_batch_size = 500
            deleted_total = 0
            for i in range(0, len(ip_ids), delete_batch_size):
                batch_ip_ids = ip_ids[i:i+delete_batch_size]
                deleted, _ = IPObject.owner_persons.through.objects.filter(
                    ipobject_id__in=batch_ip_ids
                ).delete()
                deleted_total += deleted
                if (i // delete_batch_size) % 10 == 0:
                    self.stdout.write(f"              –£–¥–∞–ª–µ–Ω–æ {deleted_total} —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏...")

            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π: {deleted_total}")

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏ –ø–∞—á–∫–∞–º–∏
            through_objs = [
                IPObject.owner_persons.through(
                    ipobject_id=ip_id,
                    person_id=person_id
                )
                for ip_id, person_id in person_relations
            ]

            create_batch_size = 1000
            created_count = 0
            with tqdm(total=len(through_objs), desc="           –î–æ–±–∞–≤–ª–µ–Ω–∏–µ", unit=" —Å–≤") as pbar:
                for i in range(0, len(through_objs), create_batch_size):
                    batch = through_objs[i:i+create_batch_size]
                    IPObject.owner_persons.through.objects.bulk_create(batch, batch_size=create_batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))

        self.stdout.write(f"        ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


class UtilityModelParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='utility-model').first()

    def get_required_columns(self):
        return ['registration number', 'utility model name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class IndustrialDesignParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='industrial-design').first()

    def get_required_columns(self):
        return ['registration number', 'industrial design name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class IntegratedCircuitTopologyParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='integrated-circuit-topology').first()

    def get_required_columns(self):
        return ['registration number', 'microchip name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä —Ç–æ–ø–æ–ª–æ–≥–∏–π –º–∏–∫—Ä–æ—Å—Ö–µ–º –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class ComputerProgramParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='computer-program').first()

    def get_required_columns(self):
        return ['registration number', 'program name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class DatabaseParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='database').first()

    def get_required_columns(self):
        return ['registration number', 'db name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}


class Command(BaseCommand):
    help = '–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞'

    def add_arguments(self, parser):
        parser.add_argument('--catalogue-id', type=int, help='ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')
        parser.add_argument('--ip-type', type=str,
                        choices=['invention', 'utility-model', 'industrial-design',
                                'integrated-circuit-topology', 'computer-program', 'database'],
                        help='–¢–∏–ø –†–ò–î –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–∞—Ä—Å—è—Ç—Å—è –≤—Å–µ)')
        parser.add_argument('--dry-run', action='store_true', help='–†–µ–∂–∏–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î')
        parser.add_argument('--encoding', type=str, default='utf-8', help='–ö–æ–¥–∏—Ä–æ–≤–∫–∞ CSV —Ñ–∞–π–ª–∞')
        parser.add_argument('--delimiter', type=str, default=',', help='–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV —Ñ–∞–π–ª–µ')
        parser.add_argument('--batch-size', type=int, default=100, help='–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è bulk-–æ–ø–µ—Ä–∞—Ü–∏–π')
        parser.add_argument('--min-year', type=int, default=2000, help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏')
        parser.add_argument('--skip-filters', action='store_true', help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏)')
        parser.add_argument('--only-active', action='store_true', help='–ü–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç—ã (actual = True)')
        parser.add_argument('--max-rows', type=int, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)')
        parser.add_argument('--force', action='store_true', help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–∂–µ –µ—Å–ª–∏ –∫–∞—Ç–∞–ª–æ–≥ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω')
        parser.add_argument('--mark-processed', action='store_true',
                        help='–ü–æ–º–µ—Ç–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (–¥–∞–∂–µ –µ—Å–ª–∏ –±—ã–ª–∏ –æ—à–∏–±–∫–∏)')

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.parsers = {
            'invention': InventionParser(self),
            'utility-model': UtilityModelParser(self),
            'industrial-design': IndustrialDesignParser(self),
            'integrated-circuit-topology': IntegratedCircuitTopologyParser(self),
            'computer-program': ComputerProgramParser(self),
            'database': DatabaseParser(self),
        }

    def handle(self, *args, **options):
        self.dry_run = options['dry_run']
        self.encoding = options['encoding']
        self.delimiter = options['delimiter']
        self.batch_size = options['batch_size']
        self.min_year = options['min_year']
        self.skip_filters = options['skip_filters']
        self.only_active = options['only_active']
        self.max_rows = options.get('max_rows')
        self.force = options.get('force', False)
        self.mark_processed = options.get('mark_processed', False)

        if self.dry_run:
            self.stdout.write(self.style.WARNING("\nüîç –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î\n"))

        if self.only_active:
            self.stdout.write(self.style.WARNING("üìå –†–µ–∂–∏–º: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (actual = True)"))

        if self.force:
            self.stdout.write(self.style.WARNING("‚ö†Ô∏è  –†–µ–∂–∏–º: –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏)"))

        catalogues = self.get_catalogues(options.get('catalogue_id'), options.get('ip_type'))

        if not catalogues:
            raise CommandError('–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–∞—Ç–∞–ª–æ–≥–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')

        total_stats = {
            'catalogues': len(catalogues),
            'processed': 0,
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        for catalogue in catalogues:
            self.stdout.write(self.style.SUCCESS(f"\n{'='*60}"))
            self.stdout.write(self.style.SUCCESS(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞: {catalogue.name}"))
            self.stdout.write(self.style.SUCCESS(f"   ID: {catalogue.id}, –¢–∏–ø: {catalogue.ip_type.name if catalogue.ip_type else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'}"))
            self.stdout.write(self.style.SUCCESS(f"{'='*60}"))

            stats = self.process_catalogue(catalogue)

            for key in ['processed', 'created', 'updated', 'skipped', 'errors']:
                total_stats[key] += stats.get(key, 0)
            total_stats['skipped_by_date'] += stats.get('skipped_by_date', 0)

        self.print_final_stats(total_stats)

    def get_catalogues(self, catalogue_id=None, ip_type_slug=None):
        queryset = FipsOpenDataCatalogue.objects.all()

        if catalogue_id:
            queryset = queryset.filter(id=catalogue_id)
        elif ip_type_slug:
            queryset = queryset.filter(ip_type__slug=ip_type_slug)
        else:
            queryset = queryset.exclude(catalogue_file='')

        return queryset.order_by('ip_type__id', '-publication_date')

    def process_catalogue(self, catalogue):
        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        if not catalogue.catalogue_file:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –£ –∫–∞—Ç–∞–ª–æ–≥–∞ ID={catalogue.id} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª"))
            stats['errors'] += 1
            return stats

        if not self.force and hasattr(catalogue, 'parsed_date') and catalogue.parsed_date:
            self.stdout.write(self.style.WARNING(
                f"  ‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ —É–∂–µ –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω {catalogue.parsed_date.strftime('%d.%m.%Y %H:%M')}"
            ))
            self.stdout.write(self.style.WARNING(f"     –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --force –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"))
            stats['skipped'] += 1
            return stats

        ip_type_slug = catalogue.ip_type.slug if catalogue.ip_type else None

        if ip_type_slug not in self.parsers:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –ù–µ—Ç –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è —Ç–∏–ø–∞ –†–ò–î: {ip_type_slug}"))
            stats['errors'] += 1
            return stats

        parser = self.parsers[ip_type_slug]
        df = self.load_csv(catalogue)

        if df is None or df.empty:
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å"))
            stats['skipped'] += 1
            return stats

        self.stdout.write(f"  üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")

        missing_columns = self.check_required_columns(df, parser.get_required_columns())
        if missing_columns:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}"))
            stats['errors'] += 1
            return stats

        if not self.skip_filters:
            df = self.apply_filters(df)

        if df.empty:
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"))
            stats['skipped'] += 1
            return stats

        self.stdout.write(f"  üìä –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df)} –∑–∞–ø–∏—Å–µ–π")

        if self.max_rows and len(df) > self.max_rows:
            df = df.head(self.max_rows)
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {self.max_rows} –∑–∞–ø–∏—Å–µ–π"))

        try:
            parser_stats = parser.parse_dataframe(df, catalogue)
            stats.update(parser_stats)

            if not self.dry_run and hasattr(catalogue, 'parsed_date'):
                if stats['errors'] == 0 or self.mark_processed:
                    catalogue.parsed_date = timezone.now()
                    catalogue.save(update_fields=['parsed_date'])
                    self.stdout.write(self.style.SUCCESS(f"  ‚úÖ –ö–∞—Ç–∞–ª–æ–≥ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π"))
                else:
                    self.stdout.write(self.style.WARNING(
                        f"  ‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ –Ω–µ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫"
                    ))

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}"))
            logger.error(f"Error parsing catalogue {catalogue.id}: {e}", exc_info=True)
            stats['errors'] += 1

        return stats

    def load_csv(self, catalogue):
        file_path = catalogue.catalogue_file.path

        if not os.path.exists(file_path):
            self.stdout.write(self.style.ERROR(f"  ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"))
            return None

        try:
            strategies = [
                {'encoding': self.encoding, 'delimiter': self.delimiter, 'skipinitialspace': True},
                {'encoding': 'cp1251', 'delimiter': self.delimiter, 'skipinitialspace': True},
                {'encoding': 'utf-8', 'delimiter': ';', 'skipinitialspace': True},
                {'encoding': 'cp1251', 'delimiter': ';', 'skipinitialspace': True},
                {'encoding': 'utf-8', 'delimiter': '\t', 'skipinitialspace': True},
            ]

            for strategy in strategies:
                try:
                    df = pd.read_csv(file_path, **strategy, dtype=str, keep_default_na=False)
                    self.stdout.write(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {strategy}")

                    df.columns = [col.strip().strip('\ufeff').strip('"') for col in df.columns]

                    return df
                except Exception as e:
                    continue

            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π")

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}"))
            return None

    def check_required_columns(self, df, required_columns):
        missing = [col for col in required_columns if col not in df.columns]
        return missing

    def apply_filters(self, df):
        original_count = len(df)

        if 'registration date' in df.columns:
            df = self.filter_by_registration_year(df)

        if self.only_active and 'actual' in df.columns:
            df = self.filter_by_actual(df)

        filtered_count = len(df)
        if filtered_count < original_count:
            self.stdout.write(f"  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {original_count} ‚Üí {filtered_count} –∑–∞–ø–∏—Å–µ–π")

        return df

    def filter_by_registration_year(self, df):
        def extract_year(date_str):
            try:
                if pd.isna(date_str) or not date_str:
                    return None

                date_str = str(date_str).strip()
                if not date_str:
                    return None

                for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
                    try:
                        return datetime.strptime(date_str, fmt).year
                    except (ValueError, TypeError):
                        continue

                try:
                    return pd.to_datetime(date_str).year
                except (ValueError, TypeError):
                    return None
            except:
                return None

        self.stdout.write("  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≥–æ–¥—É —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏...")
        df['_year'] = df['registration date'].apply(extract_year)

        years_dist = df['_year'].value_counts().sort_index()
        years_list = list(years_dist.items())
        if len(years_list) > 0:
            self.stdout.write(f"     –î–∏–∞–ø–∞–∑–æ–Ω –≥–æ–¥–æ–≤: {years_list[0][0]:.0f} - {years_list[-1][0]:.0f}")
            self.stdout.write(f"     –ü–µ—Ä–≤—ã–µ 5: {years_list[:5]}")
            self.stdout.write(f"     –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5: {years_list[-5:]}")

        filtered_df = df[df['_year'] >= self.min_year].copy()
        filtered_df.drop('_year', axis=1, inplace=True)

        return filtered_df

    def filter_by_actual(self, df):
        def parse_actual(value):
            if pd.isna(value) or not value:
                return False
            value = str(value).lower().strip()
            return value in ['1', 'true', 'yes', '–¥–∞', '–¥–µ–π—Å—Ç–≤—É–µ—Ç', 't', '1.0', '–∞–∫—Ç–∏–≤–µ–Ω']

        df['_actual'] = df['actual'].apply(parse_actual)
        filtered_df = df[df['_actual'] == True].copy()
        filtered_df.drop('_actual', axis=1, inplace=True)

        return filtered_df

    def print_final_stats(self, stats):
        self.stdout.write(self.style.SUCCESS(f"\n{'='*60}"))
        self.stdout.write(self.style.SUCCESS("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê"))
        self.stdout.write(self.style.SUCCESS(f"{'='*60}"))
        self.stdout.write(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞—Ç–∞–ª–æ–≥–æ–≤: {stats['catalogues']}")
        self.stdout.write(f"üìù –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}")
        self.stdout.write(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ: {stats['created']}")
        self.stdout.write(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}")
        self.stdout.write(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ –≤—Å–µ–≥–æ: {stats['skipped']}")
        self.stdout.write(f"   ‚îî‚îÄ –ø–æ –¥–∞—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {stats.get('skipped_by_date', 0)}")

        if stats['errors'] > 0:
            self.stdout.write(self.style.ERROR(f"‚ùå –û—à–∏–±–æ–∫: {stats['errors']}"))
        else:
            self.stdout.write(self.style.SUCCESS(f"‚úÖ –û—à–∏–±–æ–∫: {stats['errors']}"))

        if self.dry_run:
            self.stdout.write(self.style.WARNING("\nüîç –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î"))

        self.stdout.write(self.style.SUCCESS(f"{'='*60}"))

```


-----

# –§–∞–π–ª: management\commands\__init__.py

```

```


-----

# –§–∞–π–ª: management\parsers\base.py

```
"""
–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –§–ò–ü–°
"""

import logging
import re
from datetime import datetime
from typing import Optional, Tuple, List, Dict, Any, Set
from collections import defaultdict

from django.db import models
from django.utils.text import slugify
import pandas as pd

from intellectual_property.models import IPObject, IPType
from core.models import Person, Organization, Country

# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç—ã –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
from .processors import (
    RussianTextProcessor,
    OrganizationNormalizer,
    PersonNameFormatter,
    RIDNameFormatter,
    EntityTypeDetector
)

logger = logging.getLogger(__name__)


class BaseFIPSParser:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –§–ò–ü–°"""

    def __init__(self, command):
        self.command = command
        self.stdout = command.stdout
        self.style = command.style

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
        self.processor = RussianTextProcessor()
        self.org_normalizer = OrganizationNormalizer()
        self.type_detector = EntityTypeDetector()
        self.person_formatter = PersonNameFormatter()
        self.rid_formatter = RIDNameFormatter()

        # –ö—ç—à–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.country_cache = {}
        self.person_cache = {}
        self.organization_cache = {}
        self.foiv_cache = {}
        self.rf_rep_cache = {}
        self.city_cache = {}
        self.activity_type_cache = {}
        self.ceo_position_cache = {}

    def get_ip_type(self):
        """–î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –¥–æ—á–µ—Ä–Ω–∏—Ö –∫–ª–∞—Å—Å–∞—Ö"""
        raise NotImplementedError

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
        raise NotImplementedError

    def parse_dataframe(self, df, catalogue):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame"""
        raise NotImplementedError

    def clean_string(self, value):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if pd.isna(value) or value is None:
            return ''
        value = str(value).strip()
        if value in ['', 'None', 'null', 'NULL', 'nan']:
            return ''
        return value

    def parse_date(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        if pd.isna(value) or not value:
            return None

        date_str = str(value).strip()
        if not date_str:
            return None

        for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
            try:
                return datetime.strptime(date_str, fmt).date()
            except (ValueError, TypeError):
                continue

        try:
            return pd.to_datetime(date_str).date()
        except (ValueError, TypeError):
            return None

    def parse_bool(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –±—É–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if pd.isna(value) or not value:
            return False
        value = str(value).lower().strip()
        return value in ['1', 'true', 'yes', '–¥–∞', '–¥–µ–π—Å—Ç–≤—É–µ—Ç', 't', '1.0', '–∞–∫—Ç–∏–≤–µ–Ω']

    def get_or_create_country(self, code):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –ø–æ –∫–æ–¥—É"""
        if not code or pd.isna(code):
            return None

        code = str(code).upper().strip()
        if len(code) != 2:
            return None

        if code in self.country_cache:
            return self.country_cache[code]

        try:
            country = Country.objects.filter(code=code).first()
            if country:
                self.country_cache[code] = country
                return country

            country = Country.objects.filter(code_alpha3=code).first()
            if country:
                self.country_cache[code] = country
                return country

            return None

        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω—ã {code}: {e}"))
            return None

    def parse_authors(self, authors_str):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –∞–≤—Ç–æ—Ä–∞–º–∏"""
        if pd.isna(authors_str) or not authors_str:
            return []

        authors_str = str(authors_str)
        authors_list = re.split(r'[\n,]\s*', authors_str)

        result = []
        for author in authors_list:
            author = author.strip()
            if not author or author == '""' or author == 'null':
                continue

            author = author.strip('"')
            author = re.sub(r'\s*\([A-Z]{2}\)', '', author)
            author = self.person_formatter.format(author)

            parts = author.split()

            if len(parts) >= 2:
                last_name = parts[0]
                first_name = parts[1] if len(parts) > 1 else ''
                middle_name = parts[2] if len(parts) > 2 else ''

                first_name_clean = first_name.replace('.', '')
                middle_name_clean = middle_name.replace('.', '')

                result.append({
                    'last_name': last_name,
                    'first_name': first_name_clean,
                    'middle_name': middle_name_clean,
                    'full_name': author,
                })
            else:
                result.append({
                    'last_name': author,
                    'first_name': '',
                    'middle_name': '',
                    'full_name': author,
                })

        return result

    def parse_patent_holders(self, holders_str):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—è–º–∏"""
        if pd.isna(holders_str) or not holders_str:
            return []

        holders_str = str(holders_str)
        holders_list = re.split(r'[\n]\s*', holders_str)

        result = []
        for holder in holders_list:
            holder = holder.strip().strip('"')
            if not holder or holder == 'null' or holder == 'None':
                continue

            holder = re.sub(r'\s*\([A-Z]{2}\)', '', holder)
            result.append(holder)

        return result

    def find_or_create_person(self, person_data):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞"""
        cache_key = f"{person_data['last_name']}|{person_data['first_name']}|{person_data['middle_name']}"

        if cache_key in self.person_cache:
            return self.person_cache[cache_key]

        persons = Person.objects.filter(
            last_name=person_data['last_name'],
            first_name=person_data['first_name']
        )

        if person_data['middle_name']:
            persons = persons.filter(middle_name=person_data['middle_name'])

        if persons.exists():
            person = persons.first()
            self.person_cache[cache_key] = person
            return person

        try:
            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
            new_id = max_id + 1

            if 'full_name' in person_data:
                full_name = person_data['full_name']
            else:
                full_name_parts = [person_data['last_name'], person_data['first_name']]
                if person_data['middle_name']:
                    full_name_parts.append(person_data['middle_name'])
                full_name = ' '.join(full_name_parts)
                full_name = self.person_formatter.format(full_name)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π slug
            base_slug = slugify(f"{person_data['last_name']} {person_data['first_name']} {person_data['middle_name']}".strip())
            if not base_slug:
                base_slug = 'person'

            unique_slug = base_slug
            counter = 1
            while Person.objects.filter(slug=unique_slug).exists():
                unique_slug = f"{base_slug}-{counter}"
                counter += 1

            person = Person.objects.create(
                ceo_id=new_id,
                ceo=full_name,
                last_name=person_data['last_name'],
                first_name=person_data['first_name'],
                middle_name=person_data['middle_name'],
                slug=unique_slug
            )
            self.person_cache[cache_key] = person
            return person
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Person: {e}"))
            return None

    def find_or_create_person_from_name(self, full_name):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞ –ø–æ –ø–æ–ª–Ω–æ–º—É –∏–º–µ–Ω–∏"""
        if pd.isna(full_name) or not full_name:
            return None

        full_name = str(full_name).strip().strip('"')
        full_name = self.person_formatter.format(full_name)

        if full_name in self.person_cache:
            return self.person_cache[full_name]

        parts = full_name.split()

        if len(parts) >= 2:
            last_name = parts[0]
            first_name = parts[1] if len(parts) > 1 else ''
            middle_name = parts[2] if len(parts) > 2 else ''

            first_name_clean = first_name.replace('.', '')
            middle_name_clean = middle_name.replace('.', '')

            person_data = {
                'last_name': last_name,
                'first_name': first_name_clean,
                'middle_name': middle_name_clean,
                'full_name': full_name,
            }
        else:
            person_data = {
                'last_name': full_name,
                'first_name': '',
                'middle_name': '',
                'full_name': full_name,
            }

        return self.find_or_create_person(person_data)

    def find_similar_organization(self, org_name):
        """–£—Å–∏–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        if pd.isna(org_name) or not org_name:
            return None

        org_name = str(org_name).strip().strip('"')

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        direct_match = Organization.objects.filter(
            models.Q(name=org_name) |
            models.Q(full_name=org_name) |
            models.Q(short_name=org_name)
        ).first()
        if direct_match:
            return direct_match

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
        norm_data = self.org_normalizer.normalize_for_search(org_name)
        normalized = norm_data['normalized']
        keywords = norm_data['keywords']

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in keywords:
            if len(keyword) >= 3:
                similar = Organization.objects.filter(
                    models.Q(name__icontains=keyword) |
                    models.Q(full_name__icontains=keyword) |
                    models.Q(short_name__icontains=keyword)
                ).first()
                if similar:
                    return similar

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ –ø–æ –ø–µ—Ä–≤—ã–º 30 —Å–∏–º–≤–æ–ª–∞–º
        if len(normalized) > 30:
            prefix = normalized[:30]
            similar = Organization.objects.filter(
                models.Q(name__icontains=prefix) |
                models.Q(full_name__icontains=prefix) |
                models.Q(short_name__icontains=prefix)
            ).first()
            if similar:
                return similar

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ü–æ–∏—Å–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–ª–æ–≤–∞–º
        words = org_name.split()
        for word in words:
            if len(word) > 4:
                similar = Organization.objects.filter(
                    models.Q(name__icontains=word) |
                    models.Q(full_name__icontains=word)
                ).first()
                if similar:
                    return similar

        return None

    def find_or_create_organization(self, org_name):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        if pd.isna(org_name) or not org_name:
            return None

        org_name = str(org_name).strip().strip('"')

        if not org_name or org_name == 'null' or org_name == 'None':
            return None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if org_name in self.organization_cache:
            return self.organization_cache[org_name]

        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ
        similar = self.find_similar_organization(org_name)
        if similar:
            self.organization_cache[org_name] = similar
            return similar

        # –ù–µ –Ω–∞—à–ª–∏ - —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º
        try:
            max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
            new_id = max_id + 1

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º slug –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
            base_slug = slugify(org_name[:50])
            if not base_slug:
                base_slug = 'organization'

            unique_slug = base_slug
            counter = 1
            while Organization.objects.filter(slug=unique_slug).exists():
                unique_slug = f"{base_slug}-{counter}"
                counter += 1

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            org = Organization.objects.create(
                organization_id=new_id,
                name=org_name,
                full_name=org_name,
                short_name=org_name[:500] if len(org_name) > 500 else org_name,
                slug=unique_slug,
                register_opk=False,
                strategic=False,
            )

            self.organization_cache[org_name] = org
            return org
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Organization: {e}"))
            return None

```


-----

# –§–∞–π–ª: management\parsers\computer_program.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú
"""

from intellectual_property.models import IPType
from .base import BaseFIPSParser


class ComputerProgramParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='computer-program').first()

    def get_required_columns(self):
        return ['registration number', 'program name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}

```


-----

# –§–∞–π–ª: management\parsers\database.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
"""

from intellectual_property.models import IPType
from .base import BaseFIPSParser


class DatabaseParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='database').first()

    def get_required_columns(self):
        return ['registration number', 'db name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}

```


-----

# –§–∞–π–ª: management\parsers\industrial_design.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
"""

from intellectual_property.models import IPType
from .base import BaseFIPSParser


class IndustrialDesignParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='industrial-design').first()

    def get_required_columns(self):
        return ['registration number', 'industrial design name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}

```


-----

# –§–∞–π–ª: management\parsers\integrated_circuit.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º
"""

from intellectual_property.models import IPType
from .base import BaseFIPSParser


class IntegratedCircuitTopologyParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='integrated-circuit-topology').first()

    def get_required_columns(self):
        return ['registration number', 'microchip name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä —Ç–æ–ø–æ–ª–æ–≥–∏–π –º–∏–∫—Ä–æ—Å—Ö–µ–º –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}

```


-----

# –§–∞–π–ª: management\parsers\invention.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
"""

import logging
from collections import defaultdict

from django.db import models
from django.utils.text import slugify
from tqdm import tqdm
import pandas as pd

# –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∏–º–ø–æ—Ä—Ç—ã
from intellectual_property.models import IPObject, IPType, Person
from core.models import Organization  # <-- –î–æ–±–∞–≤–ª–µ–Ω —ç—Ç–æ—Ç –∏–º–ø–æ—Ä—Ç
from .base import BaseFIPSParser

logger = logging.getLogger(__name__)


class InventionParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='invention').first()

    def get_required_columns(self):
        return ['registration number', 'invention name']

    def _has_data_changed(self, obj, new_data):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ"""
        fields_to_check = [
            ('name', obj.name, new_data['name']),
            ('application_date', obj.application_date, new_data['application_date']),
            ('registration_date', obj.registration_date, new_data['registration_date']),
            ('patent_starting_date', obj.patent_starting_date, new_data['patent_starting_date']),
            ('expiration_date', obj.expiration_date, new_data['expiration_date']),
            ('actual', obj.actual, new_data['actual']),
            ('publication_url', obj.publication_url, new_data['publication_url']),
            ('abstract', obj.abstract, new_data['abstract']),
            ('claims', obj.claims, new_data['claims']),
            ('creation_year', obj.creation_year, new_data['creation_year']),
        ]

        for field_name, old_val, new_val in fields_to_check:
            if old_val != new_val:
                return True
        return False

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π..."))

        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'invention' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats

        # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Ç–∞–ª–æ–≥–∞
        upload_date = catalogue.upload_date.date() if catalogue.upload_date else None

        # –®–ê–ì 1: –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞
        self.stdout.write("  üì• –ß—Ç–µ–Ω–∏–µ CSV...")
        all_reg_numbers = []
        reg_num_to_row = {}

        with tqdm(total=len(df), desc="     –ü—Ä–æ–≥—Ä–µ—Å—Å", unit=" –∑–∞–ø", leave=False) as pbar:
            for idx, row in df.iterrows():
                reg_num = self.clean_string(row.get('registration number'))
                if reg_num:
                    all_reg_numbers.append(reg_num)
                    reg_num_to_row[reg_num] = row
                pbar.update(1)

        self.stdout.write(f"  üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: {len(all_reg_numbers)}")

        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –ü–ê–ß–ö–ê–ú–ò
        self.stdout.write("  üîç –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î...")
        existing_objects = {}
        batch_size = 500

        with tqdm(total=len(all_reg_numbers), desc="     –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—á–∫–∞–º–∏", unit=" –∑–∞–ø") as pbar:
            for i in range(0, len(all_reg_numbers), batch_size):
                batch_numbers = all_reg_numbers[i:i+batch_size]

                for obj in IPObject.objects.filter(
                    registration_number__in=batch_numbers,
                    ip_type=ip_type
                ).select_related('ip_type'):
                    existing_objects[obj.registration_number] = obj

                pbar.update(len(batch_numbers))

                if (i // batch_size) % 10 == 0:
                    pbar.set_postfix({"–Ω–∞–π–¥–µ–Ω–æ": len(existing_objects)})

        self.stdout.write(f"  üìä –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(existing_objects)}")

        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.stdout.write("  üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        to_create = []
        to_update = []
        skipped_by_date = []
        unchanged_count = 0
        error_reg_numbers = []

        authors_cache = defaultdict(list)
        holders_cache = defaultdict(list)

        with tqdm(total=len(reg_num_to_row), desc="     –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–µ–π", unit=" –∑–∞–ø") as pbar:
            for reg_num, row in reg_num_to_row.items():
                try:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –¥–∞—Ç–µ
                    if not self.command.force and upload_date and reg_num in existing_objects:
                        existing = existing_objects[reg_num]
                        if existing.updated_at and existing.updated_at.date() >= upload_date:
                            skipped_by_date.append(reg_num)
                            pbar.update(1)
                            continue

                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                    name = self.clean_string(row.get('invention name'))
                    if name:
                        name = self.rid_formatter.format(name)
                    else:
                        name = f"–ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ ‚Ññ{reg_num}"

                    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã
                    application_date = self.parse_date(row.get('application date'))
                    registration_date = self.parse_date(row.get('registration date'))
                    patent_starting_date = self.parse_date(row.get('patent starting date'))
                    expiration_date = self.parse_date(row.get('expiration date'))
                    actual = self.parse_bool(row.get('actual'))
                    publication_url = self.clean_string(row.get('publication URL'))
                    abstract = self.clean_string(row.get('abstract'))
                    claims = self.clean_string(row.get('claims'))

                    creation_year = None
                    if application_date:
                        creation_year = application_date.year
                    elif registration_date:
                        creation_year = registration_date.year

                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                    obj_data = {
                        'registration_number': reg_num,
                        'ip_type': ip_type,
                        'name': name,
                        'application_date': application_date,
                        'registration_date': registration_date,
                        'patent_starting_date': patent_starting_date,
                        'expiration_date': expiration_date,
                        'actual': actual,
                        'publication_url': publication_url,
                        'abstract': abstract,
                        'claims': claims,
                        'creation_year': creation_year,
                    }

                    if reg_num in existing_objects:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
                        existing = existing_objects[reg_num]
                        if self._has_data_changed(existing, obj_data):
                            to_update.append(obj_data)
                        else:
                            unchanged_count += 1
                    else:
                        to_create.append(obj_data)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–≤—Ç–æ—Ä–æ–≤ –∏ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
                    authors_str = row.get('authors')
                    if not pd.isna(authors_str) and authors_str:
                        authors_cache[reg_num] = self.parse_authors(authors_str)

                    holders_str = row.get('patent holders')
                    if not pd.isna(holders_str) and holders_str:
                        holders_cache[reg_num] = self.parse_patent_holders(holders_str)

                except Exception as e:
                    error_reg_numbers.append(reg_num)
                    self.stdout.write(self.style.ERROR(f"\n  ‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    logger.error(f"Error preparing invention {reg_num}: {e}", exc_info=True)

                pbar.update(1)
                if pbar.n % 1000 == 0:
                    pbar.set_postfix({
                        "–Ω–æ–≤—ã–µ": len(to_create),
                        "–æ–±–Ω–æ–≤": len(to_update),
                        "–±–µ–∑ –∏–∑–º": unchanged_count,
                        "–ø—Ä–æ–ø—É—â": len(skipped_by_date)
                    })

        stats['skipped_by_date'] = len(skipped_by_date)
        stats['skipped'] += len(skipped_by_date)
        stats['errors'] = len(error_reg_numbers)
        stats['unchanged'] = unchanged_count

        self.stdout.write(f"     –ò—Ç–æ–≥–æ: –Ω–æ–≤—ã—Ö={len(to_create)}, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ={len(to_update)}, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={unchanged_count}")

        # –®–ê–ì 4: –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
        if to_create and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –°–æ–∑–¥–∞–Ω–∏–µ {len(to_create)} –∑–∞–ø–∏—Å–µ–π...")
            create_objects = [IPObject(**data) for data in to_create]

            batch_size = 1000
            created_count = 0

            with tqdm(total=len(create_objects), desc="     –°–æ–∑–¥–∞–Ω–∏–µ", unit=" –∑–∞–ø") as pbar:
                for i in range(0, len(create_objects), batch_size):
                    batch = create_objects[i:i+batch_size]
                    IPObject.objects.bulk_create(batch, batch_size=batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))
                    pbar.set_postfix({"—Å–æ–∑–¥–∞–Ω–æ": created_count})

            stats['created'] = created_count

            # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –Ω–æ–≤—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏
            self.stdout.write("     –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞...")

            with tqdm(total=len(to_create), desc="     –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞", unit=" –∑–∞–ø") as pbar:
                for i in range(0, len(to_create), batch_size):
                    batch_data = to_create[i:i+batch_size]
                    batch_nums = [d['registration_number'] for d in batch_data]

                    for obj in IPObject.objects.filter(
                        registration_number__in=batch_nums,
                        ip_type=ip_type
                    ):
                        existing_objects[obj.registration_number] = obj

                    pbar.update(len(batch_data))

        # –®–ê–ì 5: –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π
        if to_update and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {len(to_update)} –∑–∞–ø–∏—Å–µ–π...")
            updated_count = 0

            with tqdm(total=len(to_update), desc="     –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ", unit=" –∑–∞–ø") as pbar:
                for data in to_update:
                    obj = existing_objects[data['registration_number']]
                    update_fields = []

                    if obj.name != data['name']:
                        obj.name = data['name']
                        update_fields.append('name')

                    if obj.application_date != data['application_date']:
                        obj.application_date = data['application_date']
                        update_fields.append('application_date')

                    if obj.registration_date != data['registration_date']:
                        obj.registration_date = data['registration_date']
                        update_fields.append('registration_date')

                    if obj.patent_starting_date != data['patent_starting_date']:
                        obj.patent_starting_date = data['patent_starting_date']
                        update_fields.append('patent_starting_date')

                    if obj.expiration_date != data['expiration_date']:
                        obj.expiration_date = data['expiration_date']
                        update_fields.append('expiration_date')

                    if obj.actual != data['actual']:
                        obj.actual = data['actual']
                        update_fields.append('actual')

                    if obj.publication_url != data['publication_url']:
                        obj.publication_url = data['publication_url']
                        update_fields.append('publication_url')

                    if obj.abstract != data['abstract']:
                        obj.abstract = data['abstract']
                        update_fields.append('abstract')

                    if obj.claims != data['claims']:
                        obj.claims = data['claims']
                        update_fields.append('claims')

                    if obj.creation_year != data['creation_year']:
                        obj.creation_year = data['creation_year']
                        update_fields.append('creation_year')

                    if update_fields:
                        obj.save(update_fields=update_fields)
                        updated_count += 1

                    pbar.update(1)
                    if pbar.n % 100 == 0:
                        pbar.set_postfix({"–æ–±–Ω–æ–≤–ª–µ–Ω–æ": updated_count})

            stats['updated'] = updated_count
            self.stdout.write(f"     –†–µ–∞–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ: {updated_count} –∏–∑ {len(to_update)}")

        # –®–ê–ì 6: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤
        if authors_cache and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ ({len(authors_cache)} –∑–∞–ø–∏—Å–µ–π)...")
            self._process_authors_batch_with_progress(existing_objects, authors_cache)

        # –®–ê–ì 7: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
        if holders_cache and not self.command.dry_run:
            self.stdout.write(f"  üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π ({len(holders_cache)} –∑–∞–ø–∏—Å–µ–π)...")
            self._process_holders_batch_with_progress(existing_objects, holders_cache)

        stats['processed'] = len(df) - stats['skipped'] - stats['errors']

        self.stdout.write(self.style.SUCCESS(f"  ‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"     –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats['unchanged']}, "
                         f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –≤—Å–µ–≥–æ: {stats['skipped']} (–∏–∑ –Ω–∏—Ö –ø–æ –¥–∞—Ç–µ: {stats['skipped_by_date']}), "
                         f"–û—à–∏–±–æ–∫: {stats['errors']}")

        return stats

    def _process_authors_batch_with_progress(self, existing_objects, authors_cache):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –∏ —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ø–∞—á–∫–∏"""
        self.stdout.write(f"     ‚ö° –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤...")

        # –®–ê–ì 1: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
        self.stdout.write("        –®–∞–≥ 1/6: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤...")
        author_to_key = {}
        total_relations = 0

        for reg_num, authors_data in authors_cache.items():
            ip_object = existing_objects.get(reg_num)
            if not ip_object:
                continue

            for author_data in authors_data:
                key = f"{author_data['last_name']}|{author_data['first_name']}|{author_data['middle_name']}"
                if key not in author_to_key:
                    author_to_key[key] = {
                        'data': author_data,
                        'ip_objects': []
                    }
                author_to_key[key]['ip_objects'].append(ip_object)
                total_relations += 1

        all_keys = list(author_to_key.keys())
        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤: {len(all_keys)}, –≤—Å–µ–≥–æ —Å–≤—è–∑–µ–π: {total_relations}")

        # –®–ê–ì 2: –ü–æ–∏—Å–∫ –≤ –ë–î
        self.stdout.write("        –®–∞–≥ 2/6: –ü–æ–∏—Å–∫ –≤ –ë–î...")
        existing_people = {}
        batch_size = 50

        with tqdm(total=len(all_keys), desc="           –ü–æ–∏—Å–∫", unit=" –∫–ª—é—á") as pbar:
            for i in range(0, len(all_keys), batch_size):
                batch_keys = all_keys[i:i+batch_size]

                name_conditions = models.Q()
                for key in batch_keys:
                    last, first, middle = key.split('|')
                    if middle:
                        name_conditions |= models.Q(
                            last_name=last,
                            first_name=first,
                            middle_name=middle
                        )
                    else:
                        name_conditions |= models.Q(
                            last_name=last,
                            first_name=first,
                            middle_name__isnull=True
                        ) | models.Q(
                            last_name=last,
                            first_name=first,
                            middle_name=''
                        )

                for person in Person.objects.filter(name_conditions):
                    key = f"{person.last_name}|{person.first_name}|{person.middle_name or ''}"
                    existing_people[key] = person
                    self.person_cache[key] = person

                pbar.update(len(batch_keys))
                if (i // batch_size) % 10 == 0:
                    pbar.set_postfix({"–Ω–∞–π–¥–µ–Ω–æ": len(existing_people)})

        self.stdout.write(f"        –ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {len(existing_people)}")

        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
        self.stdout.write("        –®–∞–≥ 3/6: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤...")
        people_to_create = []
        key_to_new_person = {}

        max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
        next_id = max_id + 1
        existing_slugs = set(Person.objects.values_list('slug', flat=True))

        with tqdm(total=len(all_keys), desc="           –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞", unit=" –∫–ª—é—á") as pbar:
            for key, info in author_to_key.items():
                if key not in existing_people:
                    author_data = info['data']

                    name_parts = [author_data['last_name'], author_data['first_name']]
                    if author_data['middle_name']:
                        name_parts.append(author_data['middle_name'])

                    base_slug = slugify(' '.join(name_parts).strip())
                    if not base_slug:
                        base_slug = 'person'

                    unique_slug = base_slug
                    counter = 1
                    while unique_slug in existing_slugs or any(p.slug == unique_slug for p in people_to_create):
                        unique_slug = f"{base_slug}-{counter}"
                        counter += 1

                    person = Person(
                        ceo_id=next_id,
                        ceo=author_data['full_name'],
                        last_name=author_data['last_name'],
                        first_name=author_data['first_name'],
                        middle_name=author_data['middle_name'] or '',
                        slug=unique_slug
                    )
                    people_to_create.append(person)
                    key_to_new_person[key] = person
                    next_id += 1
                    existing_slugs.add(unique_slug)

                pbar.update(1)
                if pbar.n % 10000 == 0:
                    pbar.set_postfix({"–∫ —Å–æ–∑–¥–∞–Ω–∏—é": len(people_to_create)})

        self.stdout.write(f"        –ù–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {len(people_to_create)}")

        # –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
        if people_to_create:
            self.stdout.write(f"        –®–∞–≥ 4/6: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤...")
            batch_size = 500
            created_count = 0

            with tqdm(total=len(people_to_create), desc="           –°–æ–∑–¥–∞–Ω–∏–µ", unit=" —á–µ–ª") as pbar:
                for i in range(0, len(people_to_create), batch_size):
                    batch = people_to_create[i:i+batch_size]
                    Person.objects.bulk_create(batch, batch_size=batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))
                    pbar.set_postfix({"—Å–æ–∑–¥–∞–Ω–æ": created_count})

            for person in people_to_create:
                key = f"{person.last_name}|{person.first_name}|{person.middle_name}"
                self.person_cache[key] = person

        # –®–ê–ì 5: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π
        self.stdout.write("        –®–∞–≥ 5/6: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π...")
        unique_pairs = set()
        through_objs = []

        for key, info in author_to_key.items():
            person = existing_people.get(key) or key_to_new_person.get(key)
            if not person:
                continue

            unique_ip_objects = {ip.pk: ip for ip in info['ip_objects']}

            for ip_object in unique_ip_objects.values():
                pair = (ip_object.pk, person.pk)
                if pair not in unique_pairs:
                    unique_pairs.add(pair)
                    through_objs.append(
                        IPObject.authors.through(
                            ipobject_id=ip_object.pk,
                            person_id=person.pk
                        )
                    )

        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {len(through_objs)}")

        # –®–ê–ì 6: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π
        if through_objs:
            self.stdout.write(f"        –®–∞–≥ 6/6: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π...")

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID IP-–æ–±—ä–µ–∫—Ç–æ–≤
            ip_ids = list(set(obj.ipobject_id for obj in through_objs))
            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π –¥–ª—è {len(ip_ids)} IP-–æ–±—ä–µ–∫—Ç–æ–≤...")

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ –ü–ê–ß–ö–ê–ú–ò –ø–æ 500 ID
            delete_batch_size = 500
            deleted_total = 0

            for i in range(0, len(ip_ids), delete_batch_size):
                batch_ip_ids = ip_ids[i:i+delete_batch_size]
                deleted, _ = IPObject.authors.through.objects.filter(
                    ipobject_id__in=batch_ip_ids
                ).delete()
                deleted_total += deleted

                if (i // delete_batch_size) % 10 == 0:
                    self.stdout.write(f"              –£–¥–∞–ª–µ–Ω–æ {deleted_total} —Å–≤—è–∑–µ–π...")

            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π: {deleted_total}")

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏ –ø–∞—á–∫–∞–º–∏
            create_batch_size = 1000
            created_count = 0

            with tqdm(total=len(through_objs), desc="           –î–æ–±–∞–≤–ª–µ–Ω–∏–µ", unit=" —Å–≤—è–∑—å") as pbar:
                for i in range(0, len(through_objs), create_batch_size):
                    batch = through_objs[i:i+create_batch_size]
                    IPObject.authors.through.objects.bulk_create(batch, batch_size=create_batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))
                    pbar.set_postfix({"—Å–æ–∑–¥–∞–Ω–æ": created_count})

        self.stdout.write(f"        ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def _process_holders_batch_with_progress(self, existing_objects, holders_cache):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –∏ —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ø–∞—á–∫–∏"""
        self.stdout.write(f"     ‚ö° –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π...")

        # –®–ê–ì 1: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
        self.stdout.write("        –®–∞–≥ 1/7: –°–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π...")
        all_holders = set()
        for holders_list in holders_cache.values():
            all_holders.update(holders_list)

        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π: {len(all_holders)}")

        # –®–ê–ì 2: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
        self.stdout.write("        –®–∞–≥ 2/7: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤...")
        person_holders = []
        org_holders = []

        with tqdm(total=len(all_holders), desc="           –ê–Ω–∞–ª–∏–∑", unit=" –æ–±") as pbar:
            for holder in all_holders:
                if self.type_detector.detect_type(holder) == 'person':
                    person_holders.append(holder)
                else:
                    org_holders.append(holder)
                pbar.update(1)

        self.stdout.write(f"        –õ—é–¥–∏: {len(person_holders)}, –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: {len(org_holders)}")

        # –®–ê–ì 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–ß–ê–°–¢–Ø–ú–ò)
        self.stdout.write("        –®–∞–≥ 3/7: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π...")
        org_map = {}

        if org_holders:
            CHUNK_SIZE = 1000
            total_orgs = len(org_holders)

            self.stdout.write(f"        –û–±—Ä–∞–±–æ—Ç–∫–∞ {total_orgs} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —á–∞—Å—Ç—è–º–∏ –ø–æ {CHUNK_SIZE}...")

            for chunk_start in range(0, total_orgs, CHUNK_SIZE):
                chunk_end = min(chunk_start + CHUNK_SIZE, total_orgs)
                chunk_holders = org_holders[chunk_start:chunk_end]

                # –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤ —ç—Ç–æ–π —á–∞—Å—Ç–∏
                existing_orgs = {}
                for org in Organization.objects.filter(name__in=chunk_holders):
                    existing_orgs[org.name] = org
                    self.organization_cache[org.name] = org

                # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤ —ç—Ç–æ–π —á–∞—Å—Ç–∏
                orgs_to_create = []
                for holder in chunk_holders:
                    if holder not in existing_orgs and holder not in self.organization_cache:
                        max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
                        new_id = max_id + len(orgs_to_create) + 1

                        base_slug = slugify(holder[:50])
                        if not base_slug:
                            base_slug = 'organization'

                        unique_slug = base_slug
                        counter = 1
                        while Organization.objects.filter(slug=unique_slug).exists() or any(o.slug == unique_slug for o in orgs_to_create):
                            unique_slug = f"{base_slug}-{counter}"
                            counter += 1

                        org = Organization(
                            organization_id=new_id,
                            name=holder,
                            full_name=holder,
                            short_name=holder[:500] if len(holder) > 500 else holder,
                            slug=unique_slug,
                            register_opk=False,
                            strategic=False,
                        )
                        orgs_to_create.append(org)
                        self.organization_cache[holder] = org

                # –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ —Å–æ–∑–¥–∞–µ–º –≤ –ë–î
                if orgs_to_create:
                    batch_size = 500
                    for i in range(0, len(orgs_to_create), batch_size):
                        batch = orgs_to_create[i:i+batch_size]
                        Organization.objects.bulk_create(batch, batch_size=batch_size)

                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                del existing_orgs
                del orgs_to_create

                progress = (chunk_end / total_orgs) * 100
                self.stdout.write(f"           –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%")

            # –§–∏–Ω–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
            for holder in org_holders:
                org_map[holder] = self.organization_cache.get(holder)

        # –®–ê–ì 4: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª—é–¥–µ–π (–ß–ê–°–¢–Ø–ú–ò)
        self.stdout.write("        –®–∞–≥ 4/7: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª—é–¥–µ–π...")
        person_map = {}

        if person_holders:
            CHUNK_SIZE = 500
            total_people = len(person_holders)

            self.stdout.write(f"        –û–±—Ä–∞–±–æ—Ç–∫–∞ {total_people} –ª—é–¥–µ–π —á–∞—Å—Ç—è–º–∏ –ø–æ {CHUNK_SIZE}...")

            for chunk_start in range(0, total_people, CHUNK_SIZE):
                chunk_end = min(chunk_start + CHUNK_SIZE, total_people)
                chunk_holders = person_holders[chunk_start:chunk_end]

                # –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
                existing_people = {}
                for holder in chunk_holders:
                    parts = holder.split()
                    if len(parts) >= 2:
                        last_name = parts[0]
                        first_name = parts[1]
                        middle_name = parts[2] if len(parts) > 2 else ''

                        persons = Person.objects.filter(
                            last_name=last_name,
                            first_name=first_name
                        )
                        if middle_name:
                            persons = persons.filter(middle_name=middle_name)

                        person = persons.first()
                        if person:
                            existing_people[holder] = person
                            self.person_cache[holder] = person

                # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö
                people_to_create = []
                for holder in chunk_holders:
                    if holder not in existing_people and holder not in self.person_cache:
                        parts = holder.split()
                        if len(parts) >= 2:
                            last_name = parts[0]
                            first_name = parts[1]
                            middle_name = parts[2] if len(parts) > 2 else ''

                            name_parts = [last_name, first_name]
                            if middle_name:
                                name_parts.append(middle_name)

                            base_slug = slugify(' '.join(name_parts))
                            if not base_slug:
                                base_slug = 'person'

                            unique_slug = base_slug
                            counter = 1
                            while Person.objects.filter(slug=unique_slug).exists() or any(p.slug == unique_slug for p in people_to_create):
                                unique_slug = f"{base_slug}-{counter}"
                                counter += 1

                            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
                            new_id = max_id + len(people_to_create) + 1

                            person = Person(
                                ceo_id=new_id,
                                ceo=holder,
                                last_name=last_name,
                                first_name=first_name,
                                middle_name=middle_name or '',
                                slug=unique_slug
                            )
                            people_to_create.append(person)
                            self.person_cache[holder] = person

                # –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ —Å–æ–∑–¥–∞–µ–º –≤ –ë–î
                if people_to_create:
                    batch_size = 500
                    for i in range(0, len(people_to_create), batch_size):
                        batch = people_to_create[i:i+batch_size]
                        Person.objects.bulk_create(batch, batch_size=batch_size)

                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                del existing_people
                del people_to_create

                progress = (chunk_end / total_people) * 100
                self.stdout.write(f"           –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%")

            # –§–∏–Ω–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
            for holder in person_holders:
                person_map[holder] = self.person_cache.get(holder)

        # –®–ê–ì 5: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π
        self.stdout.write("        –®–∞–≥ 5/7: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π...")

        org_relations = set()
        person_relations = set()

        with tqdm(total=sum(len(h) for h in holders_cache.values()), desc="           –°–±–æ—Ä —Å–≤—è–∑–µ–π", unit=" —Å–≤") as pbar:
            for reg_num, holders_list in holders_cache.items():
                ip_object = existing_objects.get(reg_num)
                if not ip_object:
                    continue

                for holder in holders_list:
                    if holder in org_map and org_map[holder]:
                        org_relations.add((ip_object.pk, org_map[holder].pk))
                    elif holder in person_map and person_map[holder]:
                        person_relations.add((ip_object.pk, person_map[holder].pk))
                    pbar.update(1)

        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏: {len(org_relations)}")
        self.stdout.write(f"        –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏: {len(person_relations)}")

        # –®–ê–ì 6: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏
        if org_relations:
            self.stdout.write("        –®–∞–≥ 6/7: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏...")

            ip_ids = list(set(ip_id for ip_id, _ in org_relations))

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ –ü–ê–ß–ö–ê–ú–ò
            delete_batch_size = 500
            deleted_total = 0
            for i in range(0, len(ip_ids), delete_batch_size):
                batch_ip_ids = ip_ids[i:i+delete_batch_size]
                deleted, _ = IPObject.owner_organizations.through.objects.filter(
                    ipobject_id__in=batch_ip_ids
                ).delete()
                deleted_total += deleted
                if (i // delete_batch_size) % 10 == 0:
                    self.stdout.write(f"              –£–¥–∞–ª–µ–Ω–æ {deleted_total} —Å–≤—è–∑–µ–π —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏...")

            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π: {deleted_total}")

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏ –ø–∞—á–∫–∞–º–∏
            through_objs = [
                IPObject.owner_organizations.through(
                    ipobject_id=ip_id,
                    organization_id=org_id
                )
                for ip_id, org_id in org_relations
            ]

            create_batch_size = 1000
            created_count = 0
            with tqdm(total=len(through_objs), desc="           –î–æ–±–∞–≤–ª–µ–Ω–∏–µ", unit=" —Å–≤") as pbar:
                for i in range(0, len(through_objs), create_batch_size):
                    batch = through_objs[i:i+create_batch_size]
                    IPObject.owner_organizations.through.objects.bulk_create(batch, batch_size=create_batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))

        # –®–ê–ì 7: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏
        if person_relations:
            self.stdout.write("        –®–∞–≥ 7/7: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏...")

            ip_ids = list(set(ip_id for ip_id, _ in person_relations))

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ –ü–ê–ß–ö–ê–ú–ò
            delete_batch_size = 500
            deleted_total = 0
            for i in range(0, len(ip_ids), delete_batch_size):
                batch_ip_ids = ip_ids[i:i+delete_batch_size]
                deleted, _ = IPObject.owner_persons.through.objects.filter(
                    ipobject_id__in=batch_ip_ids
                ).delete()
                deleted_total += deleted
                if (i // delete_batch_size) % 10 == 0:
                    self.stdout.write(f"              –£–¥–∞–ª–µ–Ω–æ {deleted_total} —Å–≤—è–∑–µ–π —Å –ª—é–¥—å–º–∏...")

            self.stdout.write(f"           –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π: {deleted_total}")

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏ –ø–∞—á–∫–∞–º–∏
            through_objs = [
                IPObject.owner_persons.through(
                    ipobject_id=ip_id,
                    person_id=person_id
                )
                for ip_id, person_id in person_relations
            ]

            create_batch_size = 1000
            created_count = 0
            with tqdm(total=len(through_objs), desc="           –î–æ–±–∞–≤–ª–µ–Ω–∏–µ", unit=" —Å–≤") as pbar:
                for i in range(0, len(through_objs), create_batch_size):
                    batch = through_objs[i:i+create_batch_size]
                    IPObject.owner_persons.through.objects.bulk_create(batch, batch_size=create_batch_size)
                    created_count += len(batch)
                    pbar.update(len(batch))

        self.stdout.write(f"        ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

```


-----

# –§–∞–π–ª: management\parsers\utility_model.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
"""

from intellectual_property.models import IPType
from .base import BaseFIPSParser


class UtilityModelParser(BaseFIPSParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""

    def get_ip_type(self):
        return IPType.objects.filter(slug='utility-model').first()

    def get_required_columns(self):
        return ['registration number', 'utility model name']

    def parse_dataframe(self, df, catalogue):
        self.stdout.write(self.style.SUCCESS("  –ü–∞—Ä—Å–µ—Ä –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"))
        return {'processed': 0, 'created': 0, 'updated': 0, 'skipped': 0, 'errors': 0}

```


-----

# –§–∞–π–ª: management\parsers\__init__.py

```
from .invention import InventionParser
from .utility_model import UtilityModelParser
from .industrial_design import IndustrialDesignParser
from .integrated_circuit import IntegratedCircuitTopologyParser
from .computer_program import ComputerProgramParser
from .database import DatabaseParser

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –∏–∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –º–µ—Å—Ç–∞
from .processors import (
    RussianTextProcessor,
    OrganizationNormalizer,
    PersonNameFormatter,
    RIDNameFormatter,
    EntityTypeDetector
)

__all__ = [
    'InventionParser',
    'UtilityModelParser',
    'IndustrialDesignParser',
    'IntegratedCircuitTopologyParser',
    'ComputerProgramParser',
    'DatabaseParser',
    'RussianTextProcessor',
    'OrganizationNormalizer',
    'PersonNameFormatter',
    'RIDNameFormatter',
    'EntityTypeDetector',
]

```


-----

# –§–∞–π–ª: management\parsers\processors\entity_detector.py

```
"""
–î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π
"""

from .text_processor import RussianTextProcessor


class EntityTypeDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def detect_type(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏"""
        if self.processor.is_person(text):
            return 'person'
        return 'organization'

```


-----

# –§–∞–π–ª: management\parsers\processors\organization.py

```
"""
–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)
"""

import re
from typing import Dict, Any

import pandas as pd

from core.models import OrganizationNormalizationRule
from .text_processor import RussianTextProcessor


class OrganizationNormalizer:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)"""

    def __init__(self):
        self.rules_cache = None
        self.processor = RussianTextProcessor()
        self.load_rules()

    def load_rules(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∏–∑ –ë–î"""
        try:
            rules = OrganizationNormalizationRule.objects.all().order_by('priority')
            self.rules_cache = [
                {
                    'original': rule.original_text.lower(),
                    'replacement': rule.replacement_text.lower(),
                    'type': rule.rule_type,
                    'priority': rule.priority
                }
                for rule in rules
            ]
        except Exception as e:
            self.rules_cache = []
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏, –Ω–æ –Ω–µ –ø–∞–¥–∞–µ–º

    def normalize_for_search(self, name: str) -> Dict[str, Any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –¢–û–õ–¨–ö–û –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        –°–∞–º–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –≤ CSV
        """
        if pd.isna(name) or not name:
            return {'normalized': '', 'keywords': [], 'original': name}

        original = str(name).strip()
        name_lower = original.lower()

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ –ë–î –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        normalized = name_lower
        if self.rules_cache:
            for rule in self.rules_cache:
                try:
                    if rule['type'] == 'ignore':
                        pattern = r'\b' + re.escape(rule['original']) + r'\b'
                        normalized = re.sub(pattern, '', normalized)
                    else:
                        pattern = r'\b' + re.escape(rule['original']) + r'\b'
                        normalized = re.sub(pattern, rule['replacement'], normalized)
                except Exception:
                    continue

        # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞
        normalized = re.sub(r'["\'¬´¬ª‚Äû‚Äú‚Äù]', '', normalized)
        normalized = re.sub(r'[^\w\s-]', ' ', normalized)
        normalized = ' '.join(normalized.split())

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        keywords = []

        # –°–ª–æ–≤–∞ –≤ –∫–∞–≤—ã—á–∫–∞—Ö
        quoted = re.findall(r'"([^"]+)"', original)
        for q in quoted:
            words = q.lower().split()
            keywords.extend([w for w in words if len(w) > 3])

        # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        abbrs = re.findall(r'\b[–ê-–Ø–ÅA-Z]{2,}\b', original)
        keywords.extend([a.lower() for a in abbrs if len(a) >= 2])

        # –ö–æ–¥—ã (–ò–ù–ù, –û–ì–†–ù –∏ —Ç.–¥.)
        codes = re.findall(r'\b\d{10,}\b', original)
        keywords.extend(codes)

        return {
            'normalized': normalized,
            'keywords': list(set(keywords)),
            'original': original,
        }

    def format_organization_name(self, name: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        return name

```


-----

# –§–∞–π–ª: management\parsers\processors\person.py

```
"""
–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω –ª—é–¥–µ–π
"""

from .text_processor import RussianTextProcessor


class PersonNameFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω –ª—é–¥–µ–π"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def format(self, name: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –§–ò–û"""
        return self.processor.format_person_name(name)

```


-----

# –§–∞–π–ª: management\parsers\processors\rid.py

```
"""
–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –†–ò–î
"""

from .text_processor import RussianTextProcessor


class RIDNameFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –†–ò–î"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def format(self, text: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –†–ò–î"""
        if not text or not isinstance(text, str):
            return text

        if len(text.strip()) <= 1:
            return text

        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ –¥–µ–ª–∞–µ–º –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –∑–∞–≥–ª–∞–≤–Ω–æ–π
        words = text.lower().split()
        if words:
            words[0] = words[0][0].upper() + words[0][1:]
        return ' '.join(words)

```


-----

# –§–∞–π–ª: management\parsers\processors\text_processor.py

```
"""
–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º natasha
"""

from natasha import (
    Segmenter,
    MorphVocab,
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    NewsNERTagger,
    Doc,
    NamesExtractor
)


class RussianTextProcessor:
    """
    –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º natasha
    """

    # –°–ø–∏—Å–æ–∫ —Ä–∏–º—Å–∫–∏—Ö —Ü–∏—Ñ—Ä
    ROMAN_NUMERALS = {
        'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X',
        'XI', 'XII', 'XIII', 'XIV', 'XV', 'XVI', 'XVII', 'XVIII', 'XIX', 'XX',
        'XXI', 'XXII', 'XXIII', 'XXIV', 'XXV', 'XXX', 'XL', 'L', 'LX', 'XC',
        'C', 'CD', 'D', 'DC', 'CM', 'M'
    }

    # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
    ORG_ABBR = {
        '–û–û–û', '–ó–ê–û', '–û–ê–û', '–ê–û', '–ü–ê–û', '–ù–ê–û',
        '–§–ì–£–ü', '–§–ì–ë–£', '–§–ì–ê–û–£', '–§–ì–ê–£', '–§–ì–ö–£',
        '–ù–ò–ò', '–ö–ë', '–û–ö–ë', '–°–ö–ë', '–¶–ö–ë', '–ü–ö–ë',
        '–ù–ü–û', '–ù–ü–ü', '–ù–ü–§', '–ù–ü–¶', '–ù–ò–¶',
        '–ú–£–ü', '–ì–£–ü', '–ò–ß–ü', '–¢–û–û', '–ê–û–ó–¢', '–ê–û–û–¢',
        '–†–§', '–†–ê–ù', '–°–û –†–ê–ù', '–£—Ä–û –†–ê–ù', '–î–í–û –†–ê–ù',
        '–ú–ì–£', '–°–ü–±–ì–£', '–ú–§–¢–ò', '–ú–ò–§–ò', '–ú–ì–¢–£', '–ú–ê–ò',
        '–õ–¢–î', '–ò–ù–ö', '–ö–û', '–ì–ú–ë–•', '–ê–ì', '–°–ê', '–ù–í', '–ë–í', '–°–ï',
        '–ö–æ', 'Ltd', 'Inc', 'GmbH', 'AG', 'SA', 'NV', 'BV', 'SE',
    }

    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ natasha
        self.segmenter = Segmenter()
        self.morph_vocab = MorphVocab()
        self.emb = NewsEmbedding()
        self.morph_tagger = NewsMorphTagger(self.emb)
        self.syntax_parser = NewsSyntaxParser(self.emb)
        self.ner_tagger = NewsNERTagger(self.emb)
        self.names_extractor = NamesExtractor(self.morph_vocab)

        # –ö—ç—à–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.doc_cache = {}
        self.morph_cache = {}

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã –≤ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        self.ORG_ABBR.update(self.ROMAN_NUMERALS)

    def get_doc(self, text: str) -> Doc:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not text:
            return None

        if text in self.doc_cache:
            return self.doc_cache[text]

        doc = Doc(text)
        doc.segment(self.segmenter)
        doc.tag_morph(self.morph_tagger)
        doc.parse_syntax(self.syntax_parser)
        doc.tag_ner(self.ner_tagger)

        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
        for token in doc.tokens:
            token.lemmatize(self.morph_vocab)

        for span in doc.spans:
            span.normalize(self.morph_vocab)

        self.doc_cache[text] = doc
        return doc

    def is_roman_numeral(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∏–º—Å–∫—É—é —Ü–∏—Ñ—Ä—É"""
        if not text:
            return False
        clean_text = text.strip('.,;:!?()').upper()
        return clean_text in self.ROMAN_NUMERALS

    def is_abbr(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        if not text:
            return False
        clean_text = text.strip('.,;:!?()').upper()
        return clean_text in self.ORG_ABBR

    def is_person(self, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –§–ò–û —á–µ–ª–æ–≤–µ–∫–∞"""
        if not text or len(text) < 6:
            return False

        # –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        if any(ind in text for ind in self.ORG_ABBR if len(ind) > 2):
            return False

        org_indicators = ['–û–±—â–µ—Å—Ç–≤–æ', '–ö–æ–º–ø–∞–Ω–∏—è', '–ö–æ—Ä–ø–æ—Ä–∞—Ü–∏—è', '–ó–∞–≤–æ–¥',
                         '–ò–Ω—Å—Ç–∏—Ç—É—Ç', '–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '–ê–∫–∞–¥–µ–º–∏—è', '–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è',
                         '–§–∏—Ä–º–∞', '–¶–µ–Ω—Ç—Ä']

        if any(ind.lower() in text.lower() for ind in org_indicators):
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ NER
        doc = self.get_doc(text)
        if doc and doc.spans:
            for span in doc.spans:
                if span.type == 'PER':
                    return True

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –§–ò–û
        words = text.split()
        if 2 <= len(words) <= 4:
            name_like = 0
            for word in words:
                clean = word.rstrip('.,')
                if clean and clean[0].isupper() and len(clean) > 1:
                    name_like += 1
            return name_like >= len(words) - 1

        return False

    def extract_person_parts(self, text: str) -> dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π –§–ò–û —Å –ø–æ–º–æ—â—å—é natasha"""
        matches = list(self.names_extractor(text))
        if matches:
            fact = matches[0].fact
            parts = []
            if fact.last:
                parts.append(fact.last)
            if fact.first:
                parts.append(fact.first)
            if fact.middle:
                parts.append(fact.middle)

            return {
                'last': fact.last or '',
                'first': fact.first or '',
                'middle': fact.middle or '',
                'full': ' '.join(parts)
            }

        # Fallback: —Ä—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥
        return self._parse_name_manually(text)

    def _parse_name_manually(self, text: str) -> dict:
        """–†—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –∏–º–µ–Ω–∏"""
        words = text.split()

        if len(words) == 3:
            return {
                'last': words[0],
                'first': words[1],
                'middle': words[2],
                'full': text
            }
        elif len(words) == 2:
            return {
                'last': words[0],
                'first': words[1],
                'middle': '',
                'full': text
            }
        else:
            return {
                'last': text,
                'first': '',
                'middle': '',
                'full': text
            }

    def format_person_name(self, name: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –§–ò–û —á–µ–ª–æ–≤–µ–∫–∞"""
        if not name:
            return name

        parts = self.extract_person_parts(name)
        if parts.get('full'):
            return parts['full']

        return name

```


-----

# –§–∞–π–ª: management\parsers\processors\__init__.py

```
from .text_processor import RussianTextProcessor
from .organization import OrganizationNormalizer
from .person import PersonNameFormatter
from .rid import RIDNameFormatter
from .entity_detector import EntityTypeDetector

__all__ = [
    'RussianTextProcessor',
    'OrganizationNormalizer',
    'PersonNameFormatter',
    'RIDNameFormatter',
    'EntityTypeDetector',
]

```


-----

# –§–∞–π–ª: management\utils\csv_loader.py

```
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ CSV —Ñ–∞–π–ª–æ–≤
"""

import pandas as pd


def load_csv_with_strategies(file_path, encoding, delimiter, stdout=None):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏
    """
    strategies = [
        {'encoding': encoding, 'delimiter': delimiter, 'skipinitialspace': True},
        {'encoding': 'cp1251', 'delimiter': delimiter, 'skipinitialspace': True},
        {'encoding': 'utf-8', 'delimiter': ';', 'skipinitialspace': True},
        {'encoding': 'cp1251', 'delimiter': ';', 'skipinitialspace': True},
        {'encoding': 'utf-8', 'delimiter': '\t', 'skipinitialspace': True},
    ]

    for strategy in strategies:
        try:
            df = pd.read_csv(file_path, **strategy, dtype=str, keep_default_na=False)
            if stdout:
                stdout.write(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {strategy}")

            df.columns = [col.strip().strip('\ufeff').strip('"') for col in df.columns]
            return df
        except Exception:
            continue

    raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π")

```


-----

# –§–∞–π–ª: management\utils\filters.py

```
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ DataFrame
"""

from datetime import datetime
import pandas as pd


def filter_by_registration_year(df, min_year, stdout=None):
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è DataFrame –ø–æ –≥–æ–¥—É —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
    """
    def extract_year(date_str):
        try:
            if pd.isna(date_str) or not date_str:
                return None

            date_str = str(date_str).strip()
            if not date_str:
                return None

            for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
                try:
                    return datetime.strptime(date_str, fmt).year
                except (ValueError, TypeError):
                    continue

            try:
                return pd.to_datetime(date_str).year
            except (ValueError, TypeError):
                return None
        except:
            return None

    if stdout:
        stdout.write("  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≥–æ–¥—É —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏...")

    if 'registration date' not in df.columns:
        if stdout:
            stdout.write("  ‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'registration date' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –≥–æ–¥—É")
        return df

    df['_year'] = df['registration date'].apply(extract_year)

    if stdout:
        # –§–∏–ª—å—Ç—Ä—É–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        valid_years = df['_year'].dropna()
        if not valid_years.empty:
            years_dist = valid_years.value_counts().sort_index()
            years_list = list(years_dist.items())
            if len(years_list) > 0:
                stdout.write(f"     –î–∏–∞–ø–∞–∑–æ–Ω –≥–æ–¥–æ–≤: {years_list[0][0]:.0f} - {years_list[-1][0]:.0f}")
                stdout.write(f"     –ü–µ—Ä–≤—ã–µ 5: {years_list[:5]}")
                stdout.write(f"     –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5: {years_list[-5:]}")

    filtered_df = df[df['_year'] >= min_year].copy() if '_year' in df.columns else df.copy()
    if '_year' in filtered_df.columns:
        filtered_df.drop('_year', axis=1, inplace=True)

    return filtered_df


def filter_by_actual(df, stdout=None):
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è DataFrame –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (actual = True)
    """
    def parse_actual(value):
        if pd.isna(value) or not value:
            return False
        value = str(value).lower().strip()
        return value in ['1', 'true', 'yes', '–¥–∞', '–¥–µ–π—Å—Ç–≤—É–µ—Ç', 't', '1.0', '–∞–∫—Ç–∏–≤–µ–Ω']

    if 'actual' not in df.columns:
        if stdout:
            stdout.write("  ‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'actual' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏")
        return df

    df['_actual'] = df['actual'].apply(parse_actual)
    filtered_df = df[df['_actual'] == True].copy()
    filtered_df.drop('_actual', axis=1, inplace=True)

    return filtered_df


def apply_filters(df, min_year, only_active, stdout=None):
    """
    –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∫ DataFrame
    """
    original_count = len(df)

    if min_year is not None:
        df = filter_by_registration_year(df, min_year, stdout)

    if only_active:
        df = filter_by_actual(df, stdout)

    filtered_count = len(df)
    if stdout and filtered_count < original_count:
        stdout.write(f"  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {original_count} ‚Üí {filtered_count} –∑–∞–ø–∏—Å–µ–π")

    return df

```


-----

# –§–∞–π–ª: management\utils\__init__.py

```

```
