# –§–∞–π–ª: management\help.txt

```
python manage.py pars_fips_catalogue \
  --catalogue-id 42 \
  --ip-type invention \
  --dry-run \
  --encoding utf-8 \
  --delimiter , \
  --batch-size 1000 \
  --min-year 2020 \
  --max-year 2023 \
  --skip-filters \
  --only-active \
  --max-rows 10000 \
  --force \
  --mark-processed \
  --process-by-year \
  --year-step 1 \
  --start-year 2021

  # –¢–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è —Å 2020 –≥–æ–¥–∞, –ø–æ –≥–æ–¥–∞–º
python manage.py pars_fips_catalogue --ip-type invention --min-year 2020 --only-active --process-by-year

# –¢–µ—Å—Ç –¥–ª—è 2023 –≥–æ–¥–∞ (10 –∑–∞–ø–∏—Å–µ–π)
python manage.py pars_fips_catalogue --ip-type computer-program --min-year 2023 --max-year 2023 --max-rows 10 --process-by-year

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ–ø–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞
python manage.py pars_fips_catalogue --catalogue-id 42 --force

# –í—Å–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –≠–í–ú –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤
python manage.py pars_fips_catalogue --ip-type computer-program --skip-filters

# ============================================================================
# –ü–ê–†–°–ï–† –ö–ê–¢–ê–õ–û–ì–û–í –û–¢–ö–†–´–¢–´–• –î–ê–ù–ù–´–• –§–ò–ü–° –†–û–°–ü–ê–¢–ï–ù–¢–ê
# ============================================================================

# ----------------------------------------------------------------------------
# –û–°–ù–û–í–ù–´–ï –†–ï–ñ–ò–ú–´ –†–ê–ë–û–¢–´
# ----------------------------------------------------------------------------

# –†–µ–∂–∏–º ONLY-ACTIVE: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (actual = True)
# –†–µ–∂–∏–º MIN-YEAR 2020: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π 2020 –≥–æ–¥–∞ –∏ –ø–æ–∑–∂–µ
# –†–µ–∂–∏–º MAX-YEAR 2023: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π –¥–æ 2023 –≥–æ–¥–∞ –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ
# –†–µ–∂–∏–º PROCESS-BY-YEAR: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –≥–æ–¥–∞–º (—É–º–µ–Ω—å—à–∞–µ—Ç –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ –ë–î)
# –†–µ–∂–∏–º DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î
# –†–µ–∂–∏–º FORCE: –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

# ----------------------------------------------------------------------------
# –¢–ò–ü–´ –†–ò–î –î–õ–Ø –ü–ê–†–°–ò–ù–ì–ê
# ----------------------------------------------------------------------------

# --ip-type invention                      # –ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è
# --ip-type utility-model                  # –ü–æ–ª–µ–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏
# --ip-type industrial-design              # –ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã
# --ip-type integrated-circuit-topology    # –¢–æ–ø–æ–ª–æ–≥–∏–∏ –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º
# --ip-type computer-program               # –ü—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –≠–í–ú
# --ip-type database                       # –ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

# ----------------------------------------------------------------------------
# –ü–†–ò–ú–ï–†–´ –ó–ê–ü–£–°–ö–ê
# ----------------------------------------------------------------------------

# 1. –¢–ï–°–¢–û–í–´–ô –ó–ê–ü–£–°–ö (–º–∞–ª–æ –∑–∞–ø–∏—Å–µ–π)
# ----------------------------------------------------------------------------

# –¢–µ—Å—Ç –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π (10 –∑–∞–ø–∏—Å–µ–π)
python manage.py pars_fips_catalogue --ip-type invention --max-rows 10

# –¢–µ—Å—Ç –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π 2026 –≥–æ–¥–∞ (10 –∑–∞–ø–∏—Å–µ–π)
python manage.py pars_fips_catalogue --ip-type invention --min-year 2026 --max-rows 10

# –¢–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú (–∞–∫—Ç–∏–≤–Ω—ã–µ, 2023 –≥–æ–¥, 100 –∑–∞–ø–∏—Å–µ–π)
python manage.py pars_fips_catalogue --ip-type computer-program --min-year 2023 --max-year 2023 --only-active --max-rows 100

# –¢–µ—Å—Ç –≤ —Ä–µ–∂–∏–º–µ DRY-RUN (–ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)
python manage.py pars_fips_catalogue --ip-type database --min-year 2024 --dry-run --max-rows 50

# ----------------------------------------------------------------------------
# 2. –û–ë–†–ê–ë–û–¢–ö–ê –ü–û –ì–û–î–ê–ú (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø –î–õ–Ø –ë–û–õ–¨–®–ò–• –û–ë–™–ï–ú–û–í)
# ----------------------------------------------------------------------------

# –í—Å–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è —Å 2020 –≥–æ–¥–∞, —Ä–∞–∑–±–∏–≤–∞—è –ø–æ –≥–æ–¥–∞–º
python manage.py pars_fips_catalogue --ip-type invention --min-year 2020 --process-by-year

# –í—Å–µ –ø–æ–ª–µ–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å 2018 –ø–æ 2022 –≥–æ–¥ —Å —à–∞–≥–æ–º 2 –≥–æ–¥–∞
python manage.py pars_fips_catalogue --ip-type utility-model --min-year 2018 --max-year 2022 --process-by-year --year-step 2

# –ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú —Å 2021 –≥–æ–¥–∞ (–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ä–∞–Ω–Ω–∏–µ)
python manage.py pars_fips_catalogue --ip-type computer-program --min-year 2020 --process-by-year --start-year 2021

# –¢–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã 2023 –≥–æ–¥–∞
python manage.py pars_fips_catalogue --ip-type industrial-design --min-year 2023 --max-year 2023 --only-active --process-by-year

# ----------------------------------------------------------------------------
# 3. –ü–û–õ–ù–´–ô –ü–ê–†–°–ò–ù–ì (–í–°–ï –ó–ê–ü–ò–°–ò)
# ----------------------------------------------------------------------------

# –í—Å–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è (–±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤)
python manage.py pars_fips_catalogue --ip-type invention --skip-filters

# –í—Å–µ –ø–æ–ª–µ–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤)
python manage.py pars_fips_catalogue --ip-type utility-model --skip-filters

# –í—Å–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –≠–í–ú (–±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤)
python manage.py pars_fips_catalogue --ip-type computer-program --skip-filters

# ----------------------------------------------------------------------------
# 4. –ü–ê–†–°–ò–ù–ì –ü–û –ö–ê–¢–ê–õ–û–ì–ê–ú
# ----------------------------------------------------------------------------

# –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ –ø–æ ID
python manage.py pars_fips_catalogue --catalogue-id 42

# –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–º –ø–µ—Ä–µ–ø–∞—Ä—Å–∏–Ω–≥–æ–º
python manage.py pars_fips_catalogue --catalogue-id 42 --force

# –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ —Å –ø–æ–º–µ—Ç–∫–æ–π –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (–¥–∞–∂–µ —Å –æ—à–∏–±–∫–∞–º–∏)
python manage.py pars_fips_catalogue --catalogue-id 42 --mark-processed

# ----------------------------------------------------------------------------
# 5. –ü–ê–†–°–ò–ù–ì –° –§–ò–õ–¨–¢–†–ê–¶–ò–ï–ô –ü–û –ì–û–î–ê–ú
# ----------------------------------------------------------------------------

# –í—Å–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è —Å 2020 –≥–æ–¥–∞
python manage.py pars_fips_catalogue --ip-type invention --min-year 2020

# –í—Å–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è —Å 2015 –ø–æ 2020 –≥–æ–¥
python manage.py pars_fips_catalogue --ip-type invention --min-year 2015 --max-year 2020

# –í—Å–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è —Å 2020 –≥–æ–¥–∞ (—Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ)
python manage.py pars_fips_catalogue --ip-type invention --min-year 2020 --only-active

# –í—Å–µ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è —Å 2020 –≥–æ–¥–∞ (—Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ, –ø–æ –≥–æ–¥–∞–º)
python manage.py pars_fips_catalogue --ip-type invention --min-year 2020 --only-active --process-by-year

# ----------------------------------------------------------------------------
# 6. –ü–†–û–î–í–ò–ù–£–¢–´–ï –°–¶–ï–ù–ê–†–ò–ò
# ----------------------------------------------------------------------------

# –ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ–ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å 2020 –≥–æ–¥–∞ (–∏–≥–Ω–æ—Ä–∏—Ä—É—è –¥–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏)
python manage.py pars_fips_catalogue --ip-type invention --min-year 2020 --force --process-by-year

# –ü–∞—Ä—Å–∏–Ω–≥ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º
python manage.py pars_fips_catalogue --ip-type database --encoding cp1251 --delimiter ';' --min-year 2020

# –ü–∞—Ä—Å–∏–Ω–≥ —Å –±–æ–ª—å—à–∏–º–∏ –ø–∞—á–∫–∞–º–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
python manage.py pars_fips_catalogue --ip-type invention --batch-size 5000 --min-year 2020 --process-by-year

# –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –†–ò–î —Å 2020 –≥–æ–¥–∞ (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)
python manage.py pars_fips_catalogue --ip-type invention --min-year 2020 --process-by-year
python manage.py pars_fips_catalogue --ip-type utility-model --min-year 2020 --process-by-year
python manage.py pars_fips_catalogue --ip-type industrial-design --min-year 2020 --process-by-year
python manage.py pars_fips_catalogue --ip-type integrated-circuit-topology --min-year 2020 --process-by-year
python manage.py pars_fips_catalogue --ip-type computer-program --min-year 2020 --process-by-year
python manage.py pars_fips_catalogue --ip-type database --min-year 2020 --process-by-year

# ----------------------------------------------------------------------------
# –ü–û–õ–ù–û–ï –û–ü–ò–°–ê–ù–ò–ï –ü–ê–†–ê–ú–ï–¢–†–û–í
# ----------------------------------------------------------------------------

usage: manage.py pars_fips_catalogue [-h] [--catalogue-id CATALOGUE_ID] 
                                     [--ip-type {invention,utility-model,industrial-design,integrated-circuit-topology,computer-program,database}] 
                                     [--dry-run] [--encoding ENCODING] [--delimiter DELIMITER] 
                                     [--batch-size BATCH_SIZE] [--min-year MIN_YEAR] [--max-year MAX_YEAR] 
                                     [--skip-filters] [--only-active] [--max-rows MAX_ROWS] [--force] 
                                     [--mark-processed] [--process-by-year] [--year-step YEAR_STEP] 
                                     [--start-year START_YEAR] [--version] [-v {0,1,2,3}] 
                                     [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] 
                                     [--no-color] [--force-color] [--skip-checks]

–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ –≥–æ–¥–∞–º

options:
  -h, --help            show this help message and exit
  
  --catalogue-id CATALOGUE_ID
                        ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
                        
  --ip-type {invention,utility-model,industrial-design,integrated-circuit-topology,computer-program,database}
                        –¢–∏–ø –†–ò–î –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–∞—Ä—Å—è—Ç—Å—è –≤—Å–µ)
                        
  --dry-run             –†–µ–∂–∏–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
  
  --encoding ENCODING   –ö–æ–¥–∏—Ä–æ–≤–∫–∞ CSV —Ñ–∞–π–ª–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: utf-8)
  
  --delimiter DELIMITER
                        –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV —Ñ–∞–π–ª–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: ,)
                        
  --batch-size BATCH_SIZE
                        –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è bulk-–æ–ø–µ—Ä–∞—Ü–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 100)
                        
  --min-year MIN_YEAR   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2000)
  
  --max-year MAX_YEAR   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
  
  --skip-filters        –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏)
  
  --only-active         –ü–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç—ã (actual = True)
  
  --max-rows MAX_ROWS   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
  
  --force               –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–∂–µ –µ—Å–ª–∏ –∫–∞—Ç–∞–ª–æ–≥ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
  
  --mark-processed      –ü–æ–º–µ—Ç–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (–¥–∞–∂–µ –µ—Å–ª–∏ –±—ã–ª–∏ –æ—à–∏–±–∫–∏)
  
  --process-by-year     –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –≥–æ–¥–∞–º (—É–º–µ–Ω—å—à–∞–µ—Ç –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ –ë–î)
  
  --year-step YEAR_STEP
                        –®–∞–≥ –ø–æ –≥–æ–¥–∞–º –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1)
                        
  --start-year START_YEAR
                        –ù–∞—á–∞–ª—å–Ω—ã–π –≥–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –Ω–∞—á–∞—Ç—å –Ω–µ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ)
  
  --version             Show program's version number and exit.
  
  -v, --verbosity {0,1,2,3}
                        Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 
                        3=very verbose output
                        
  --settings SETTINGS   The Python path to a settings module, e.g. "myproject.settings.main". 
                        If this isn't provided, the DJANGO_SETTINGS_MODULE environment variable 
                        will be used.
                        
  --pythonpath PYTHONPATH
                        A directory to add to the Python path, e.g. 
                        "/home/djangoprojects/myproject".
                        
  --traceback           Display a full stack trace on CommandError exceptions.
  
  --no-color            Don't colorize the command output.
  
  --force-color         Force colorization of the command output.
  
  --skip-checks         Skip system checks.

# ----------------------------------------------------------------------------
# –ü–†–ò–ú–ï–†–´ –í–´–í–û–î–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ò
# ----------------------------------------------------------------------------

# –ü—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ –≥–æ–¥–∞–º –≤—ã —É–≤–∏–¥–∏—Ç–µ:
# ============================================================
# üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞: –ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è 2024
#    ID: 42, –¢–∏–ø: –ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ
# ============================================================
#   
#   üìÖ –ù–∞–π–¥–µ–Ω—ã –≥–æ–¥—ã –≤ –∫–∞—Ç–∞–ª–æ–≥–µ: 2020 - 2024 (–≤—Å–µ–≥–æ 5 –ª–µ—Ç)
#   
#   üìÖ –ì–æ–¥ 2024 (1/5)
#   üîπ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π –¥–ª—è 2024 –≥–æ–¥–∞
#   üîπ –ß—Ç–µ–Ω–∏–µ CSV –∏ —Å–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
#   üîπ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: 1250
#   ...
#   ‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π –¥–ª—è 2024 –≥–æ–¥–∞ –∑–∞–≤–µ—Ä—à–µ–Ω
#      –°–æ–∑–¥–∞–Ω–æ: 120, –û–±–Ω–æ–≤–ª–µ–Ω–æ: 30, –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: 1100
#   
#   üìÖ –ì–æ–¥ 2023 (2/5)
#   ...

# –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
# ============================================================
# üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê
# ============================================================
# üìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞—Ç–∞–ª–æ–≥–æ–≤: 1
# üìù –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 6250
# ‚úÖ –°–æ–∑–¥–∞–Ω–æ: 600
# üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ: 150
# ‚è∏Ô∏è  –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: 5500
# ‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ –≤—Å–µ–≥–æ: 0
#    ‚îî‚îÄ –ø–æ –¥–∞—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: 0
# ‚úÖ –û—à–∏–±–æ–∫: 0
# ============================================================

# ----------------------------------------------------------------------------
# –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ
# ----------------------------------------------------------------------------

# –î–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö (–±–æ–ª–µ–µ 100 000 –∑–∞–ø–∏—Å–µ–π):
#   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --process-by-year –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –ø–æ –≥–æ–¥–∞–º
#   - –£–≤–µ–ª–∏—á—å—Ç–µ --batch-size –¥–æ 1000-5000 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
#   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --only-active –µ—Å–ª–∏ –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –¥–µ–π—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ç–µ–Ω—Ç—ã
#   - –ü—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å –ø–∞–º—è—Ç—å—é —É–º–µ–Ω—å—à–∏—Ç–µ --batch-size

# –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:
#   - –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --dry-run –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
#   - –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–π—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π —á–µ—Ä–µ–∑ --max-rows
#   - –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –æ–¥–∏–Ω –≥–æ–¥ —á–µ—Ä–µ–∑ --min-year –∏ --max-year

# –î–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞:
#   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --force –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
#   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --mark-processed –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø–æ–º–µ—Ç–∏—Ç—å –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞–∂–µ —Å –æ—à–∏–±–∫–∞–º–∏

# –î–ª—è –æ—Ç–ª–∞–¥–∫–∏:
#   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ -v 2 –∏–ª–∏ -v 3 –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
#   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --traceback –¥–ª—è –ø–æ–ª–Ω–æ–π —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ –æ—à–∏–±–æ–∫
```


-----

# –§–∞–π–ª: management\__init__.py

```

```


-----

# –§–∞–π–ª: management\commands\pars_fips_catalogue.py

```
"""
–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞.
–û–±–µ—Ä—Ç–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–ª–µ–≥–∏—Ä—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –ø–∞—Ä—Å–µ—Ä–∞–º.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ –≥–æ–¥–∞–º –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –ë–î.
"""

import logging
import os
import gc
from datetime import datetime

from django.core.management.base import BaseCommand, CommandError
from django.utils import timezone
import pandas as pd

from intellectual_property.models import FipsOpenDataCatalogue

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä—ã –∏–∑ –ø–∞–∫–µ—Ç–∞ parsers
from ..parsers import (
    InventionParser, UtilityModelParser, IndustrialDesignParser,
    IntegratedCircuitTopologyParser, ComputerProgramParser, DatabaseParser
)
from ..utils.csv_loader import load_csv_with_strategies
from ..utils.filters import apply_filters, filter_by_actual

logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = '–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –§–ò–ü–° –†–æ—Å–ø–∞—Ç–µ–Ω—Ç–∞'

    def add_arguments(self, parser):
        parser.add_argument('--catalogue-id', type=int, help='ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')
        parser.add_argument('--ip-type', type=str,
                        choices=['invention', 'utility-model', 'industrial-design',
                                'integrated-circuit-topology', 'computer-program', 'database'],
                        help='–¢–∏–ø –†–ò–î –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–∞—Ä—Å—è—Ç—Å—è –≤—Å–µ)')
        parser.add_argument('--dry-run', action='store_true', help='–†–µ–∂–∏–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î')
        parser.add_argument('--encoding', type=str, default='utf-8', help='–ö–æ–¥–∏—Ä–æ–≤–∫–∞ CSV —Ñ–∞–π–ª–∞')
        parser.add_argument('--delimiter', type=str, default=',', help='–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV —Ñ–∞–π–ª–µ')
        parser.add_argument('--batch-size', type=int, default=100, help='–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è bulk-–æ–ø–µ—Ä–∞—Ü–∏–π')
        parser.add_argument('--min-year', type=int, default=2000, help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏')
        parser.add_argument('--max-year', type=int, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏')
        parser.add_argument('--skip-filters', action='store_true', help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏)')
        parser.add_argument('--only-active', action='store_true', help='–ü–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç—ã (actual = True)')
        parser.add_argument('--max-rows', type=int, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)')
        parser.add_argument('--force', action='store_true', help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–∂–µ –µ—Å–ª–∏ –∫–∞—Ç–∞–ª–æ–≥ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω')
        parser.add_argument('--mark-processed', action='store_true',
                        help='–ü–æ–º–µ—Ç–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (–¥–∞–∂–µ –µ—Å–ª–∏ –±—ã–ª–∏ –æ—à–∏–±–∫–∏)')
        parser.add_argument('--process-by-year', action='store_true',
                        help='–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –≥–æ–¥–∞–º (—É–º–µ–Ω—å—à–∞–µ—Ç –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ –ë–î)')
        parser.add_argument('--year-step', type=int, default=1,
                        help='–®–∞–≥ –ø–æ –≥–æ–¥–∞–º –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1)')
        parser.add_argument('--start-year', type=int,
                        help='–ù–∞—á–∞–ª—å–Ω—ã–π –≥–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –Ω–∞—á–∞—Ç—å –Ω–µ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ)')

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.parsers = {
            'invention': InventionParser(self),
            'utility-model': UtilityModelParser(self),
            'industrial-design': IndustrialDesignParser(self),
            'integrated-circuit-topology': IntegratedCircuitTopologyParser(self),
            'computer-program': ComputerProgramParser(self),
            'database': DatabaseParser(self),
        }

    def handle(self, *args, **options):
        self.dry_run = options['dry_run']
        self.encoding = options['encoding']
        self.delimiter = options['delimiter']
        self.batch_size = options['batch_size']
        self.min_year = options['min_year']
        self.max_year = options.get('max_year')
        self.skip_filters = options['skip_filters']
        self.only_active = options['only_active']
        self.max_rows = options.get('max_rows')
        self.force = options.get('force', False)
        self.mark_processed = options.get('mark_processed', False)
        self.process_by_year = options.get('process_by_year', False)
        self.year_step = options.get('year_step', 1)
        self.start_year = options.get('start_year')

        if self.dry_run:
            self.stdout.write(self.style.WARNING("\nüîç –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î\n"))

        if self.only_active:
            self.stdout.write(self.style.WARNING("üìå –†–µ–∂–∏–º: –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (actual = True)"))

        if self.force:
            self.stdout.write(self.style.WARNING("‚ö†Ô∏è  –†–µ–∂–∏–º: –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏)"))

        if self.process_by_year:
            self.stdout.write(self.style.WARNING(
                f"üìÖ –†–µ–∂–∏–º: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –≥–æ–¥–∞–º —Å {self.min_year} –ø–æ {self.max_year or '–≤—Å–µ'} (—à–∞–≥ {self.year_step})"
            ))

        catalogues = self.get_catalogues(options.get('catalogue_id'), options.get('ip_type'))

        if not catalogues:
            raise CommandError('–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–∞—Ç–∞–ª–æ–≥–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')

        total_stats = {
            'catalogues': len(catalogues),
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        for catalogue in catalogues:
            self.stdout.write(self.style.SUCCESS(f"\n{'='*60}"))
            self.stdout.write(self.style.SUCCESS(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞: {catalogue.name}"))
            self.stdout.write(self.style.SUCCESS(f"   ID: {catalogue.id}, –¢–∏–ø: {catalogue.ip_type.name if catalogue.ip_type else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'}"))
            self.stdout.write(self.style.SUCCESS(f"{'='*60}"))

            stats = self.process_catalogue(catalogue)

            for key in ['processed', 'created', 'updated', 'unchanged', 'skipped', 'errors']:
                total_stats[key] += stats.get(key, 0)
            total_stats['skipped_by_date'] += stats.get('skipped_by_date', 0)

        self.print_final_stats(total_stats)

    def get_catalogues(self, catalogue_id=None, ip_type_slug=None):
        queryset = FipsOpenDataCatalogue.objects.all()

        if catalogue_id:
            queryset = queryset.filter(id=catalogue_id)
        elif ip_type_slug:
            queryset = queryset.filter(ip_type__slug=ip_type_slug)
        else:
            queryset = queryset.exclude(catalogue_file='')

        return queryset.order_by('ip_type__id', '-publication_date')

    def extract_year_from_date(self, date_str):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞ –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Å –¥–∞—Ç–æ–π"""
        try:
            if pd.isna(date_str) or not date_str:
                return None
            date_str = str(date_str).strip()
            if not date_str:
                return None
            
            for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
                try:
                    return datetime.strptime(date_str, fmt).year
                except (ValueError, TypeError):
                    continue
            
            try:
                return pd.to_datetime(date_str).year
            except (ValueError, TypeError):
                return None
        except:
            return None

    def get_years_from_catalogue(self, catalogue):
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ –≥–æ–¥–æ–≤, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ CSV —Ñ–∞–π–ª–µ –∫–∞—Ç–∞–ª–æ–≥–∞
        """
        df = self.load_csv(catalogue)
        if df is None or df.empty:
            return []
        
        if 'registration date' not in df.columns:
            self.stdout.write(self.style.WARNING(
                f"  ‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'registration date' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –Ω–µ –º–æ–≥—É –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≥–æ–¥—ã"
            ))
            return []
        
        df['_year'] = df['registration date'].apply(self.extract_year_from_date)
        all_years = sorted(df['_year'].dropna().unique().astype(int).tolist())
        
        if not all_years:
            self.stdout.write(self.style.WARNING("  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –≥–æ–¥—ã –∏–∑ –¥–∞—Ç"))
            return []
        
        # –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥
        self.stdout.write(f"  üìä –í—Å–µ –≥–æ–¥—ã –≤ –∫–∞—Ç–∞–ª–æ–≥–µ: {all_years[0]} - {all_years[-1]} (–≤—Å–µ–≥–æ {len(all_years)} –ª–µ—Ç)")
        
        if len(all_years) > 20:
            self.stdout.write(f"     –ü–µ—Ä–≤—ã–µ 10 –ª–µ—Ç: {all_years[:10]}")
            self.stdout.write(f"     –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –ª–µ—Ç: {all_years[-10:]}")
        else:
            self.stdout.write(f"     –í—Å–µ –≥–æ–¥—ã: {all_years}")
        
        # –ï–°–õ–ò –£–ö–ê–ó–ê–ù --skip-filters - –í–û–ó–í–†–ê–©–ê–ï–ú –í–°–ï –ì–û–î–´ –ë–ï–ó –§–ò–õ–¨–¢–†–ê–¶–ò–ò!
        if self.skip_filters:
            self.stdout.write(f"  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ (--skip-filters), –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –≤—Å–µ –≥–æ–¥—ã")
            return all_years
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –≥–æ–¥—É (–¢–û–õ–¨–ö–û –µ—Å–ª–∏ –Ω–µ skip_filters)
        years = all_years
        if self.min_year is not None:
            years = [y for y in all_years if y >= self.min_year]
            self.stdout.write(f"  üîç –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ min_year={self.min_year}: {years[0] if years else '–Ω–µ—Ç'} - {years[-1] if years else '–Ω–µ—Ç'} (–≤—Å–µ–≥–æ {len(years)} –ª–µ—Ç)")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º—É –≥–æ–¥—É
        if self.max_year is not None:
            years = [y for y in years if y <= self.max_year]
            self.stdout.write(f"  üîç –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ max_year={self.max_year}: {years[0] if years else '–Ω–µ—Ç'} - {years[-1] if years else '–Ω–µ—Ç'} (–≤—Å–µ–≥–æ {len(years)} –ª–µ—Ç)")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –≥–æ–¥, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if self.start_year and self.start_year in years:
            start_idx = years.index(self.start_year)
            years = years[start_idx:]
            self.stdout.write(f"  üîç –ù–∞—á–∏–Ω–∞–µ–º —Å {self.start_year}: {years[0]} - {years[-1]} (–≤—Å–µ–≥–æ {len(years)} –ª–µ—Ç)")
        
        return years

    def process_catalogue(self, catalogue):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–±–∏–≤–∫–∏ –ø–æ –≥–æ–¥–∞–º
        """
        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        if not catalogue.catalogue_file:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –£ –∫–∞—Ç–∞–ª–æ–≥–∞ ID={catalogue.id} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª"))
            stats['errors'] += 1
            return stats

        if not self.force and hasattr(catalogue, 'parsed_date') and catalogue.parsed_date:
            self.stdout.write(self.style.WARNING(
                f"  ‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ —É–∂–µ –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω {catalogue.parsed_date.strftime('%d.%m.%Y %H:%M')}"
            ))
            self.stdout.write(self.style.WARNING(f"     –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --force –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"))
            stats['skipped'] += 1
            return stats

        ip_type_slug = catalogue.ip_type.slug if catalogue.ip_type else None

        if ip_type_slug not in self.parsers:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –ù–µ—Ç –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è —Ç–∏–ø–∞ –†–ò–î: {ip_type_slug}"))
            stats['errors'] += 1
            return stats

        parser = self.parsers[ip_type_slug]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if not self.process_by_year or self.skip_filters or self.min_year is None:
            # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Å—Ä–∞–∑—É
            stats = self._process_catalogue_normal(catalogue, parser, stats)
        else:
            # –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ –≥–æ–¥–∞–º
            stats = self._process_catalogue_by_year(catalogue, parser, stats)
        
        # –ü–æ–º–µ—á–∞–µ–º –∫–∞—Ç–∞–ª–æ–≥ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π
        if not self.dry_run and hasattr(catalogue, 'parsed_date'):
            if stats['errors'] == 0 or self.mark_processed:
                catalogue.parsed_date = timezone.now()
                catalogue.save(update_fields=['parsed_date'])
                self.stdout.write(self.style.SUCCESS(f"  ‚úÖ –ö–∞—Ç–∞–ª–æ–≥ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π"))
            else:
                self.stdout.write(self.style.WARNING(
                    f"  ‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ –Ω–µ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫"
                ))

        return stats

    def _process_catalogue_normal(self, catalogue, parser, stats):
        """–û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –±–µ–∑ —Ä–∞–∑–±–∏–≤–∫–∏ –ø–æ –≥–æ–¥–∞–º"""
        df = self.load_csv(catalogue)
        if df is None or df.empty:
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å"))
            stats['skipped'] += 1
            return stats
        
        self.stdout.write(f"  üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")

        missing_columns = self.check_required_columns(df, parser.get_required_columns())
        if missing_columns:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}"))
            stats['errors'] += 1
            return stats
        
        if not self.skip_filters:
            df = apply_filters(df, self.min_year, self.only_active, self.stdout, self.max_year)
        
        if df.empty:
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"))
            stats['skipped'] += 1
            return stats
        
        self.stdout.write(f"  üìä –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
        if self.max_rows and len(df) > self.max_rows:
            df = df.head(self.max_rows)
            self.stdout.write(self.style.WARNING(f"  ‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {self.max_rows} –∑–∞–ø–∏—Å–µ–π"))
        
        try:
            parser_stats = parser.parse_dataframe(df, catalogue)
            stats.update(parser_stats)
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}"))
            logger.error(f"Error parsing catalogue {catalogue.id}: {e}", exc_info=True)
            stats['errors'] += 1
        
        return stats

    def _process_catalogue_by_year(self, catalogue, parser, stats):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –ø–æ –≥–æ–¥–∞–º"""
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≥–æ–¥–æ–≤ - —Ç–µ–ø–µ—Ä—å —Å —É—á–µ—Ç–æ–º skip_filters!
        years = self.get_years_from_catalogue(catalogue)
        
        if not years:
            self.stdout.write(self.style.WARNING(
                f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≥–æ–¥—ã –≤ –∫–∞—Ç–∞–ª–æ–≥–µ, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–ª–∏–∫–æ–º"
            ))
            return self._process_catalogue_normal(catalogue, parser, stats)
        
        self.stdout.write(self.style.SUCCESS(
            f"\n  üìÖ –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(years)} –ª–µ—Ç: {years[0]} - {years[-1]}"
        ))
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π DataFrame –æ–¥–∏–Ω —Ä–∞–∑
        full_df = self.load_csv(catalogue)
        if full_df is None or full_df.empty:
            stats['skipped'] += 1
            return stats
        
        missing_columns = self.check_required_columns(full_df, parser.get_required_columns())
        if missing_columns:
            self.stdout.write(self.style.ERROR(f"  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}"))
            stats['errors'] += 1
            return stats
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –≥–æ–¥–æ–º
        full_df['_year'] = full_df['registration date'].apply(self.extract_year_from_date)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥–æ–¥—ã —Å –∑–∞–¥–∞–Ω–Ω—ã–º —à–∞–≥–æ–º
        years_to_process = years[::self.year_step]
        
        for year_idx, year in enumerate(years_to_process, 1):
            self.stdout.write(self.style.SUCCESS(
                f"\n  üìÖ –ì–æ–¥ {year} ({year_idx}/{len(years_to_process)})"
            ))
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º DataFrame –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –≥–æ–¥–∞
            year_df = full_df[full_df['_year'] == year].copy()
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (actual) –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if self.only_active and not self.skip_filters:
                year_df = filter_by_actual(year_df, self.stdout)
            
            if year_df.empty:
                self.stdout.write(self.style.WARNING(f"     ‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–æ–¥–∞ {year} –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"))
                continue
            
            if self.max_rows:
                year_df = year_df.head(min(self.max_rows, len(year_df)))
            
            try:
                year_stats = parser.parse_dataframe(year_df, catalogue, year=year)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                stats['processed'] += year_stats.get('processed', 0)
                stats['created'] += year_stats.get('created', 0)
                stats['updated'] += year_stats.get('updated', 0)
                stats['unchanged'] += year_stats.get('unchanged', 0)
                stats['errors'] += year_stats.get('errors', 0)
                
                self.stdout.write(f"     –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–æ–¥–∞ {year}: "
                                f"—Å–æ–∑–¥–∞–Ω–æ={year_stats.get('created', 0)}, "
                                f"–æ–±–Ω–æ–≤–ª–µ–Ω–æ={year_stats.get('updated', 0)}, "
                                f"–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={year_stats.get('unchanged', 0)}")
                
            except Exception as e:
                self.stdout.write(self.style.ERROR(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –≥–æ–¥–∞ {year}: {e}"))
                logger.error(f"Error parsing year {year} for catalogue {catalogue.id}: {e}", exc_info=True)
                stats['errors'] += 1
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –≥–æ–¥–∞
            gc.collect()
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É
        if '_year' in full_df.columns:
            del full_df['_year']
        
        return stats

    def load_csv(self, catalogue):
        file_path = catalogue.catalogue_file.path

        if not os.path.exists(file_path):
            self.stdout.write(self.style.ERROR(f"  ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"))
            return None

        df = load_csv_with_strategies(file_path, self.encoding, self.delimiter, self.stdout)
        return df

    def check_required_columns(self, df, required_columns):
        missing = [col for col in required_columns if col not in df.columns]
        return missing

    def print_final_stats(self, stats):
        self.stdout.write(self.style.SUCCESS(f"\n{'='*60}"))
        self.stdout.write(self.style.SUCCESS("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê"))
        self.stdout.write(self.style.SUCCESS(f"{'='*60}"))
        self.stdout.write(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞—Ç–∞–ª–æ–≥–æ–≤: {stats['catalogues']}")
        self.stdout.write(f"üìù –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}")
        self.stdout.write(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ: {stats['created']}")
        self.stdout.write(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}")
        self.stdout.write(f"‚è∏Ô∏è  –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats.get('unchanged', 0)}")
        self.stdout.write(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ –≤—Å–µ–≥–æ: {stats['skipped']}")
        self.stdout.write(f"   ‚îî‚îÄ –ø–æ –¥–∞—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {stats.get('skipped_by_date', 0)}")

        if stats['errors'] > 0:
            self.stdout.write(self.style.ERROR(f"‚ùå –û—à–∏–±–æ–∫: {stats['errors']}"))
        else:
            self.stdout.write(self.style.SUCCESS(f"‚úÖ –û—à–∏–±–æ–∫: {stats['errors']}"))

        if self.dry_run:
            self.stdout.write(self.style.WARNING("\nüîç –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î"))

        self.stdout.write(self.style.SUCCESS(f"{'='*60}"))
```


-----

# –§–∞–π–ª: management\commands\__init__.py

```

```


-----

# –§–∞–π–ª: management\parsers\base.py

```
"""
–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –§–ò–ü–°
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä year –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ –≥–æ–¥–∞–º
"""

import logging
import re
from datetime import datetime
from typing import Optional, List, Dict, Any, Tuple
import gc

from django.db import models
from django.utils.text import slugify
import pandas as pd

from intellectual_property.models import IPObject, IPType
from core.models import Person, Organization, Country

from .processors import (
    RussianTextProcessor,
    OrganizationNormalizer,
    PersonNameFormatter,
    RIDNameFormatter,
    EntityTypeDetector
)

from ..utils.progress import batch_iterator

logger = logging.getLogger(__name__)


class BaseFIPSParser:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –§–ò–ü–°"""

    def __init__(self, command):
        self.command = command
        self.stdout = command.stdout
        self.style = command.style

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
        self.processor = RussianTextProcessor()
        self.org_normalizer = OrganizationNormalizer()
        self.type_detector = EntityTypeDetector()
        self.person_formatter = PersonNameFormatter()
        self.rid_formatter = RIDNameFormatter()

        # –ö—ç—à–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.country_cache = {}
        self.person_cache = {}
        self.organization_cache = {}
        self.foiv_cache = {}
        self.rf_rep_cache = {}
        self.city_cache = {}
        self.activity_type_cache = {}
        self.ceo_position_cache = {}

    def get_ip_type(self):
        """–î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –¥–æ—á–µ—Ä–Ω–∏—Ö –∫–ª–∞—Å—Å–∞—Ö"""
        raise NotImplementedError

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
        raise NotImplementedError

    def parse_dataframe(self, df, catalogue, year=None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            catalogue: –æ–±—ä–µ–∫—Ç –∫–∞—Ç–∞–ª–æ–≥–∞
            year: –≥–æ–¥ –¥–ª—è —Ç–µ–∫—É—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        raise NotImplementedError

    def clean_string(self, value):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if pd.isna(value) or value is None:
            return ''
        value = str(value).strip()
        if value in ['', 'None', 'null', 'NULL', 'nan']:
            return ''
        return value

    def parse_date(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        if pd.isna(value) or not value:
            return None

        date_str = str(value).strip()
        if not date_str:
            return None

        for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
            try:
                return datetime.strptime(date_str, fmt).date()
            except (ValueError, TypeError):
                continue

        try:
            return pd.to_datetime(date_str).date()
        except (ValueError, TypeError):
            return None

    def parse_bool(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –±—É–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if pd.isna(value) or not value:
            return False
        value = str(value).lower().strip()
        return value in ['1', 'true', 'yes', '–¥–∞', '–¥–µ–π—Å—Ç–≤—É–µ—Ç', 't', '1.0', '–∞–∫—Ç–∏–≤–µ–Ω']

    def get_or_create_country(self, code):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –ø–æ –∫–æ–¥—É"""
        if not code or pd.isna(code):
            return None

        code = str(code).upper().strip()
        if len(code) != 2:
            return None

        if code in self.country_cache:
            return self.country_cache[code]

        try:
            country = Country.objects.filter(code=code).first()
            if country:
                self.country_cache[code] = country
                return country

            country = Country.objects.filter(code_alpha3=code).first()
            if country:
                self.country_cache[code] = country
                return country

            return None

        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω—ã {code}: {e}"))
            return None

    def parse_authors(self, authors_str):
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –∞–≤—Ç–æ—Ä–∞–º–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –∞–≤—Ç–æ—Ä–æ–≤
        """
        if pd.isna(authors_str) or not authors_str:
            return []

        authors_str = str(authors_str)
        authors_list = re.split(r'[\n,]\s*', authors_str)

        result = []
        for author in authors_list:
            author = author.strip()
            if not author or author == '""' or author == 'null':
                continue

            author = author.strip('"')
            author = re.sub(r'\s*\([A-Z]{2}\)', '', author)
            author = self.person_formatter.format(author)

            parts = author.split()

            if len(parts) >= 2:
                last_name = parts[0]
                first_name = parts[1] if len(parts) > 1 else ''
                middle_name = parts[2] if len(parts) > 2 else ''

                first_name_clean = first_name.replace('.', '')
                middle_name_clean = middle_name.replace('.', '')

                result.append({
                    'last_name': last_name,
                    'first_name': first_name_clean,
                    'middle_name': middle_name_clean,
                    'full_name': author,
                })
            else:
                result.append({
                    'last_name': author,
                    'first_name': '',
                    'middle_name': '',
                    'full_name': author,
                })

        return result

    def parse_patent_holders(self, holders_str):
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –ø–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—è–º–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π
        """
        if pd.isna(holders_str) or not holders_str:
            return []

        holders_str = str(holders_str)
        holders_list = re.split(r'[\n]\s*', holders_str)

        result = []
        for holder in holders_list:
            holder = holder.strip().strip('"')
            if not holder or holder == 'null' or holder == 'None':
                continue

            holder = re.sub(r'\s*\([A-Z]{2}\)', '', holder)
            result.append(holder)

        return result

    def find_or_create_person(self, person_data):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞"""
        cache_key = f"{person_data['last_name']}|{person_data['first_name']}|{person_data['middle_name']}"

        if cache_key in self.person_cache:
            return self.person_cache[cache_key]

        persons = Person.objects.filter(
            last_name=person_data['last_name'],
            first_name=person_data['first_name']
        )

        if person_data['middle_name']:
            persons = persons.filter(middle_name=person_data['middle_name'])

        if persons.exists():
            person = persons.first()
            self.person_cache[cache_key] = person
            return person

        try:
            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
            new_id = max_id + 1

            if 'full_name' in person_data:
                full_name = person_data['full_name']
            else:
                full_name_parts = [person_data['last_name'], person_data['first_name']]
                if person_data['middle_name']:
                    full_name_parts.append(person_data['middle_name'])
                full_name = ' '.join(full_name_parts)
                full_name = self.person_formatter.format(full_name)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π slug
            base_slug = slugify(f"{person_data['last_name']} {person_data['first_name']} {person_data['middle_name']}".strip())
            if not base_slug:
                base_slug = 'person'

            unique_slug = base_slug
            counter = 1
            while Person.objects.filter(slug=unique_slug).exists():
                unique_slug = f"{base_slug}-{counter}"
                counter += 1

            person = Person.objects.create(
                ceo_id=new_id,
                ceo=full_name,
                last_name=person_data['last_name'],
                first_name=person_data['first_name'],
                middle_name=person_data['middle_name'],
                slug=unique_slug
            )
            self.person_cache[cache_key] = person
            return person
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Person: {e}"))
            return None

    def find_or_create_person_from_name(self, full_name):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞ –ø–æ –ø–æ–ª–Ω–æ–º—É –∏–º–µ–Ω–∏"""
        if pd.isna(full_name) or not full_name:
            return None

        full_name = str(full_name).strip().strip('"')
        full_name = self.person_formatter.format(full_name)

        if full_name in self.person_cache:
            return self.person_cache[full_name]

        parts = full_name.split()

        if len(parts) >= 2:
            last_name = parts[0]
            first_name = parts[1] if len(parts) > 1 else ''
            middle_name = parts[2] if len(parts) > 2 else ''

            first_name_clean = first_name.replace('.', '')
            middle_name_clean = middle_name.replace('.', '')

            person_data = {
                'last_name': last_name,
                'first_name': first_name_clean,
                'middle_name': middle_name_clean,
                'full_name': full_name,
            }
        else:
            person_data = {
                'last_name': full_name,
                'first_name': '',
                'middle_name': '',
                'full_name': full_name,
            }

        return self.find_or_create_person(person_data)

    def find_similar_organization(self, org_name):
        """–£—Å–∏–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        if pd.isna(org_name) or not org_name:
            return None

        org_name = str(org_name).strip().strip('"')

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        direct_match = Organization.objects.filter(
            models.Q(name=org_name) |
            models.Q(full_name=org_name) |
            models.Q(short_name=org_name)
        ).first()
        if direct_match:
            return direct_match

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
        norm_data = self.org_normalizer.normalize_for_search(org_name)
        normalized = norm_data['normalized']
        keywords = norm_data['keywords']

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in keywords:
            if len(keyword) >= 3:
                similar = Organization.objects.filter(
                    models.Q(name__icontains=keyword) |
                    models.Q(full_name__icontains=keyword) |
                    models.Q(short_name__icontains=keyword)
                ).first()
                if similar:
                    return similar

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ –ø–æ –ø–µ—Ä–≤—ã–º 30 —Å–∏–º–≤–æ–ª–∞–º
        if len(normalized) > 30:
            prefix = normalized[:30]
            similar = Organization.objects.filter(
                models.Q(name__icontains=prefix) |
                models.Q(full_name__icontains=prefix) |
                models.Q(short_name__icontains=prefix)
            ).first()
            if similar:
                return similar

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ü–æ–∏—Å–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–ª–æ–≤–∞–º
        words = org_name.split()
        for word in words:
            if len(word) > 4:
                similar = Organization.objects.filter(
                    models.Q(name__icontains=word) |
                    models.Q(full_name__icontains=word)
                ).first()
                if similar:
                    return similar

        return None

    def find_or_create_organization(self, org_name):
        """–ü–æ–∏—Å–∫ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        if pd.isna(org_name) or not org_name:
            return None

        org_name = str(org_name).strip().strip('"')

        if not org_name or org_name == 'null' or org_name == 'None':
            return None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if org_name in self.organization_cache:
            return self.organization_cache[org_name]

        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ
        similar = self.find_similar_organization(org_name)
        if similar:
            self.organization_cache[org_name] = similar
            return similar

        # –ù–µ –Ω–∞—à–ª–∏ - —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º
        try:
            max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
            new_id = max_id + 1

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º slug –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
            base_slug = slugify(org_name[:50])
            if not base_slug:
                base_slug = 'organization'

            unique_slug = base_slug
            counter = 1
            while Organization.objects.filter(slug=unique_slug).exists():
                unique_slug = f"{base_slug}-{counter}"
                counter += 1

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            org = Organization.objects.create(
                organization_id=new_id,
                name=org_name,
                full_name=org_name,
                short_name=org_name[:500] if len(org_name) > 500 else org_name,
                slug=unique_slug,
                register_opk=False,
                strategic=False,
            )

            self.organization_cache[org_name] = org
            return org
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Organization: {e}"))
            return None

    # =========================================================================
    # –ú–ï–¢–û–î–´ –î–õ–Ø –ú–ê–°–°–û–í–û–ì–û –°–û–ó–î–ê–ù–ò–Ø –õ–Æ–î–ï–ô
    # =========================================================================

    def _create_persons_bulk(self, persons_df: pd.DataFrame) -> Dict[str, Person]:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ª—é–¥–µ–π –∏–∑ DataFrame —Å –∏–Ω–¥–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        
        Args:
            persons_df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–æ–π 'entity_name'
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–∏–º—è: –æ–±—ä–µ–∫—Ç Person}
        """
        person_map = {}
        
        if persons_df.empty:
            self.stdout.write("      –ù–µ—Ç –ª—é–¥–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return person_map
        
        all_names = persons_df['entity_name'].tolist()
        total_names = len(all_names)
        
        self.stdout.write(f"      –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_names}")
        
        # –®–ê–ì 1: –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ª—é–¥–µ–π
        self.stdout.write(f"      –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ª—é–¥–µ–π –≤ –ë–î...")
        
        name_to_parts = self._extract_name_parts(all_names)
        existing_persons = self._find_existing_persons(name_to_parts)
        
        # –®–ê–ì 2: –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–≤—ã—Ö –ª—é–¥–µ–π
        valid_names = list(name_to_parts.keys())
        new_names = [name for name in valid_names if name not in existing_persons]
        new_count = len(new_names)
        
        self.stdout.write(f"      –ù–æ–≤—ã—Ö –ª—é–¥–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {new_count}")
        
        # –®–ê–ì 3: –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã—Ö –ª—é–¥–µ–π
        if new_names:
            new_persons_map = self._create_new_persons(new_names)
            person_map.update(new_persons_map)
        
        # –®–ê–ì 4: –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ª—é–¥–µ–π –≤ –º–∞–ø–ø–∏–Ω–≥
        person_map.update(existing_persons)
        
        self.stdout.write(f"      ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ª—é–¥–µ–π: {len(person_map)}")
        
        return person_map

    def _extract_name_parts(self, names: List[str]) -> Dict[str, Tuple[str, str, str]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π –§–ò–û –∏–∑ —Å–ø–∏—Å–∫–∞ –∏–º–µ–Ω
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–ø–æ–ª–Ω–æ–µ_–∏–º—è: (—Ñ–∞–º–∏–ª–∏—è, –∏–º—è, –æ—Ç—á–µ—Å—Ç–≤–æ)}
        """
        name_to_parts = {}
        for name in names:
            if pd.isna(name) or not name:
                continue
            name = str(name).strip()
            if not name:
                continue
            
            parts = name.split()
            if len(parts) >= 2:
                last = parts[0]
                first = parts[1]
                middle = parts[2] if len(parts) > 2 else ''
                name_to_parts[name] = (last, first, middle)
        
        return name_to_parts

    def _find_existing_persons(self, name_to_parts: Dict[str, Tuple[str, str, str]]) -> Dict[str, Person]:
        """
        –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ª—é–¥–µ–π –≤ –ë–î
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–∏–º—è: –æ–±—ä–µ–∫—Ç Person}
        """
        existing_persons = {}
        found_count = 0
        batch_size = 100
        all_names_list = list(name_to_parts.keys())
        
        for i in range(0, len(all_names_list), batch_size):
            batch_names = all_names_list[i:i+batch_size]
            
            # –°—Ç—Ä–æ–∏–º —É—Å–ª–æ–≤–∏—è –ø–æ–∏—Å–∫–∞
            name_conditions = models.Q()
            batch_name_to_parts = {}
            
            for name in batch_names:
                last, first, middle = name_to_parts[name]
                batch_name_to_parts[name] = (last, first, middle)
                
                if middle:
                    name_conditions |= models.Q(
                        last_name=last, 
                        first_name=first, 
                        middle_name=middle
                    )
                else:
                    name_conditions |= models.Q(
                        last_name=last, 
                        first_name=first
                    ) & (models.Q(middle_name='') | models.Q(middle_name__isnull=True))
            
            # –ò—â–µ–º –ª—é–¥–µ–π
            for person in Person.objects.filter(name_conditions).only(
                'ceo_id', 'last_name', 'first_name', 'middle_name', 'ceo', 'slug'
            ):
                for name, (last, first, middle) in batch_name_to_parts.items():
                    if (person.last_name == last and 
                        person.first_name == first and 
                        (not middle or person.middle_name == middle)):
                        existing_persons[name] = person
                        self.person_cache[name] = person
                        found_count += 1
                        break
            
            if (i + len(batch_names)) % 500 == 0 or (i + len(batch_names)) >= len(all_names_list):
                self.stdout.write(f"         –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + len(batch_names)}/{len(all_names_list)} –∏–º–µ–Ω")
        
        self.stdout.write(f"      –ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {found_count}")
        return existing_persons

    def _create_new_persons(self, new_names: List[str]) -> Dict[str, Person]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ª—é–¥–µ–π
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–∏–º—è: –æ–±—ä–µ–∫—Ç Person}
        """
        self.stdout.write(f"      –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è...")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ slugs
        existing_slugs = set(Person.objects.values_list('slug', flat=True))
        self.stdout.write(f"         –°—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö slug-–æ–≤ –≤ –ë–î: {len(existing_slugs)}")
        
        people_to_create = []
        
        for name in new_names:
            if pd.isna(name) or not name:
                continue
            
            name = str(name).strip()
            parts = name.split()
            
            if len(parts) >= 2:
                last_name = parts[0]
                first_name = parts[1]
                middle_name = parts[2] if len(parts) > 2 else ''
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π slug
                name_parts_list = [last_name, first_name]
                if middle_name:
                    name_parts_list.append(middle_name)
                
                base_slug = slugify(' '.join(name_parts_list))
                if not base_slug:
                    base_slug = 'person'
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π slug
                unique_slug, existing_slugs = self._generate_unique_slug(base_slug, existing_slugs)
                
                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –±–µ–∑ ID (ID –±—É–¥–µ—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω –ø—Ä–∏ bulk_create)
                person = Person(
                    ceo=name,
                    last_name=last_name,
                    first_name=first_name,
                    middle_name=middle_name or '',
                    slug=unique_slug
                )
                people_to_create.append(person)
        
        # –°–æ–∑–¥–∞–µ–º –ª—é–¥–µ–π
        return self._bulk_create_persons(people_to_create, len(new_names))

    def _generate_unique_slug(self, base_slug: str, existing_slugs: set) -> Tuple[str, set]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ slug
        
        Returns:
            Tuple[—É–Ω–∏–∫–∞–ª—å–Ω—ã–π_slug, –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ_–º–Ω–æ–∂–µ—Å—Ç–≤–æ_slugs]
        """
        unique_slug = base_slug
        counter = 1
        while unique_slug in existing_slugs:
            unique_slug = f"{base_slug}-{counter}"
            counter += 1
        
        existing_slugs.add(unique_slug)
        return unique_slug, existing_slugs

    def _bulk_create_persons(self, people_to_create: List[Person], total_count: int) -> Dict[str, Person]:
        """
        –ú–∞—Å—Å–æ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ª—é–¥–µ–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–∏–º—è: –æ–±—ä–µ–∫—Ç Person}
        """
        if not people_to_create:
            return {}
        
        self.stdout.write(f"      –°–æ–∑–¥–∞–Ω–∏–µ –ª—é–¥–µ–π –ø–∞—á–∫–∞–º–∏ –ø–æ 500...")
        
        BATCH_SIZE = 500
        created_count = 0
        created_map = {}
        
        for i in range(0, len(people_to_create), BATCH_SIZE):
            batch = people_to_create[i:i+BATCH_SIZE]
            
            # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π max_id –ø–µ—Ä–µ–¥ –∫–∞–∂–¥–æ–π –ø–∞—á–∫–æ–π
            max_id = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
            next_id = max_id + 1
            
            # –ù–∞–∑–Ω–∞—á–∞–µ–º ID –¥–ª—è —Ç–µ–∫—É—â–µ–π –ø–∞—á–∫–∏
            for j, person in enumerate(batch):
                person.ceo_id = next_id + j
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –ø–∞—á–∫–µ
            batch = self._filter_duplicate_persons(batch)
            if not batch:
                continue
            
            # –ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –ø–∞—á–∫–æ–π
            try:
                Person.objects.bulk_create(batch, batch_size=BATCH_SIZE, ignore_conflicts=True)
                created_count += len(batch)
                self.stdout.write(self.style.SUCCESS(f"         ‚úÖ –°–æ–∑–¥–∞–Ω–∞ –ø–∞—á–∫–∞ –∏–∑ {len(batch)} —á–µ–ª–æ–≤–µ–∫"))
            except Exception as e:
                self.stdout.write(self.style.WARNING(f"         –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø–∞—á–∫–∏: {e}"))
                created_count += self._create_persons_one_by_one(batch)
            
            if created_count % 5000 == 0 or created_count >= total_count:
                percent = (created_count / total_count) * 100 if total_count > 0 else 0
                self.stdout.write(f"         –ü—Ä–æ–≥—Ä–µ—Å—Å: {created_count}/{total_count} ({percent:.1f}%)")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ª—é–¥–µ–π –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞
        if created_count > 0:
            created_names = [p.ceo for p in people_to_create[:created_count]]
            created_map = self._fetch_created_persons(created_names)
        
        return created_map

    def _filter_duplicate_persons(self, batch: List[Person]) -> List[Person]:
        """
        –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –ø–∞—á–∫–µ –ø–æ ceo_id –∏ slug
        """
        batch_ceo_ids = [p.ceo_id for p in batch]
        batch_slugs = [p.slug for p in batch]
        
        existing_by_ceo = set(Person.objects.filter(ceo_id__in=batch_ceo_ids).values_list('ceo_id', flat=True))
        existing_by_slug = set(Person.objects.filter(slug__in=batch_slugs).values_list('slug', flat=True))
        
        if existing_by_ceo or existing_by_slug:
            self.stdout.write(self.style.WARNING(f"         –ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –ø–∞—á–∫–µ:"))
            if existing_by_ceo:
                self.stdout.write(self.style.WARNING(f"            –ø–æ ceo_id: {list(existing_by_ceo)[:5]}..."))
            if existing_by_slug:
                self.stdout.write(self.style.WARNING(f"            –ø–æ slug: {list(existing_by_slug)[:5]}..."))
            
            batch = [p for p in batch 
                    if p.ceo_id not in existing_by_ceo 
                    and p.slug not in existing_by_slug]
        
        return batch

    def _create_persons_one_by_one(self, batch: List[Person]) -> int:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ª—é–¥–µ–π –ø–æ –æ–¥–Ω–æ–º—É –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø–∞—á–∫–∏
        """
        created = 0
        for person in batch:
            for attempt in range(10):
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —Å–≤–µ–∂–∏–π max_id –ø–µ—Ä–µ–¥ –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                    current_max = Person.objects.aggregate(models.Max('ceo_id'))['ceo_id__max'] or 0
                    person.ceo_id = current_max + 1
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –æ–±–Ω–æ–≤–ª—è–µ–º slug –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                    if Person.objects.filter(slug=person.slug).exists():
                        base_slug = person.slug.split('-')[0]
                        counter = 1
                        new_slug = f"{base_slug}-{counter}"
                        while Person.objects.filter(slug=new_slug).exists():
                            counter += 1
                            new_slug = f"{base_slug}-{counter}"
                        person.slug = new_slug
                    
                    person.save()
                    created += 1
                    self.stdout.write(self.style.SUCCESS(f"            ‚úÖ –°–æ–∑–¥–∞–Ω: {person.ceo}"))
                    break
                except Exception as e:
                    if attempt == 9:
                        self.stdout.write(self.style.ERROR(f"            ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å {person.ceo}: {e}"))
                    continue
        return created

    def _fetch_created_persons(self, names: List[str]) -> Dict[str, Person]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ª—é–¥–µ–π –∏–∑ –ë–î –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞
        """
        person_map = {}
        for batch in batch_iterator(names, 1000):
            for person in Person.objects.filter(ceo__in=batch).only('ceo_id', 'ceo', 'slug'):
                person_map[person.ceo] = person
                self.person_cache[person.ceo] = person
        return person_map

    # =========================================================================
    # –ú–ï–¢–û–î–´ –î–õ–Ø –ú–ê–°–°–û–í–û–ì–û –°–û–ó–î–ê–ù–ò–Ø –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô
    # =========================================================================

    def _create_organizations_bulk(self, orgs_df: pd.DataFrame) -> Dict[str, Organization]:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏–∑ DataFrame —Å –∏–Ω–¥–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        
        Args:
            orgs_df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–æ–π 'entity_name'
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–Ω–∞–∑–≤–∞–Ω–∏–µ: –æ–±—ä–µ–∫—Ç Organization}
        """
        org_map = {}
        
        if orgs_df.empty:
            self.stdout.write("      –ù–µ—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return org_map
        
        all_names = orgs_df['entity_name'].tolist()
        total_names = len(all_names)
        
        self.stdout.write(f"      –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_names}")
        
        # –®–ê–ì 1: –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        self.stdout.write(f"      –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –≤ –ë–î...")
        
        existing_orgs = self._find_existing_organizations(all_names)
        
        # –®–ê–ì 2: –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–≤—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        new_names = [name for name in all_names if name not in existing_orgs]
        new_count = len(new_names)
        
        self.stdout.write(f"      –ù–æ–≤—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {new_count}")
        
        # –®–ê–ì 3: –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        if new_names:
            new_orgs_map = self._create_new_organizations(new_names)
            org_map.update(new_orgs_map)
        
        # –®–ê–ì 4: –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        org_map.update(existing_orgs)
        
        self.stdout.write(f"      ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π: {len(org_map)}")
        
        return org_map

    def _find_existing_organizations(self, names: List[str]) -> Dict[str, Organization]:
        """
        –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –≤ –ë–î
        """
        existing_orgs = {}
        batch_size = 100
        
        for i in range(0, len(names), batch_size):
            batch_names = names[i:i+batch_size]
            
            for org in Organization.objects.filter(name__in=batch_names).only('organization_id', 'name', 'slug'):
                existing_orgs[org.name] = org
                self.organization_cache[org.name] = org
            
            if (i + len(batch_names)) % 500 == 0 or (i + len(batch_names)) >= len(names):
                self.stdout.write(f"         –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + len(batch_names)}/{len(names)} –Ω–∞–∑–≤–∞–Ω–∏–π")
        
        self.stdout.write(f"      –ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {len(existing_orgs)}")
        return existing_orgs

    def _create_new_organizations(self, new_names: List[str]) -> Dict[str, Organization]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        """
        self.stdout.write(f"      –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è...")
        
        max_id = Organization.objects.aggregate(models.Max('organization_id'))['organization_id__max'] or 0
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ slugs
        existing_slugs = set(Organization.objects.values_list('slug', flat=True))
        self.stdout.write(f"      –í—Å–µ–≥–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö slug: {len(existing_slugs)}")
        
        orgs_to_create = []
        used_slugs_in_batch = set()
        
        for name in new_names:
            base_slug = slugify(name[:50]) or 'organization'
            unique_slug = base_slug
            counter = 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ò —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ slugs, –ò —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –≤ —ç—Ç–æ–º –±–∞—Ç—á–µ
            while unique_slug in existing_slugs or unique_slug in used_slugs_in_batch:
                unique_slug = f"{base_slug}-{counter}"
                counter += 1
            
            used_slugs_in_batch.add(unique_slug)
            existing_slugs.add(unique_slug)
            
            org = Organization(
                organization_id=max_id + len(orgs_to_create) + 1,
                name=name,
                full_name=name,
                short_name=name[:500] if len(name) > 500 else name,
                slug=unique_slug,
                register_opk=False,
                strategic=False,
            )
            orgs_to_create.append(org)
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        return self._bulk_create_organizations(orgs_to_create, len(new_names))

    def _bulk_create_organizations(self, orgs_to_create: List[Organization], total_count: int) -> Dict[str, Organization]:
        """
        –ú–∞—Å—Å–æ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        """
        org_map = {}
        batch_size = 500
        created_count = 0
        
        for batch in batch_iterator(orgs_to_create, batch_size):
            try:
                # –ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –ø–∞—á–∫–æ–π —Å ignore_conflicts
                Organization.objects.bulk_create(batch, batch_size=batch_size, ignore_conflicts=True)
                created_count += len(batch)
            except Exception as e:
                self.stdout.write(f"         –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –±–∞—Ç—á–∞: {e}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ —Å–æ–∑–¥–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É
                for org in batch:
                    try:
                        org.save()
                        created_count += 1
                    except Exception as e2:
                        self.stdout.write(f"         –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é {org.name}: {e2}")
            
            if created_count % 5000 == 0 or created_count == total_count:
                percent = (created_count / total_count) * 100 if total_count > 0 else 0
                self.stdout.write(f"         –°–æ–∑–¥–∞–Ω–æ {created_count}/{total_count} ({percent:.1f}%)")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞
        if created_count > 0:
            created_names = [o.name for o in orgs_to_create[:created_count]]
            org_map = self._fetch_created_organizations(created_names)
        
        return org_map

    def _fetch_created_organizations(self, names: List[str]) -> Dict[str, Organization]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏–∑ –ë–î –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞
        """
        org_map = {}
        for batch in batch_iterator(names, 1000):
            for org in Organization.objects.filter(name__in=batch).only('organization_id', 'name', 'slug'):
                org_map[org.name] = org
                self.organization_cache[org.name] = org
        return org_map

    # =========================================================================
    # –ú–ï–¢–û–î–´ –î–õ–Ø –†–ê–ë–û–¢–´ –°–û –°–í–Ø–ó–Ø–ú–ò (–û–ë–©–ò–ï –î–õ–Ø –í–°–ï–• –ü–ê–†–°–ï–†–û–í)
    # =========================================================================

    def _process_relations_dataframe(self, relations_data: List[Dict], reg_to_ip: Dict):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Å–≤—è–∑–µ–π —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π DataFrame
        –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –¥–æ—á–µ—Ä–Ω–∏—Ö –∫–ª–∞—Å—Å–∞—Ö –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        """
        if not relations_data:
            self.stdout.write("   –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–≤—è–∑–µ–π")
            return

        self.stdout.write("   –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å–≤—è–∑–µ–π")
        df_relations = pd.DataFrame(relations_data)
        
        self.stdout.write(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π —Å–≤—è–∑–µ–π: {len(df_relations)}")
        self.stdout.write(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤: {df_relations['reg_number'].nunique()}")

        self.stdout.write("   –î–æ–±–∞–≤–ª–µ–Ω–∏–µ ID –æ–±—ä–µ–∫—Ç–æ–≤")
        df_relations['ip_id'] = df_relations['reg_number'].map(reg_to_ip)

        missing_ip = df_relations['ip_id'].isna().sum()
        if missing_ip > 0:
            self.stdout.write(self.style.WARNING(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ {missing_ip} —Å–≤—è–∑–µ–π —Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º–∏ ID –æ–±—ä–µ–∫—Ç–æ–≤"))
            df_relations = df_relations.dropna(subset=['ip_id']).copy()
        
        df_relations['ip_id'] = df_relations['ip_id'].astype(int)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π
        self.stdout.write("   –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Natasha")
        
        unique_entities = df_relations[['entity_name', 'entity_type']].drop_duplicates()
        holders_to_check = unique_entities[unique_entities['entity_type'].isna()]['entity_name'].tolist()

        if holders_to_check:
            self.stdout.write(f"   –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è {len(holders_to_check)} –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π")
            entity_type_map = self.type_detector.detect_type_batch(holders_to_check)

            mask = df_relations['entity_type'].isna()
            df_relations.loc[mask, 'entity_type'] = \
                df_relations.loc[mask, 'entity_name'].map(entity_type_map)

        type_stats = df_relations['entity_type'].value_counts().to_dict()
        self.stdout.write(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤: –ª—é–¥–∏={type_stats.get('person', 0)}, "
                         f"–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏={type_stats.get('organization', 0)}")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º
        unique_entities = df_relations[['entity_name', 'entity_type']].drop_duplicates()
        
        persons_df = unique_entities[unique_entities['entity_type'] == 'person']
        orgs_df = unique_entities[unique_entities['entity_type'] == 'organization']

        person_map = {}
        if not persons_df.empty:
            self.stdout.write(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(persons_df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π")
            person_map = self._create_persons_bulk(persons_df)

        org_map = {}
        if not orgs_df.empty:
            self.stdout.write(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(orgs_df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
            org_map = self._create_organizations_bulk(orgs_df)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π
        self.stdout.write("   –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ –ë–î")

        authors_df = df_relations[df_relations['relation_type'] == 'author'].copy()
        holders_df = df_relations[df_relations['relation_type'] == 'holder'].copy()

        # –ê–≤—Ç–æ—Ä—ã
        author_relations = self._prepare_author_relations(authors_df, person_map)
        
        # –ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏ (–ª—é–¥–∏ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)
        holder_person_relations, holder_org_relations = self._prepare_holder_relations(
            holders_df, person_map, org_map
        )

        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π
        self._create_all_relations(author_relations, holder_person_relations, holder_org_relations)

        self.stdout.write(self.style.SUCCESS("   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Å–≤—è–∑–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞"))

    def _prepare_author_relations(self, authors_df: pd.DataFrame, person_map: Dict) -> List[Tuple[int, int]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤"""
        if authors_df.empty:
            return []
        
        person_id_map = {name: p.ceo_id for name, p in person_map.items()}
        authors_df['person_id'] = authors_df['entity_name'].map(person_id_map)
        authors_df = authors_df.dropna(subset=['person_id'])
        authors_df['person_id'] = authors_df['person_id'].astype(int)
        
        authors_unique = authors_df[['ip_id', 'person_id']].drop_duplicates()
        relations = [(row['ip_id'], row['person_id']) for _, row in authors_unique.iterrows()]
        
        self.stdout.write(f"   –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(relations)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤")
        return relations

    def _prepare_holder_relations(self, holders_df: pd.DataFrame, person_map: Dict, org_map: Dict) -> Tuple[List[Tuple[int, int]], List[Tuple[int, int]]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π"""
        person_relations = []
        org_relations = []

        if holders_df.empty:
            return person_relations, org_relations

        # –ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏-–ª—é–¥–∏
        holders_persons = holders_df[holders_df['entity_type'] == 'person'].copy()
        if not holders_persons.empty:
            person_id_map = {name: p.ceo_id for name, p in person_map.items()}
            holders_persons['person_id'] = holders_persons['entity_name'].map(person_id_map)
            holders_persons = holders_persons.dropna(subset=['person_id'])
            holders_persons['person_id'] = holders_persons['person_id'].astype(int)
            
            holders_persons_unique = holders_persons[['ip_id', 'person_id']].drop_duplicates()
            person_relations = [(row['ip_id'], row['person_id']) for _, row in holders_persons_unique.iterrows()]
            self.stdout.write(f"   –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(person_relations)} —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–ª—é–¥–µ–π")

        # –ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        holders_orgs = holders_df[holders_df['entity_type'] == 'organization'].copy()
        if not holders_orgs.empty:
            org_id_map = {name: o.organization_id for name, o in org_map.items()}
            holders_orgs['org_id'] = holders_orgs['entity_name'].map(org_id_map)
            holders_orgs = holders_orgs.dropna(subset=['org_id'])
            holders_orgs['org_id'] = holders_orgs['org_id'].astype(int)
            
            holders_orgs_unique = holders_orgs[['ip_id', 'org_id']].drop_duplicates()
            org_relations = [(row['ip_id'], row['org_id']) for _, row in holders_orgs_unique.iterrows()]
            self.stdout.write(f"   –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(org_relations)} —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")

        return person_relations, org_relations

    def _create_all_relations(self, author_relations: List[Tuple[int, int]], 
                             holder_person_relations: List[Tuple[int, int]], 
                             holder_org_relations: List[Tuple[int, int]]):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ —Å–≤—è–∑–µ–π"""
        if author_relations:
            self.stdout.write("   –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤")
            ip_ids = list(set(ip_id for ip_id, _ in author_relations))
            with tqdm(total=len(ip_ids), desc="   –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤", unit="ip") as pbar:
                self._delete_author_relations(ip_ids, pbar)
            
            with tqdm(total=len(author_relations), desc="   –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤", unit="—Å–≤") as pbar:
                self._create_author_relations(author_relations, pbar)

        if holder_person_relations:
            self.stdout.write("   –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π (–ª—é–¥–∏)")
            ip_ids = list(set(ip_id for ip_id, _ in holder_person_relations))
            with tqdm(total=len(ip_ids), desc="   –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π", unit="ip") as pbar:
                self._delete_holder_person_relations(ip_ids, pbar)
            
            with tqdm(total=len(holder_person_relations), desc="   –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π", unit="—Å–≤") as pbar:
                self._create_holder_person_relations(holder_person_relations, pbar)

        if holder_org_relations:
            self.stdout.write("   –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π (–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)")
            ip_ids = list(set(ip_id for ip_id, _ in holder_org_relations))
            with tqdm(total=len(ip_ids), desc="   –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π", unit="ip") as pbar:
                self._delete_holder_org_relations(ip_ids, pbar)
            
            with tqdm(total=len(holder_org_relations), desc="   –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π", unit="—Å–≤") as pbar:
                self._create_holder_org_relations(holder_org_relations, pbar)

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —Å–≤—è–∑–µ–π
    def _delete_author_relations(self, ip_ids: List[int], pbar):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤"""
        delete_batch_size = 500
        for i in range(0, len(ip_ids), delete_batch_size):
            batch_ids = ip_ids[i:i+delete_batch_size]
            IPObject.authors.through.objects.filter(
                ipobject_id__in=batch_ids
            ).delete()
            pbar.update(len(batch_ids))

    def _delete_holder_person_relations(self, ip_ids: List[int], pbar):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–ª—é–¥–µ–π"""
        delete_batch_size = 500
        for i in range(0, len(ip_ids), delete_batch_size):
            batch_ids = ip_ids[i:i+delete_batch_size]
            IPObject.owner_persons.through.objects.filter(
                ipobject_id__in=batch_ids
            ).delete()
            pbar.update(len(batch_ids))

    def _delete_holder_org_relations(self, ip_ids: List[int], pbar):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        delete_batch_size = 500
        for i in range(0, len(ip_ids), delete_batch_size):
            batch_ids = ip_ids[i:i+delete_batch_size]
            IPObject.owner_organizations.through.objects.filter(
                ipobject_id__in=batch_ids
            ).delete()
            pbar.update(len(batch_ids))

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤—è–∑–µ–π
    def _create_author_relations(self, relations: List[Tuple[int, int]], pbar):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –∞–≤—Ç–æ—Ä–æ–≤"""
        create_batch_size = 2000
        for batch in batch_iterator(relations, create_batch_size):
            through_objs = [
                IPObject.authors.through(
                    ipobject_id=ip_id,
                    person_id=person_id
                )
                for ip_id, person_id in batch
            ]
            IPObject.authors.through.objects.bulk_create(
                through_objs, batch_size=2000, ignore_conflicts=True
            )
            pbar.update(len(batch))

    def _create_holder_person_relations(self, relations: List[Tuple[int, int]], pbar):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–ª—é–¥–µ–π"""
        create_batch_size = 2000
        for batch in batch_iterator(relations, create_batch_size):
            through_objs = [
                IPObject.owner_persons.through(
                    ipobject_id=ip_id,
                    person_id=person_id
                )
                for ip_id, person_id in batch
            ]
            IPObject.owner_persons.through.objects.bulk_create(
                through_objs, batch_size=2000, ignore_conflicts=True
            )
            pbar.update(len(batch))

    def _create_holder_org_relations(self, relations: List[Tuple[int, int]], pbar):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        create_batch_size = 2000
        for batch in batch_iterator(relations, create_batch_size):
            through_objs = [
                IPObject.owner_organizations.through(
                    ipobject_id=ip_id,
                    organization_id=org_id
                )
                for ip_id, org_id in batch
            ]
            IPObject.owner_organizations.through.objects.bulk_create(
                through_objs, batch_size=2000, ignore_conflicts=True
            )
            pbar.update(len(batch))
```


-----

# –§–∞–π–ª: management\parsers\computer_program.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –µ–¥–∏–Ω–æ–≥–æ DataFrame –¥–ª—è —Å–≤—è–∑–µ–π
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä year –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ –≥–æ–¥–∞–º
"""

import logging
import gc
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict
import re

import pandas as pd
from django.db import models, transaction
from django.utils.text import slugify
from tqdm import tqdm

from intellectual_property.models import IPObject, IPType
from core.models import Organization

from .base import BaseFIPSParser
from ..utils.progress import batch_iterator

logger = logging.getLogger(__name__)


class ComputerProgramParser(BaseFIPSParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–≤—è–∑–µ–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–¥–∏–Ω—ã–π DataFrame –¥–ª—è –≤—Å–µ—Ö —Å–≤—è–∑–µ–π (–∞–≤—Ç–æ—Ä—ã + –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏)
    """

    def get_ip_type(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø –†–ò–î 'computer-program'"""
        return IPType.objects.filter(slug='computer-program').first()

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è CSV"""
        return ['registration number', 'program name']

    def _has_data_changed(self, obj, new_data):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç–∞
        """
        fields_to_check = [
            ('name', obj.name, new_data['name']),
            ('application_date', obj.application_date, new_data['application_date']),
            ('registration_date', obj.registration_date, new_data['registration_date']),
            ('actual', obj.actual, new_data['actual']),
            ('publication_url', obj.publication_url, new_data['publication_url']),
            ('creation_year', obj.creation_year, new_data.get('creation_year')),
        ]

        for field_name, old_val, new_val in fields_to_check:
            if old_val != new_val:
                return True
        return False

    def parse_dataframe(self, df, catalogue, year=None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            catalogue: –æ–±—ä–µ–∫—Ç –∫–∞—Ç–∞–ª–æ–≥–∞
            year: –≥–æ–¥ –¥–ª—è —Ç–µ–∫—É—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        year_msg = f" –¥–ª—è {year} –≥–æ–¥–∞" if year else ""
        self.stdout.write(f"\nüîπ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú{year_msg}")

        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –†–ò–î
        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'computer-program' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats

        upload_date = catalogue.upload_date.date() if catalogue.upload_date else None

        # =====================================================================
        # –®–ê–ì 1: –°–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
        # =====================================================================
        self.stdout.write("üîπ –ß—Ç–µ–Ω–∏–µ CSV –∏ —Å–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        reg_num_to_row = {}
        skipped_empty = 0
        
        for idx, row in df.iterrows():
            reg_num = self.clean_string(row.get('registration number'))
            if reg_num:
                reg_num_to_row[reg_num] = row
            else:
                skipped_empty += 1

        self.stdout.write(f"üîπ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: {len(reg_num_to_row)} (–ø—Ä–æ–ø—É—â–µ–Ω–æ –ø—É—Å—Ç—ã—Ö: {skipped_empty})")

        # =====================================================================
        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î
        # =====================================================================
        self.stdout.write("üîπ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î")
        
        existing_objects = {}
        batch_size = 500
        reg_numbers = list(reg_num_to_row.keys())
        
        with tqdm(total=len(reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—á–∫–∞–º–∏", unit="–∑–∞–ø") as pbar:
            for i in range(0, len(reg_numbers), batch_size):
                batch_numbers = reg_numbers[i:i+batch_size]
                
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_numbers,
                    ip_type=ip_type
                ).select_related('ip_type'):
                    existing_objects[obj.registration_number] = obj
                
                pbar.update(len(batch_numbers))

        self.stdout.write(f"üîπ –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(existing_objects)}")

        # =====================================================================
        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è IPObject
        # =====================================================================
        self.stdout.write("üîπ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject")
        
        to_create = []
        to_update = []
        skipped_by_date = []
        unchanged_count = 0
        error_reg_numbers = []

        relations_data = []
        
        with tqdm(total=len(reg_num_to_row), desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject", unit="–∑–∞–ø") as pbar:
            for reg_num, row in reg_num_to_row.items():
                try:
                    if not self.command.force and upload_date and reg_num in existing_objects:
                        existing = existing_objects[reg_num]
                        if existing.updated_at and existing.updated_at.date() >= upload_date:
                            skipped_by_date.append(reg_num)
                            pbar.update(1)
                            continue

                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                    name = self.clean_string(row.get('program name'))
                    if name:
                        name = self.rid_formatter.format(name)
                    else:
                        name = f"–ü—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è –≠–í–ú ‚Ññ{reg_num}"

                    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã
                    application_date = self.parse_date(row.get('application date'))
                    registration_date = self.parse_date(row.get('registration date'))
                    actual = self.parse_bool(row.get('actual'))
                    publication_url = self.clean_string(row.get('publication URL'))
                    
                    creation_year = None
                    creation_year_str = row.get('creation year')
                    if not pd.isna(creation_year_str) and creation_year_str:
                        try:
                            creation_year = int(float(creation_year_str))
                        except (ValueError, TypeError):
                            pass
                    
                    if not creation_year and application_date:
                        creation_year = application_date.year
                    elif not creation_year and registration_date:
                        creation_year = registration_date.year

                    obj_data = {
                        'registration_number': reg_num,
                        'ip_type_id': ip_type.id,
                        'name': name,
                        'application_date': application_date,
                        'registration_date': registration_date,
                        'actual': actual,
                        'publication_url': publication_url,
                        'creation_year': creation_year,
                    }

                    if reg_num in existing_objects:
                        if self._has_data_changed(existing_objects[reg_num], obj_data):
                            to_update.append(obj_data)
                        else:
                            unchanged_count += 1
                    else:
                        to_create.append(obj_data)

                    # –ê–≤—Ç–æ—Ä—ã
                    authors_str = row.get('authors')
                    if not pd.isna(authors_str) and authors_str:
                        authors = self._parse_program_authors(authors_str)
                        for author in authors:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': author['full_name'],
                                'entity_type': 'person',
                                'relation_type': 'author',
                                'entity_data': author
                            })

                    # –ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏
                    holders_str = row.get('right holders')
                    if not pd.isna(holders_str) and holders_str:
                        holders = self._parse_right_holders(holders_str)
                        for holder in holders:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': holder,
                                'entity_type': None,
                                'relation_type': 'holder',
                                'entity_data': {'full_name': holder}
                            })

                except Exception as e:
                    error_reg_numbers.append(reg_num)
                    if len(error_reg_numbers) < 10:
                        self.stdout.write(self.style.ERROR(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    elif len(error_reg_numbers) == 10:
                        self.stdout.write(self.style.WARNING("\n‚ö†Ô∏è ... –∏ –¥–∞–ª–µ–µ –æ—à–∏–±–∫–∏ –ø–æ–¥–∞–≤–ª—è—é—Ç—Å—è"))
                    
                    logger.error(f"Error preparing computer program {reg_num}: {e}", exc_info=True)

                pbar.update(1)

        self.stdout.write(f"üîπ –ò—Ç–æ–≥–æ: –Ω–æ–≤—ã—Ö={len(to_create)}, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ={len(to_update)}, "
                         f"–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={unchanged_count}, –æ—à–∏–±–æ–∫={len(error_reg_numbers)}")

        stats['skipped_by_date'] = len(skipped_by_date)
        stats['skipped'] += len(skipped_by_date)
        stats['errors'] = len(error_reg_numbers)
        stats['unchanged'] = unchanged_count

        # =====================================================================
        # –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ IPObject
        # =====================================================================
        if to_create and not self.command.dry_run:
            self.stdout.write(f"üîπ –°–æ–∑–¥–∞–Ω–∏–µ {len(to_create)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_create), desc="–°–æ–∑–¥–∞–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['created'] = self._bulk_create_objects(to_create, pbar)

        if to_update and not self.command.dry_run:
            self.stdout.write(f"üîπ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {len(to_update)} –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_update), desc="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['updated'] = self._bulk_update_objects(to_update, existing_objects, pbar)

        # =====================================================================
        # –®–ê–ì 5: –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ reg_number -> ip_id
        # =====================================================================
        self.stdout.write("üîπ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        all_reg_numbers = list(set(
            list(existing_objects.keys()) + 
            [data['registration_number'] for data in to_create]
        ))
        
        reg_to_ip = {}
        with tqdm(total=len(all_reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ ID –æ–±—ä–µ–∫—Ç–æ–≤", unit="–∑–∞–ø") as pbar:
            batch_size = 1000
            for i in range(0, len(all_reg_numbers), batch_size):
                batch_nums = all_reg_numbers[i:i+batch_size]
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_nums,
                    ip_type=ip_type
                ).only('id', 'registration_number'):
                    reg_to_ip[obj.registration_number] = obj.id
                pbar.update(len(batch_nums))

        self.stdout.write(f"üîπ –ó–∞–≥—Ä—É–∂–µ–Ω–æ ID –¥–ª—è {len(reg_to_ip)} –æ–±—ä–µ–∫—Ç–æ–≤")

        # =====================================================================
        # –®–ê–ì 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π DataFrame
        # =====================================================================
        if relations_data and not self.command.dry_run:
            self.stdout.write("üîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π")
            self._process_relations_dataframe(relations_data, reg_to_ip)

        gc.collect()

        stats['processed'] = len(df) - stats['skipped'] - stats['errors']

        year_info = f" –¥–ª—è {year} –≥–æ–¥–∞" if year else ""
        self.stdout.write(self.style.SUCCESS(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú{year_info} –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"   –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats['unchanged']}")
        self.stdout.write(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']} (–∏–∑ –Ω–∏—Ö –ø–æ –¥–∞—Ç–µ: {stats['skipped_by_date']})")
        self.stdout.write(f"   –û—à–∏–±–æ–∫: {stats['errors']}")

        return stats

    def _parse_program_authors(self, authors_str: str) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –∞–≤—Ç–æ—Ä–∞–º–∏ –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú
        """
        if pd.isna(authors_str) or not authors_str:
            return []

        authors_str = str(authors_str)
        authors_list = re.split(r'[\n]\s*', authors_str)

        result = []
        for author in authors_list:
            author = author.strip()
            if not author or author == '""' or author == 'null':
                continue

            author = author.strip('"')
            author = re.sub(r'\s*\([A-Z]{2}\)$', '', author)
            author = self.person_formatter.format(author)

            parts = author.split()

            if len(parts) >= 2:
                last_name = parts[0]
                first_name = parts[1] if len(parts) > 1 else ''
                middle_name = parts[2] if len(parts) > 2 else ''

                first_name_clean = first_name.replace('.', '')
                middle_name_clean = middle_name.replace('.', '')

                result.append({
                    'last_name': last_name,
                    'first_name': first_name_clean,
                    'middle_name': middle_name_clean,
                    'full_name': author,
                })
            else:
                result.append({
                    'last_name': author,
                    'first_name': '',
                    'middle_name': '',
                    'full_name': author,
                })

        return result

    def _parse_right_holders(self, holders_str: str) -> List[str]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—è–º–∏ –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú
        """
        if pd.isna(holders_str) or not holders_str:
            return []
        
        holders_str = str(holders_str)
        holders_list = re.split(r'[\n]\s*', holders_str)
        
        result = []
        for holder in holders_list:
            holder = holder.strip().strip('"')
            if not holder or holder == 'null' or holder == 'None' or holder.lower() == '–Ω–µ—Ç':
                continue
            holder = re.sub(r'\s*\([A-Z]{2}\)$', '', holder)
            result.append(holder)
        
        return result

    def _bulk_create_objects(self, to_create: List[Dict], pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        created_count = 0
        batch_size = 1000

        for batch in batch_iterator(to_create, batch_size):
            create_objects = [IPObject(**data) for data in batch]
            IPObject.objects.bulk_create(create_objects, batch_size=batch_size)
            created_count += len(batch)
            pbar.update(len(batch))

        return created_count

    def _bulk_update_objects(self, to_update: List[Dict], existing_objects: Dict, pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        updated_count = 0
        BATCH_UPDATE_SIZE = 500

        for batch in batch_iterator(to_update, BATCH_UPDATE_SIZE):
            with transaction.atomic():
                for data in batch:
                    obj = existing_objects[data['registration_number']]
                    update_fields = []
                    for field, value in data.items():
                        if field != 'registration_number' and getattr(obj, field) != value:
                            setattr(obj, field, value)
                            update_fields.append(field)
                    if update_fields:
                        obj.save(update_fields=update_fields)
                        updated_count += 1
            pbar.update(len(batch))

        return updated_count
```


-----

# –§–∞–π–ª: management\parsers\database.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –µ–¥–∏–Ω–æ–≥–æ DataFrame –¥–ª—è —Å–≤—è–∑–µ–π
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä year –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ –≥–æ–¥–∞–º
"""

import logging
import gc
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict
import re

import pandas as pd
from django.db import models, transaction
from django.utils.text import slugify
from tqdm import tqdm

from intellectual_property.models import IPObject, IPType
from core.models import Organization

from .base import BaseFIPSParser
from ..utils.progress import batch_iterator

logger = logging.getLogger(__name__)


class DatabaseParser(BaseFIPSParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–≤—è–∑–µ–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–¥–∏–Ω—ã–π DataFrame –¥–ª—è –≤—Å–µ—Ö —Å–≤—è–∑–µ–π (–∞–≤—Ç–æ—Ä—ã + –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏)
    """

    def get_ip_type(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø –†–ò–î 'database'"""
        return IPType.objects.filter(slug='database').first()

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è CSV"""
        return ['registration number', 'db name']

    def _has_data_changed(self, obj, new_data):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç–∞
        """
        fields_to_check = [
            ('name', obj.name, new_data['name']),
            ('application_date', obj.application_date, new_data['application_date']),
            ('registration_date', obj.registration_date, new_data['registration_date']),
            ('expiration_date', obj.expiration_date, new_data.get('expiration_date')),
            ('actual', obj.actual, new_data['actual']),
            ('publication_url', obj.publication_url, new_data['publication_url']),
            ('creation_year', obj.creation_year, new_data.get('creation_year')),
            ('publication_year', obj.publication_year, new_data.get('publication_year')),
            ('update_year', obj.update_year, new_data.get('update_year')),
        ]

        for field_name, old_val, new_val in fields_to_check:
            if old_val != new_val:
                return True
        return False

    def parse_dataframe(self, df, catalogue, year=None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            catalogue: –æ–±—ä–µ–∫—Ç –∫–∞—Ç–∞–ª–æ–≥–∞
            year: –≥–æ–¥ –¥–ª—è —Ç–µ–∫—É—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        year_msg = f" –¥–ª—è {year} –≥–æ–¥–∞" if year else ""
        self.stdout.write(f"\nüîπ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö{year_msg}")

        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –†–ò–î
        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'database' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats

        upload_date = catalogue.upload_date.date() if catalogue.upload_date else None

        # =====================================================================
        # –®–ê–ì 1: –°–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
        # =====================================================================
        self.stdout.write("üîπ –ß—Ç–µ–Ω–∏–µ CSV –∏ —Å–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        reg_num_to_row = {}
        skipped_empty = 0
        
        for idx, row in df.iterrows():
            reg_num = self.clean_string(row.get('registration number'))
            if reg_num:
                reg_num_to_row[reg_num] = row
            else:
                skipped_empty += 1

        self.stdout.write(f"üîπ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: {len(reg_num_to_row)} (–ø—Ä–æ–ø—É—â–µ–Ω–æ –ø—É—Å—Ç—ã—Ö: {skipped_empty})")

        # =====================================================================
        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î
        # =====================================================================
        self.stdout.write("üîπ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î")
        
        existing_objects = {}
        batch_size = 500
        reg_numbers = list(reg_num_to_row.keys())
        
        with tqdm(total=len(reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—á–∫–∞–º–∏", unit="–∑–∞–ø") as pbar:
            for i in range(0, len(reg_numbers), batch_size):
                batch_numbers = reg_numbers[i:i+batch_size]
                
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_numbers,
                    ip_type=ip_type
                ).select_related('ip_type'):
                    existing_objects[obj.registration_number] = obj
                
                pbar.update(len(batch_numbers))

        self.stdout.write(f"üîπ –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(existing_objects)}")

        # =====================================================================
        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è IPObject
        # =====================================================================
        self.stdout.write("üîπ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject")
        
        to_create = []
        to_update = []
        skipped_by_date = []
        unchanged_count = 0
        error_reg_numbers = []

        relations_data = []
        
        with tqdm(total=len(reg_num_to_row), desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject", unit="–∑–∞–ø") as pbar:
            for reg_num, row in reg_num_to_row.items():
                try:
                    if not self.command.force and upload_date and reg_num in existing_objects:
                        existing = existing_objects[reg_num]
                        if existing.updated_at and existing.updated_at.date() >= upload_date:
                            skipped_by_date.append(reg_num)
                            pbar.update(1)
                            continue

                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                    name = self.clean_string(row.get('db name'))
                    if name:
                        name = self.rid_formatter.format(name)
                    else:
                        name = f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö ‚Ññ{reg_num}"

                    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã
                    application_date = self.parse_date(row.get('application date'))
                    registration_date = self.parse_date(row.get('registration date'))
                    expiration_date = self.parse_date(row.get('expiration date'))
                    actual = self.parse_bool(row.get('actual'))
                    publication_url = self.clean_string(row.get('publication URL'))
                    
                    creation_year = None
                    creation_year_str = row.get('creation year')
                    if not pd.isna(creation_year_str) and creation_year_str:
                        try:
                            creation_year = int(float(creation_year_str))
                        except (ValueError, TypeError):
                            pass
                    
                    publication_year = None
                    publication_year_str = row.get('publication year')
                    if not pd.isna(publication_year_str) and publication_year_str:
                        try:
                            publication_year = int(float(publication_year_str))
                        except (ValueError, TypeError):
                            pass
                    
                    update_year = None
                    update_year_str = row.get('update year')
                    if not pd.isna(update_year_str) and update_year_str:
                        try:
                            update_year = int(float(update_year_str))
                        except (ValueError, TypeError):
                            pass
                    
                    if not creation_year and application_date:
                        creation_year = application_date.year
                    elif not creation_year and registration_date:
                        creation_year = registration_date.year

                    obj_data = {
                        'registration_number': reg_num,
                        'ip_type_id': ip_type.id,
                        'name': name,
                        'application_date': application_date,
                        'registration_date': registration_date,
                        'expiration_date': expiration_date,
                        'actual': actual,
                        'publication_url': publication_url,
                        'creation_year': creation_year,
                        'publication_year': publication_year,
                        'update_year': update_year,
                    }

                    if reg_num in existing_objects:
                        if self._has_data_changed(existing_objects[reg_num], obj_data):
                            to_update.append(obj_data)
                        else:
                            unchanged_count += 1
                    else:
                        to_create.append(obj_data)

                    # –ê–≤—Ç–æ—Ä—ã
                    authors_str = row.get('authors')
                    if not pd.isna(authors_str) and authors_str:
                        authors = self._parse_database_authors(authors_str)
                        for author in authors:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': author['full_name'],
                                'entity_type': 'person',
                                'relation_type': 'author',
                                'entity_data': author
                            })

                    # –ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏
                    holders_str = row.get('right holders')
                    if not pd.isna(holders_str) and holders_str:
                        holders = self._parse_right_holders(holders_str)
                        for holder in holders:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': holder,
                                'entity_type': None,
                                'relation_type': 'holder',
                                'entity_data': {'full_name': holder}
                            })

                except Exception as e:
                    error_reg_numbers.append(reg_num)
                    if len(error_reg_numbers) < 10:
                        self.stdout.write(self.style.ERROR(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    elif len(error_reg_numbers) == 10:
                        self.stdout.write(self.style.WARNING("\n‚ö†Ô∏è ... –∏ –¥–∞–ª–µ–µ –æ—à–∏–±–∫–∏ –ø–æ–¥–∞–≤–ª—è—é—Ç—Å—è"))
                    
                    logger.error(f"Error preparing database {reg_num}: {e}", exc_info=True)

                pbar.update(1)

        self.stdout.write(f"üîπ –ò—Ç–æ–≥–æ: –Ω–æ–≤—ã—Ö={len(to_create)}, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ={len(to_update)}, "
                         f"–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={unchanged_count}, –æ—à–∏–±–æ–∫={len(error_reg_numbers)}")

        stats['skipped_by_date'] = len(skipped_by_date)
        stats['skipped'] += len(skipped_by_date)
        stats['errors'] = len(error_reg_numbers)
        stats['unchanged'] = unchanged_count

        # =====================================================================
        # –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ IPObject
        # =====================================================================
        if to_create and not self.command.dry_run:
            self.stdout.write(f"üîπ –°–æ–∑–¥–∞–Ω–∏–µ {len(to_create)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_create), desc="–°–æ–∑–¥–∞–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['created'] = self._bulk_create_objects(to_create, pbar)

        if to_update and not self.command.dry_run:
            self.stdout.write(f"üîπ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {len(to_update)} –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_update), desc="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['updated'] = self._bulk_update_objects(to_update, existing_objects, pbar)

        # =====================================================================
        # –®–ê–ì 5: –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ reg_number -> ip_id
        # =====================================================================
        self.stdout.write("üîπ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        all_reg_numbers = list(set(
            list(existing_objects.keys()) + 
            [data['registration_number'] for data in to_create]
        ))
        
        reg_to_ip = {}
        with tqdm(total=len(all_reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ ID –æ–±—ä–µ–∫—Ç–æ–≤", unit="–∑–∞–ø") as pbar:
            batch_size = 1000
            for i in range(0, len(all_reg_numbers), batch_size):
                batch_nums = all_reg_numbers[i:i+batch_size]
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_nums,
                    ip_type=ip_type
                ).only('id', 'registration_number'):
                    reg_to_ip[obj.registration_number] = obj.id
                pbar.update(len(batch_nums))

        self.stdout.write(f"üîπ –ó–∞–≥—Ä—É–∂–µ–Ω–æ ID –¥–ª—è {len(reg_to_ip)} –æ–±—ä–µ–∫—Ç–æ–≤")

        # =====================================================================
        # –®–ê–ì 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π DataFrame
        # =====================================================================
        if relations_data and not self.command.dry_run:
            self.stdout.write("üîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π")
            self._process_relations_dataframe(relations_data, reg_to_ip)

        gc.collect()

        stats['processed'] = len(df) - stats['skipped'] - stats['errors']

        year_info = f" –¥–ª—è {year} –≥–æ–¥–∞" if year else ""
        self.stdout.write(self.style.SUCCESS(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö{year_info} –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"   –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats['unchanged']}")
        self.stdout.write(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']} (–∏–∑ –Ω–∏—Ö –ø–æ –¥–∞—Ç–µ: {stats['skipped_by_date']})")
        self.stdout.write(f"   –û—à–∏–±–æ–∫: {stats['errors']}")

        return stats

    def _parse_database_authors(self, authors_str: str) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –∞–≤—Ç–æ—Ä–∞–º–∏ –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
        """
        if pd.isna(authors_str) or not authors_str:
            return []

        authors_str = str(authors_str)
        authors_list = re.split(r'[\n]\s*', authors_str)

        result = []
        for author in authors_list:
            author = author.strip()
            if not author or author == '""' or author == 'null':
                continue

            author = author.strip('"')
            author = re.sub(r'\s*\([A-Z]{2}\)$', '', author)
            author = self.person_formatter.format(author)

            parts = author.split()

            if len(parts) >= 2:
                last_name = parts[0]
                first_name = parts[1] if len(parts) > 1 else ''
                middle_name = parts[2] if len(parts) > 2 else ''

                first_name_clean = first_name.replace('.', '')
                middle_name_clean = middle_name.replace('.', '')

                result.append({
                    'last_name': last_name,
                    'first_name': first_name_clean,
                    'middle_name': middle_name_clean,
                    'full_name': author,
                })
            else:
                result.append({
                    'last_name': author,
                    'first_name': '',
                    'middle_name': '',
                    'full_name': author,
                })

        return result

    def _parse_right_holders(self, holders_str: str) -> List[str]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—è–º–∏ –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
        """
        if pd.isna(holders_str) or not holders_str:
            return []
        
        holders_str = str(holders_str)
        holders_list = re.split(r'[\n]\s*', holders_str)
        
        result = []
        for holder in holders_list:
            holder = holder.strip().strip('"')
            if not holder or holder == 'null' or holder == 'None' or holder.lower() == '–Ω–µ—Ç':
                continue
            holder = re.sub(r'\s*\([A-Z]{2}\)$', '', holder)
            result.append(holder)
        
        return result

    def _bulk_create_objects(self, to_create: List[Dict], pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        created_count = 0
        batch_size = 1000

        for batch in batch_iterator(to_create, batch_size):
            create_objects = [IPObject(**data) for data in batch]
            IPObject.objects.bulk_create(create_objects, batch_size=batch_size)
            created_count += len(batch)
            pbar.update(len(batch))

        return created_count

    def _bulk_update_objects(self, to_update: List[Dict], existing_objects: Dict, pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        updated_count = 0
        BATCH_UPDATE_SIZE = 500

        for batch in batch_iterator(to_update, BATCH_UPDATE_SIZE):
            with transaction.atomic():
                for data in batch:
                    obj = existing_objects[data['registration_number']]
                    update_fields = []
                    for field, value in data.items():
                        if field != 'registration_number' and getattr(obj, field) != value:
                            setattr(obj, field, value)
                            update_fields.append(field)
                    if update_fields:
                        obj.save(update_fields=update_fields)
                        updated_count += 1
            pbar.update(len(batch))

        return updated_count
```


-----

# –§–∞–π–ª: management\parsers\industrial_design.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –µ–¥–∏–Ω–æ–≥–æ DataFrame –¥–ª—è —Å–≤—è–∑–µ–π
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä year –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ –≥–æ–¥–∞–º
"""

import logging
import gc
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict

import pandas as pd
from django.db import models, transaction
from tqdm import tqdm

from intellectual_property.models import IPObject, IPType
from .base import BaseFIPSParser
from ..utils.progress import batch_iterator

logger = logging.getLogger(__name__)


class IndustrialDesignParser(BaseFIPSParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–≤—è–∑–µ–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–¥–∏–Ω—ã–π DataFrame –¥–ª—è –≤—Å–µ—Ö —Å–≤—è–∑–µ–π (–∞–≤—Ç–æ—Ä—ã + –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏)
    """

    def get_ip_type(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø –†–ò–î 'industrial-design'"""
        return IPType.objects.filter(slug='industrial-design').first()

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è CSV"""
        return ['registration number', 'industrial design name']

    def _has_data_changed(self, obj, new_data):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç–∞
        """
        fields_to_check = [
            ('name', obj.name, new_data['name']),
            ('application_date', obj.application_date, new_data['application_date']),
            ('registration_date', obj.registration_date, new_data['registration_date']),
            ('patent_starting_date', obj.patent_starting_date, new_data['patent_starting_date']),
            ('expiration_date', obj.expiration_date, new_data['expiration_date']),
            ('actual', obj.actual, new_data['actual']),
            ('publication_url', obj.publication_url, new_data['publication_url']),
            ('abstract', obj.abstract, new_data['abstract']),
            ('creation_year', obj.creation_year, new_data['creation_year']),
        ]

        for field_name, old_val, new_val in fields_to_check:
            if old_val != new_val:
                return True
        return False

    def parse_dataframe(self, df, catalogue, year=None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            catalogue: –æ–±—ä–µ–∫—Ç –∫–∞—Ç–∞–ª–æ–≥–∞
            year: –≥–æ–¥ –¥–ª—è —Ç–µ–∫—É—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        year_msg = f" –¥–ª—è {year} –≥–æ–¥–∞" if year else ""
        self.stdout.write(f"\nüîπ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤{year_msg}")

        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –†–ò–î
        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'industrial-design' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats

        upload_date = catalogue.upload_date.date() if catalogue.upload_date else None

        # =====================================================================
        # –®–ê–ì 1: –°–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
        # =====================================================================
        self.stdout.write("üîπ –ß—Ç–µ–Ω–∏–µ CSV –∏ —Å–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        reg_num_to_row = {}
        skipped_empty = 0
        
        for idx, row in df.iterrows():
            reg_num = self.clean_string(row.get('registration number'))
            if reg_num:
                reg_num_to_row[reg_num] = row
            else:
                skipped_empty += 1

        self.stdout.write(f"üîπ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: {len(reg_num_to_row)} (–ø—Ä–æ–ø—É—â–µ–Ω–æ –ø—É—Å—Ç—ã—Ö: {skipped_empty})")

        # =====================================================================
        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î
        # =====================================================================
        self.stdout.write("üîπ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î")
        
        existing_objects = {}
        batch_size = 500
        reg_numbers = list(reg_num_to_row.keys())
        
        with tqdm(total=len(reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—á–∫–∞–º–∏", unit="–∑–∞–ø") as pbar:
            for i in range(0, len(reg_numbers), batch_size):
                batch_numbers = reg_numbers[i:i+batch_size]
                
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_numbers,
                    ip_type=ip_type
                ).select_related('ip_type'):
                    existing_objects[obj.registration_number] = obj
                
                pbar.update(len(batch_numbers))

        self.stdout.write(f"üîπ –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(existing_objects)}")

        # =====================================================================
        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è IPObject
        # =====================================================================
        self.stdout.write("üîπ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject")
        
        to_create = []
        to_update = []
        skipped_by_date = []
        unchanged_count = 0
        error_reg_numbers = []

        relations_data = []
        
        with tqdm(total=len(reg_num_to_row), desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject", unit="–∑–∞–ø") as pbar:
            for reg_num, row in reg_num_to_row.items():
                try:
                    if not self.command.force and upload_date and reg_num in existing_objects:
                        existing = existing_objects[reg_num]
                        if existing.updated_at and existing.updated_at.date() >= upload_date:
                            skipped_by_date.append(reg_num)
                            pbar.update(1)
                            continue

                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                    name = self.clean_string(row.get('industrial design name'))
                    if name:
                        name = self.rid_formatter.format(name)
                    else:
                        name = f"–ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü ‚Ññ{reg_num}"

                    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã
                    application_date = self.parse_date(row.get('application date'))
                    registration_date = self.parse_date(row.get('registration date'))
                    patent_starting_date = self.parse_date(row.get('patent starting date'))
                    expiration_date = self.parse_date(row.get('expiration date'))
                    actual = self.parse_bool(row.get('actual'))
                    publication_url = self.clean_string(row.get('publication URL'))
                    
                    abstract = ''

                    creation_year = None
                    if application_date:
                        creation_year = application_date.year
                    elif registration_date:
                        creation_year = registration_date.year

                    obj_data = {
                        'registration_number': reg_num,
                        'ip_type_id': ip_type.id,
                        'name': name,
                        'application_date': application_date,
                        'registration_date': registration_date,
                        'patent_starting_date': patent_starting_date,
                        'expiration_date': expiration_date,
                        'actual': actual,
                        'publication_url': publication_url,
                        'abstract': abstract,
                        'creation_year': creation_year,
                    }

                    if reg_num in existing_objects:
                        if self._has_data_changed(existing_objects[reg_num], obj_data):
                            to_update.append(obj_data)
                        else:
                            unchanged_count += 1
                    else:
                        to_create.append(obj_data)

                    # –ê–≤—Ç–æ—Ä—ã
                    authors_str = row.get('authors')
                    if not pd.isna(authors_str) and authors_str:
                        authors = self.parse_authors(authors_str)
                        for author in authors:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': author['full_name'],
                                'entity_type': 'person',
                                'relation_type': 'author',
                                'entity_data': author
                            })

                    # –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏
                    holders_str = row.get('patent holders')
                    if not pd.isna(holders_str) and holders_str:
                        holders = self.parse_patent_holders(holders_str)
                        for holder in holders:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': holder,
                                'entity_type': None,
                                'relation_type': 'holder',
                                'entity_data': {'full_name': holder}
                            })

                except Exception as e:
                    error_reg_numbers.append(reg_num)
                    if len(error_reg_numbers) < 10:
                        self.stdout.write(self.style.ERROR(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    elif len(error_reg_numbers) == 10:
                        self.stdout.write(self.style.WARNING("\n‚ö†Ô∏è ... –∏ –¥–∞–ª–µ–µ –æ—à–∏–±–∫–∏ –ø–æ–¥–∞–≤–ª—è—é—Ç—Å—è"))
                    
                    logger.error(f"Error preparing industrial design {reg_num}: {e}", exc_info=True)

                pbar.update(1)

        self.stdout.write(f"üîπ –ò—Ç–æ–≥–æ: –Ω–æ–≤—ã—Ö={len(to_create)}, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ={len(to_update)}, "
                         f"–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={unchanged_count}, –æ—à–∏–±–æ–∫={len(error_reg_numbers)}")

        stats['skipped_by_date'] = len(skipped_by_date)
        stats['skipped'] += len(skipped_by_date)
        stats['errors'] = len(error_reg_numbers)
        stats['unchanged'] = unchanged_count

        # =====================================================================
        # –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ IPObject
        # =====================================================================
        if to_create and not self.command.dry_run:
            self.stdout.write(f"üîπ –°–æ–∑–¥–∞–Ω–∏–µ {len(to_create)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_create), desc="–°–æ–∑–¥–∞–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['created'] = self._bulk_create_objects(to_create, pbar)

        if to_update and not self.command.dry_run:
            self.stdout.write(f"üîπ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {len(to_update)} –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_update), desc="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['updated'] = self._bulk_update_objects(to_update, existing_objects, pbar)

        # =====================================================================
        # –®–ê–ì 5: –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ reg_number -> ip_id
        # =====================================================================
        self.stdout.write("üîπ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        all_reg_numbers = list(set(
            list(existing_objects.keys()) + 
            [data['registration_number'] for data in to_create]
        ))
        
        reg_to_ip = {}
        with tqdm(total=len(all_reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ ID –æ–±—ä–µ–∫—Ç–æ–≤", unit="–∑–∞–ø") as pbar:
            batch_size = 1000
            for i in range(0, len(all_reg_numbers), batch_size):
                batch_nums = all_reg_numbers[i:i+batch_size]
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_nums,
                    ip_type=ip_type
                ).only('id', 'registration_number'):
                    reg_to_ip[obj.registration_number] = obj.id
                pbar.update(len(batch_nums))

        self.stdout.write(f"üîπ –ó–∞–≥—Ä—É–∂–µ–Ω–æ ID –¥–ª—è {len(reg_to_ip)} –æ–±—ä–µ–∫—Ç–æ–≤")

        # =====================================================================
        # –®–ê–ì 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π DataFrame
        # =====================================================================
        if relations_data and not self.command.dry_run:
            self.stdout.write("üîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
            self._process_relations_dataframe(relations_data, reg_to_ip)

        gc.collect()

        stats['processed'] = len(df) - stats['skipped'] - stats['errors']

        year_info = f" –¥–ª—è {year} –≥–æ–¥–∞" if year else ""
        self.stdout.write(self.style.SUCCESS(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤{year_info} –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"   –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats['unchanged']}")
        self.stdout.write(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']} (–∏–∑ –Ω–∏—Ö –ø–æ –¥–∞—Ç–µ: {stats['skipped_by_date']})")
        self.stdout.write(f"   –û—à–∏–±–æ–∫: {stats['errors']}")

        return stats

    def _bulk_create_objects(self, to_create: List[Dict], pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        created_count = 0
        batch_size = 1000

        for batch in batch_iterator(to_create, batch_size):
            create_objects = [IPObject(**data) for data in batch]
            IPObject.objects.bulk_create(create_objects, batch_size=batch_size)
            created_count += len(batch)
            pbar.update(len(batch))

        return created_count

    def _bulk_update_objects(self, to_update: List[Dict], existing_objects: Dict, pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        updated_count = 0
        BATCH_UPDATE_SIZE = 500

        for batch in batch_iterator(to_update, BATCH_UPDATE_SIZE):
            with transaction.atomic():
                for data in batch:
                    obj = existing_objects[data['registration_number']]
                    update_fields = []
                    for field, value in data.items():
                        if field != 'registration_number' and getattr(obj, field) != value:
                            setattr(obj, field, value)
                            update_fields.append(field)
                    if update_fields:
                        obj.save(update_fields=update_fields)
                        updated_count += 1
            pbar.update(len(batch))

        return updated_count
```


-----

# –§–∞–π–ª: management\parsers\integrated_circuit.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –µ–¥–∏–Ω–æ–≥–æ DataFrame –¥–ª—è —Å–≤—è–∑–µ–π
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä year –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ –≥–æ–¥–∞–º
"""

import logging
import gc
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict
import re

import pandas as pd
from django.db import models, transaction
from django.utils.text import slugify
from tqdm import tqdm

from intellectual_property.models import IPObject, IPType, Person
from core.models import Organization, Country

from .base import BaseFIPSParser
from ..utils.progress import batch_iterator

logger = logging.getLogger(__name__)


class IntegratedCircuitTopologyParser(BaseFIPSParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–≤—è–∑–µ–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–¥–∏–Ω—ã–π DataFrame –¥–ª—è –≤—Å–µ—Ö —Å–≤—è–∑–µ–π (–∞–≤—Ç–æ—Ä—ã + –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏)
    """

    def get_ip_type(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø –†–ò–î 'integrated-circuit-topology'"""
        return IPType.objects.filter(slug='integrated-circuit-topology').first()

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è CSV"""
        return ['registration number', 'microchip name']

    def _has_data_changed(self, obj, new_data):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç–∞
        """
        fields_to_check = [
            ('name', obj.name, new_data['name']),
            ('application_date', obj.application_date, new_data['application_date']),
            ('registration_date', obj.registration_date, new_data['registration_date']),
            ('expiration_date', obj.expiration_date, new_data['expiration_date']),
            ('actual', obj.actual, new_data['actual']),
            ('publication_url', obj.publication_url, new_data['publication_url']),
            ('creation_year', obj.creation_year, new_data['creation_year']),
            ('first_usage_date', obj.first_usage_date, new_data.get('first_usage_date')),
        ]

        for field_name, old_val, new_val in fields_to_check:
            if old_val != new_val:
                return True
        return False

    def parse_dataframe(self, df, catalogue, year=None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            catalogue: –æ–±—ä–µ–∫—Ç –∫–∞—Ç–∞–ª–æ–≥–∞
            year: –≥–æ–¥ –¥–ª—è —Ç–µ–∫—É—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        year_msg = f" –¥–ª—è {year} –≥–æ–¥–∞" if year else ""
        self.stdout.write(f"\nüîπ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º{year_msg}")

        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –†–ò–î
        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'integrated-circuit-topology' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats

        upload_date = catalogue.upload_date.date() if catalogue.upload_date else None

        # =====================================================================
        # –®–ê–ì 1: –°–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
        # =====================================================================
        self.stdout.write("üîπ –ß—Ç–µ–Ω–∏–µ CSV –∏ —Å–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        reg_num_to_row = {}
        skipped_empty = 0
        
        for idx, row in df.iterrows():
            reg_num = self.clean_string(row.get('registration number'))
            if reg_num:
                reg_num_to_row[reg_num] = row
            else:
                skipped_empty += 1

        self.stdout.write(f"üîπ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: {len(reg_num_to_row)} (–ø—Ä–æ–ø—É—â–µ–Ω–æ –ø—É—Å—Ç—ã—Ö: {skipped_empty})")

        # =====================================================================
        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î
        # =====================================================================
        self.stdout.write("üîπ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î")
        
        existing_objects = {}
        batch_size = 500
        reg_numbers = list(reg_num_to_row.keys())
        
        with tqdm(total=len(reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—á–∫–∞–º–∏", unit="–∑–∞–ø") as pbar:
            for i in range(0, len(reg_numbers), batch_size):
                batch_numbers = reg_numbers[i:i+batch_size]
                
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_numbers,
                    ip_type=ip_type
                ).select_related('ip_type'):
                    existing_objects[obj.registration_number] = obj
                
                pbar.update(len(batch_numbers))

        self.stdout.write(f"üîπ –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(existing_objects)}")

        # =====================================================================
        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è IPObject
        # =====================================================================
        self.stdout.write("üîπ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject")
        
        to_create = []
        to_update = []
        skipped_by_date = []
        unchanged_count = 0
        error_reg_numbers = []

        relations_data = []
        first_usage_countries_data = []
        
        with tqdm(total=len(reg_num_to_row), desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject", unit="–∑–∞–ø") as pbar:
            for reg_num, row in reg_num_to_row.items():
                try:
                    if not self.command.force and upload_date and reg_num in existing_objects:
                        existing = existing_objects[reg_num]
                        if existing.updated_at and existing.updated_at.date() >= upload_date:
                            skipped_by_date.append(reg_num)
                            pbar.update(1)
                            continue

                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                    name = self.clean_string(row.get('microchip name'))
                    if name:
                        name = self.rid_formatter.format(name)
                    else:
                        name = f"–¢–æ–ø–æ–ª–æ–≥–∏—è –ò–ú–° ‚Ññ{reg_num}"

                    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã
                    application_date = self.parse_date(row.get('application date'))
                    registration_date = self.parse_date(row.get('registration date'))
                    expiration_date = self.parse_date(row.get('expiration date'))
                    actual = self.parse_bool(row.get('actual'))
                    publication_url = self.clean_string(row.get('publication URL'))
                    
                    first_usage_date = self.parse_date(row.get('first usage date'))
                    
                    creation_year = None
                    if application_date:
                        creation_year = application_date.year
                    elif registration_date:
                        creation_year = registration_date.year

                    obj_data = {
                        'registration_number': reg_num,
                        'ip_type_id': ip_type.id,
                        'name': name,
                        'application_date': application_date,
                        'registration_date': registration_date,
                        'expiration_date': expiration_date,
                        'actual': actual,
                        'publication_url': publication_url,
                        'creation_year': creation_year,
                        'first_usage_date': first_usage_date,
                    }

                    if reg_num in existing_objects:
                        if self._has_data_changed(existing_objects[reg_num], obj_data):
                            to_update.append(obj_data)
                        else:
                            unchanged_count += 1
                    else:
                        to_create.append(obj_data)

                    # –ê–≤—Ç–æ—Ä—ã
                    authors_str = row.get('authors')
                    if not pd.isna(authors_str) and authors_str:
                        authors = self.parse_authors(authors_str)
                        for author in authors:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': author['full_name'],
                                'entity_type': 'person',
                                'relation_type': 'author',
                                'entity_data': author
                            })

                    # –ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏
                    holders_str = row.get('right holders')
                    if not pd.isna(holders_str) and holders_str:
                        holders = self._parse_right_holders(holders_str)
                        for holder in holders:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': holder,
                                'entity_type': None,
                                'relation_type': 'holder',
                                'entity_data': {'full_name': holder}
                            })

                    # –°—Ç—Ä–∞–Ω—ã –ø–µ—Ä–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                    countries_str = row.get('first usage countries')
                    if not pd.isna(countries_str) and countries_str and countries_str.lower() != '–Ω–µ—Ç':
                        countries = self._parse_first_usage_countries(countries_str)
                        for country_code in countries:
                            first_usage_countries_data.append({
                                'reg_number': reg_num,
                                'country_code': country_code
                            })

                except Exception as e:
                    error_reg_numbers.append(reg_num)
                    if len(error_reg_numbers) < 10:
                        self.stdout.write(self.style.ERROR(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    elif len(error_reg_numbers) == 10:
                        self.stdout.write(self.style.WARNING("\n‚ö†Ô∏è ... –∏ –¥–∞–ª–µ–µ –æ—à–∏–±–∫–∏ –ø–æ–¥–∞–≤–ª—è—é—Ç—Å—è"))
                    
                    logger.error(f"Error preparing integrated circuit topology {reg_num}: {e}", exc_info=True)

                pbar.update(1)

        self.stdout.write(f"üîπ –ò—Ç–æ–≥–æ: –Ω–æ–≤—ã—Ö={len(to_create)}, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ={len(to_update)}, "
                         f"–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={unchanged_count}, –æ—à–∏–±–æ–∫={len(error_reg_numbers)}")

        stats['skipped_by_date'] = len(skipped_by_date)
        stats['skipped'] += len(skipped_by_date)
        stats['errors'] = len(error_reg_numbers)
        stats['unchanged'] = unchanged_count

        # =====================================================================
        # –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ IPObject
        # =====================================================================
        if to_create and not self.command.dry_run:
            self.stdout.write(f"üîπ –°–æ–∑–¥–∞–Ω–∏–µ {len(to_create)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_create), desc="–°–æ–∑–¥–∞–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['created'] = self._bulk_create_objects(to_create, pbar)

        if to_update and not self.command.dry_run:
            self.stdout.write(f"üîπ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {len(to_update)} –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_update), desc="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['updated'] = self._bulk_update_objects(to_update, existing_objects, pbar)

        # =====================================================================
        # –®–ê–ì 5: –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ reg_number -> ip_id
        # =====================================================================
        self.stdout.write("üîπ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        all_reg_numbers = list(set(
            list(existing_objects.keys()) + 
            [data['registration_number'] for data in to_create]
        ))
        
        reg_to_ip = {}
        with tqdm(total=len(all_reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ ID –æ–±—ä–µ–∫—Ç–æ–≤", unit="–∑–∞–ø") as pbar:
            batch_size = 1000
            for i in range(0, len(all_reg_numbers), batch_size):
                batch_nums = all_reg_numbers[i:i+batch_size]
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_nums,
                    ip_type=ip_type
                ).only('id', 'registration_number'):
                    reg_to_ip[obj.registration_number] = obj.id
                pbar.update(len(batch_nums))

        self.stdout.write(f"üîπ –ó–∞–≥—Ä—É–∂–µ–Ω–æ ID –¥–ª—è {len(reg_to_ip)} –æ–±—ä–µ–∫—Ç–æ–≤")

        # =====================================================================
        # –®–ê–ì 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π DataFrame
        # =====================================================================
        if relations_data and not self.command.dry_run:
            self.stdout.write("üîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π")
            self._process_relations_dataframe(relations_data, reg_to_ip)

        # =====================================================================
        # –®–ê–ì 7: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω –ø–µ—Ä–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        # =====================================================================
        if first_usage_countries_data and not self.command.dry_run:
            self.stdout.write("üîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω –ø–µ—Ä–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
            self._process_first_usage_countries(first_usage_countries_data, reg_to_ip)

        gc.collect()

        stats['processed'] = len(df) - stats['skipped'] - stats['errors']

        year_info = f" –¥–ª—è {year} –≥–æ–¥–∞" if year else ""
        self.stdout.write(self.style.SUCCESS(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö –º–∏–∫—Ä–æ—Å—Ö–µ–º{year_info} –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"   –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats['unchanged']}")
        self.stdout.write(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']} (–∏–∑ –Ω–∏—Ö –ø–æ –¥–∞—Ç–µ: {stats['skipped_by_date']})")
        self.stdout.write(f"   –û—à–∏–±–æ–∫: {stats['errors']}")

        return stats

    def _parse_right_holders(self, holders_str: str) -> List[str]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—è–º–∏ –¥–ª—è —Ç–æ–ø–æ–ª–æ–≥–∏–π –ò–ú–°
        """
        if pd.isna(holders_str) or not holders_str:
            return []
        
        holders_str = str(holders_str)
        holders_list = re.split(r'[\n]\s*', holders_str)
        
        result = []
        for holder in holders_list:
            holder = holder.strip().strip('"')
            if not holder or holder == 'null' or holder == 'None' or holder.lower() == '–Ω–µ—Ç':
                continue
            holder = re.sub(r'\s*\([A-Z]{2}\)$', '', holder)
            result.append(holder)
        
        return result

    def _parse_first_usage_countries(self, countries_str: str) -> List[str]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∞–º–∏ –ø–µ—Ä–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        """
        if pd.isna(countries_str) or not countries_str:
            return []
        
        countries_str = str(countries_str)
        if countries_str.lower() == '–Ω–µ—Ç':
            return []
        
        countries = re.split(r'[,\s]+', countries_str)
        
        result = []
        country_map = {
            '–†–§': 'RU',
            '–†–æ—Å—Å–∏—è': 'RU',
            '–†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è': 'RU',
            'RU': 'RU',
            'RUS': 'RU',
        }
        
        for country in countries:
            country = country.strip()
            if not country:
                continue
            
            if country in country_map:
                result.append(country_map[country])
            elif len(country) == 2 and country.isupper():
                result.append(country)
            else:
                country_obj = Country.objects.filter(name__icontains=country).first()
                if country_obj:
                    result.append(country_obj.code)
                else:
                    self.stdout.write(self.style.WARNING(f"      ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥ —Å—Ç—Ä–∞–Ω—ã: {country}"))
        
        return list(set(result))

    def _process_first_usage_countries(self, countries_data: List[Dict], reg_to_ip: Dict):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π —Å–æ —Å—Ç—Ä–∞–Ω–∞–º–∏ –ø–µ—Ä–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        """
        if not countries_data:
            return
        
        self.stdout.write("   –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤—è–∑–µ–π —Å–æ —Å—Ç—Ä–∞–Ω–∞–º–∏ –ø–µ—Ä–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
        
        reg_to_countries = defaultdict(set)
        for item in countries_data:
            ip_id = reg_to_ip.get(item['reg_number'])
            if ip_id:
                reg_to_countries[ip_id].add(item['country_code'])
        
        if not reg_to_countries:
            return
        
        country_codes = set()
        for countries in reg_to_countries.values():
            country_codes.update(countries)
        
        country_map = {}
        for code in country_codes:
            country = self.get_or_create_country(code)
            if country:
                country_map[code] = country
        
        ip_ids = list(reg_to_countries.keys())
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏
        with tqdm(total=len(ip_ids), desc="   –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π —Å–æ —Å—Ç—Ä–∞–Ω–∞–º–∏", unit="ip") as pbar:
            delete_batch_size = 500
            for i in range(0, len(ip_ids), delete_batch_size):
                batch_ids = ip_ids[i:i+delete_batch_size]
                IPObject.first_usage_countries.through.objects.filter(
                    ipobject_id__in=batch_ids
                ).delete()
                pbar.update(len(batch_ids))
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∏
        through_objs = []
        for ip_id, country_codes in reg_to_countries.items():
            for code in country_codes:
                country = country_map.get(code)
                if country:
                    through_objs.append(
                        IPObject.first_usage_countries.through(
                            ipobject_id=ip_id,
                            country_id=country.id
                        )
                    )
        
        if through_objs:
            with tqdm(total=len(through_objs), desc="   –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å–æ —Å—Ç—Ä–∞–Ω–∞–º–∏", unit="—Å–≤") as pbar:
                create_batch_size = 2000
                for i in range(0, len(through_objs), create_batch_size):
                    batch = through_objs[i:i+create_batch_size]
                    IPObject.first_usage_countries.through.objects.bulk_create(
                        batch, batch_size=create_batch_size, ignore_conflicts=True
                    )
                    pbar.update(len(batch))
        
        self.stdout.write("   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω –ø–µ—Ä–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def _bulk_create_objects(self, to_create: List[Dict], pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        created_count = 0
        batch_size = 1000

        for batch in batch_iterator(to_create, batch_size):
            create_objects = [IPObject(**data) for data in batch]
            IPObject.objects.bulk_create(create_objects, batch_size=batch_size)
            created_count += len(batch)
            pbar.update(len(batch))

        return created_count

    def _bulk_update_objects(self, to_update: List[Dict], existing_objects: Dict, pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        updated_count = 0
        BATCH_UPDATE_SIZE = 500

        for batch in batch_iterator(to_update, BATCH_UPDATE_SIZE):
            with transaction.atomic():
                for data in batch:
                    obj = existing_objects[data['registration_number']]
                    update_fields = []
                    for field, value in data.items():
                        if field != 'registration_number' and getattr(obj, field) != value:
                            setattr(obj, field, value)
                            update_fields.append(field)
                    if update_fields:
                        obj.save(update_fields=update_fields)
                        updated_count += 1
            pbar.update(len(batch))

        return updated_count
```


-----

# –§–∞–π–ª: management\parsers\invention.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –µ–¥–∏–Ω–æ–≥–æ DataFrame –¥–ª—è —Å–≤—è–∑–µ–π
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä year –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ –≥–æ–¥–∞–º
"""

import logging
import gc
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict

import pandas as pd
from django.db import models, transaction
from tqdm import tqdm

from intellectual_property.models import IPObject, IPType
from .base import BaseFIPSParser
from ..utils.progress import batch_iterator

logger = logging.getLogger(__name__)


class InventionParser(BaseFIPSParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–≤—è–∑–µ–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–¥–∏–Ω—ã–π DataFrame –¥–ª—è –≤—Å–µ—Ö —Å–≤—è–∑–µ–π (–∞–≤—Ç–æ—Ä—ã + –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏)
    """

    def get_ip_type(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø –†–ò–î 'invention'"""
        return IPType.objects.filter(slug='invention').first()

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è CSV"""
        return ['registration number', 'invention name']

    def _has_data_changed(self, obj, new_data):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç–∞
        """
        fields_to_check = [
            ('name', obj.name, new_data['name']),
            ('application_date', obj.application_date, new_data['application_date']),
            ('registration_date', obj.registration_date, new_data['registration_date']),
            ('patent_starting_date', obj.patent_starting_date, new_data['patent_starting_date']),
            ('expiration_date', obj.expiration_date, new_data['expiration_date']),
            ('actual', obj.actual, new_data['actual']),
            ('publication_url', obj.publication_url, new_data['publication_url']),
            ('abstract', obj.abstract, new_data['abstract']),
            ('claims', obj.claims, new_data['claims']),
            ('creation_year', obj.creation_year, new_data['creation_year']),
        ]

        for field_name, old_val, new_val in fields_to_check:
            if old_val != new_val:
                return True
        return False

    def parse_dataframe(self, df, catalogue, year=None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            catalogue: –æ–±—ä–µ–∫—Ç –∫–∞—Ç–∞–ª–æ–≥–∞
            year: –≥–æ–¥ –¥–ª—è —Ç–µ–∫—É—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        year_msg = f" –¥–ª—è {year} –≥–æ–¥–∞" if year else ""
        self.stdout.write(f"\nüîπ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π{year_msg}")

        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –†–ò–î
        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'invention' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats

        upload_date = catalogue.upload_date.date() if catalogue.upload_date else None

        # =====================================================================
        # –®–ê–ì 1: –°–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
        # =====================================================================
        self.stdout.write("üîπ –ß—Ç–µ–Ω–∏–µ CSV –∏ —Å–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        reg_num_to_row = {}
        skipped_empty = 0
        
        for idx, row in df.iterrows():
            reg_num = self.clean_string(row.get('registration number'))
            if reg_num:
                reg_num_to_row[reg_num] = row
            else:
                skipped_empty += 1

        self.stdout.write(f"üîπ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: {len(reg_num_to_row)} (–ø—Ä–æ–ø—É—â–µ–Ω–æ –ø—É—Å—Ç—ã—Ö: {skipped_empty})")

        # =====================================================================
        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î
        # =====================================================================
        self.stdout.write("üîπ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î")
        
        existing_objects = {}
        batch_size = 500
        reg_numbers = list(reg_num_to_row.keys())
        
        with tqdm(total=len(reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—á–∫–∞–º–∏", unit="–∑–∞–ø") as pbar:
            for i in range(0, len(reg_numbers), batch_size):
                batch_numbers = reg_numbers[i:i+batch_size]
                
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_numbers,
                    ip_type=ip_type
                ).select_related('ip_type'):
                    existing_objects[obj.registration_number] = obj
                
                pbar.update(len(batch_numbers))

        self.stdout.write(f"üîπ –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(existing_objects)}")

        # =====================================================================
        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è IPObject
        # =====================================================================
        self.stdout.write("üîπ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject")
        
        to_create = []
        to_update = []
        skipped_by_date = []
        unchanged_count = 0
        error_reg_numbers = []

        relations_data = []
        
        with tqdm(total=len(reg_num_to_row), desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject", unit="–∑–∞–ø") as pbar:
            for reg_num, row in reg_num_to_row.items():
                try:
                    if not self.command.force and upload_date and reg_num in existing_objects:
                        existing = existing_objects[reg_num]
                        if existing.updated_at and existing.updated_at.date() >= upload_date:
                            skipped_by_date.append(reg_num)
                            pbar.update(1)
                            continue

                    name = self.clean_string(row.get('invention name'))
                    if name:
                        name = self.rid_formatter.format(name)
                    else:
                        name = f"–ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ ‚Ññ{reg_num}"

                    application_date = self.parse_date(row.get('application date'))
                    registration_date = self.parse_date(row.get('registration date'))
                    patent_starting_date = self.parse_date(row.get('patent starting date'))
                    expiration_date = self.parse_date(row.get('expiration date'))
                    actual = self.parse_bool(row.get('actual'))
                    publication_url = self.clean_string(row.get('publication URL'))
                    abstract = self.clean_string(row.get('abstract'))
                    claims = self.clean_string(row.get('claims'))

                    creation_year = None
                    if application_date:
                        creation_year = application_date.year
                    elif registration_date:
                        creation_year = registration_date.year

                    obj_data = {
                        'registration_number': reg_num,
                        'ip_type_id': ip_type.id,
                        'name': name,
                        'application_date': application_date,
                        'registration_date': registration_date,
                        'patent_starting_date': patent_starting_date,
                        'expiration_date': expiration_date,
                        'actual': actual,
                        'publication_url': publication_url,
                        'abstract': abstract,
                        'claims': claims,
                        'creation_year': creation_year,
                    }

                    if reg_num in existing_objects:
                        if self._has_data_changed(existing_objects[reg_num], obj_data):
                            to_update.append(obj_data)
                        else:
                            unchanged_count += 1
                    else:
                        to_create.append(obj_data)

                    # –ê–≤—Ç–æ—Ä—ã
                    authors_str = row.get('authors')
                    if not pd.isna(authors_str) and authors_str:
                        authors = self.parse_authors(authors_str)
                        for author in authors:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': author['full_name'],
                                'entity_type': 'person',
                                'relation_type': 'author',
                                'entity_data': author
                            })

                    # –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏
                    holders_str = row.get('patent holders')
                    if not pd.isna(holders_str) and holders_str:
                        holders = self.parse_patent_holders(holders_str)
                        for holder in holders:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': holder,
                                'entity_type': None,
                                'relation_type': 'holder',
                                'entity_data': {'full_name': holder}
                            })

                except Exception as e:
                    error_reg_numbers.append(reg_num)
                    if len(error_reg_numbers) < 10:
                        self.stdout.write(self.style.ERROR(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    elif len(error_reg_numbers) == 10:
                        self.stdout.write(self.style.WARNING("\n‚ö†Ô∏è ... –∏ –¥–∞–ª–µ–µ –æ—à–∏–±–∫–∏ –ø–æ–¥–∞–≤–ª—è—é—Ç—Å—è"))
                    
                    logger.error(f"Error preparing invention {reg_num}: {e}", exc_info=True)

                pbar.update(1)

        self.stdout.write(f"üîπ –ò—Ç–æ–≥–æ: –Ω–æ–≤—ã—Ö={len(to_create)}, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ={len(to_update)}, "
                         f"–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={unchanged_count}, –æ—à–∏–±–æ–∫={len(error_reg_numbers)}")

        stats['skipped_by_date'] = len(skipped_by_date)
        stats['skipped'] += len(skipped_by_date)
        stats['errors'] = len(error_reg_numbers)
        stats['unchanged'] = unchanged_count

        # =====================================================================
        # –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ IPObject
        # =====================================================================
        if to_create and not self.command.dry_run:
            self.stdout.write(f"üîπ –°–æ–∑–¥–∞–Ω–∏–µ {len(to_create)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_create), desc="–°–æ–∑–¥–∞–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['created'] = self._bulk_create_objects(to_create, pbar)

        if to_update and not self.command.dry_run:
            self.stdout.write(f"üîπ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {len(to_update)} –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_update), desc="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['updated'] = self._bulk_update_objects(to_update, existing_objects, pbar)

        # =====================================================================
        # –®–ê–ì 5: –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ reg_number -> ip_id
        # =====================================================================
        self.stdout.write("üîπ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        all_reg_numbers = list(set(
            list(existing_objects.keys()) + 
            [data['registration_number'] for data in to_create]
        ))
        
        reg_to_ip = {}
        with tqdm(total=len(all_reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ ID –æ–±—ä–µ–∫—Ç–æ–≤", unit="–∑–∞–ø") as pbar:
            batch_size = 1000
            for i in range(0, len(all_reg_numbers), batch_size):
                batch_nums = all_reg_numbers[i:i+batch_size]
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_nums,
                    ip_type=ip_type
                ).only('id', 'registration_number'):
                    reg_to_ip[obj.registration_number] = obj.id
                pbar.update(len(batch_nums))

        self.stdout.write(f"üîπ –ó–∞–≥—Ä—É–∂–µ–Ω–æ ID –¥–ª—è {len(reg_to_ip)} –æ–±—ä–µ–∫—Ç–æ–≤")

        # =====================================================================
        # –®–ê–ì 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π DataFrame
        # =====================================================================
        if relations_data and not self.command.dry_run:
            self.stdout.write("üîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π")
            self._process_relations_dataframe(relations_data, reg_to_ip)

        gc.collect()

        stats['processed'] = len(df) - stats['skipped'] - stats['errors']

        year_info = f" –¥–ª—è {year} –≥–æ–¥–∞" if year else ""
        self.stdout.write(self.style.SUCCESS(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π{year_info} –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"   –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats['unchanged']}")
        self.stdout.write(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']} (–∏–∑ –Ω–∏—Ö –ø–æ –¥–∞—Ç–µ: {stats['skipped_by_date']})")
        self.stdout.write(f"   –û—à–∏–±–æ–∫: {stats['errors']}")

        return stats

    def _bulk_create_objects(self, to_create: List[Dict], pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        created_count = 0
        batch_size = 1000

        for batch in batch_iterator(to_create, batch_size):
            create_objects = [IPObject(**data) for data in batch]
            IPObject.objects.bulk_create(create_objects, batch_size=batch_size)
            created_count += len(batch)
            pbar.update(len(batch))

        return created_count

    def _bulk_update_objects(self, to_update: List[Dict], existing_objects: Dict, pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        updated_count = 0
        BATCH_UPDATE_SIZE = 500

        for batch in batch_iterator(to_update, BATCH_UPDATE_SIZE):
            with transaction.atomic():
                for data in batch:
                    obj = existing_objects[data['registration_number']]
                    update_fields = []
                    for field, value in data.items():
                        if field != 'registration_number' and getattr(obj, field) != value:
                            setattr(obj, field, value)
                            update_fields.append(field)
                    if update_fields:
                        obj.save(update_fields=update_fields)
                        updated_count += 1
            pbar.update(len(batch))

        return updated_count
```


-----

# –§–∞–π–ª: management\parsers\utility_model.py

```
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –µ–¥–∏–Ω–æ–≥–æ DataFrame –¥–ª—è —Å–≤—è–∑–µ–π
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä year –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ –≥–æ–¥–∞–º
"""

import logging
import gc
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict

import pandas as pd
from django.db import models, transaction
from tqdm import tqdm

from intellectual_property.models import IPObject, IPType
from .base import BaseFIPSParser
from ..utils.progress import batch_iterator

logger = logging.getLogger(__name__)


class UtilityModelParser(BaseFIPSParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–≤—è–∑–µ–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–¥–∏–Ω—ã–π DataFrame –¥–ª—è –≤—Å–µ—Ö —Å–≤—è–∑–µ–π (–∞–≤—Ç–æ—Ä—ã + –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏)
    """

    def get_ip_type(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø –†–ò–î 'utility-model'"""
        return IPType.objects.filter(slug='utility-model').first()

    def get_required_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è CSV"""
        return ['registration number', 'utility model name']

    def _has_data_changed(self, obj, new_data):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç–∞
        """
        fields_to_check = [
            ('name', obj.name, new_data['name']),
            ('application_date', obj.application_date, new_data['application_date']),
            ('registration_date', obj.registration_date, new_data['registration_date']),
            ('patent_starting_date', obj.patent_starting_date, new_data['patent_starting_date']),
            ('expiration_date', obj.expiration_date, new_data['expiration_date']),
            ('actual', obj.actual, new_data['actual']),
            ('publication_url', obj.publication_url, new_data['publication_url']),
            ('abstract', obj.abstract, new_data['abstract']),
            ('claims', obj.claims, new_data['claims']),
            ('creation_year', obj.creation_year, new_data['creation_year']),
        ]

        for field_name, old_val, new_val in fields_to_check:
            if old_val != new_val:
                return True
        return False

    def parse_dataframe(self, df, catalogue, year=None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ DataFrame
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            catalogue: –æ–±—ä–µ–∫—Ç –∫–∞—Ç–∞–ª–æ–≥–∞
            year: –≥–æ–¥ –¥–ª—è —Ç–µ–∫—É—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        year_msg = f" –¥–ª—è {year} –≥–æ–¥–∞" if year else ""
        self.stdout.write(f"\nüîπ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π{year_msg}")

        stats = {
            'processed': 0,
            'created': 0,
            'updated': 0,
            'unchanged': 0,
            'skipped': 0,
            'skipped_by_date': 0,
            'errors': 0
        }

        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –†–ò–î
        ip_type = self.get_ip_type()
        if not ip_type:
            self.stdout.write(self.style.ERROR("  ‚ùå –¢–∏–ø –†–ò–î 'utility-model' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"))
            stats['errors'] += 1
            return stats

        upload_date = catalogue.upload_date.date() if catalogue.upload_date else None

        # =====================================================================
        # –®–ê–ì 1: –°–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
        # =====================================================================
        self.stdout.write("üîπ –ß—Ç–µ–Ω–∏–µ CSV –∏ —Å–±–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        reg_num_to_row = {}
        skipped_empty = 0
        
        for idx, row in df.iterrows():
            reg_num = self.clean_string(row.get('registration number'))
            if reg_num:
                reg_num_to_row[reg_num] = row
            else:
                skipped_empty += 1

        self.stdout.write(f"üîπ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ CSV: {len(reg_num_to_row)} (–ø—Ä–æ–ø—É—â–µ–Ω–æ –ø—É—Å—Ç—ã—Ö: {skipped_empty})")

        # =====================================================================
        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î
        # =====================================================================
        self.stdout.write("üîπ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î")
        
        existing_objects = {}
        batch_size = 500
        reg_numbers = list(reg_num_to_row.keys())
        
        with tqdm(total=len(reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—á–∫–∞–º–∏", unit="–∑–∞–ø") as pbar:
            for i in range(0, len(reg_numbers), batch_size):
                batch_numbers = reg_numbers[i:i+batch_size]
                
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_numbers,
                    ip_type=ip_type
                ).select_related('ip_type'):
                    existing_objects[obj.registration_number] = obj
                
                pbar.update(len(batch_numbers))

        self.stdout.write(f"üîπ –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(existing_objects)}")

        # =====================================================================
        # –®–ê–ì 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è IPObject
        # =====================================================================
        self.stdout.write("üîπ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject")
        
        to_create = []
        to_update = []
        skipped_by_date = []
        unchanged_count = 0
        error_reg_numbers = []

        relations_data = []
        
        with tqdm(total=len(reg_num_to_row), desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö IPObject", unit="–∑–∞–ø") as pbar:
            for reg_num, row in reg_num_to_row.items():
                try:
                    if not self.command.force and upload_date and reg_num in existing_objects:
                        existing = existing_objects[reg_num]
                        if existing.updated_at and existing.updated_at.date() >= upload_date:
                            skipped_by_date.append(reg_num)
                            pbar.update(1)
                            continue

                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                    name = self.clean_string(row.get('utility model name'))
                    if name:
                        name = self.rid_formatter.format(name)
                    else:
                        name = f"–ü–æ–ª–µ–∑–Ω–∞—è –º–æ–¥–µ–ª—å ‚Ññ{reg_num}"

                    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã
                    application_date = self.parse_date(row.get('application date'))
                    registration_date = self.parse_date(row.get('registration date'))
                    patent_starting_date = self.parse_date(row.get('patent starting date'))
                    expiration_date = self.parse_date(row.get('expiration date'))
                    actual = self.parse_bool(row.get('actual'))
                    publication_url = self.clean_string(row.get('publication URL'))
                    
                    abstract = self.clean_string(row.get('abstract', ''))
                    claims = self.clean_string(row.get('claims', ''))

                    creation_year = None
                    if application_date:
                        creation_year = application_date.year
                    elif registration_date:
                        creation_year = registration_date.year

                    obj_data = {
                        'registration_number': reg_num,
                        'ip_type_id': ip_type.id,
                        'name': name,
                        'application_date': application_date,
                        'registration_date': registration_date,
                        'patent_starting_date': patent_starting_date,
                        'expiration_date': expiration_date,
                        'actual': actual,
                        'publication_url': publication_url,
                        'abstract': abstract,
                        'claims': claims,
                        'creation_year': creation_year,
                    }

                    if reg_num in existing_objects:
                        if self._has_data_changed(existing_objects[reg_num], obj_data):
                            to_update.append(obj_data)
                        else:
                            unchanged_count += 1
                    else:
                        to_create.append(obj_data)

                    # –ê–≤—Ç–æ—Ä—ã
                    authors_str = row.get('authors')
                    if not pd.isna(authors_str) and authors_str:
                        authors = self.parse_authors(authors_str)
                        for author in authors:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': author['full_name'],
                                'entity_type': 'person',
                                'relation_type': 'author',
                                'entity_data': author
                            })

                    # –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–∏
                    holders_str = row.get('patent holders')
                    if not pd.isna(holders_str) and holders_str:
                        holders = self.parse_patent_holders(holders_str)
                        for holder in holders:
                            relations_data.append({
                                'reg_number': reg_num,
                                'entity_name': holder,
                                'entity_type': None,
                                'relation_type': 'holder',
                                'entity_data': {'full_name': holder}
                            })

                except Exception as e:
                    error_reg_numbers.append(reg_num)
                    if len(error_reg_numbers) < 10:
                        self.stdout.write(self.style.ERROR(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏ {reg_num}: {e}"))
                    elif len(error_reg_numbers) == 10:
                        self.stdout.write(self.style.WARNING("\n‚ö†Ô∏è ... –∏ –¥–∞–ª–µ–µ –æ—à–∏–±–∫–∏ –ø–æ–¥–∞–≤–ª—è—é—Ç—Å—è"))
                    
                    logger.error(f"Error preparing utility model {reg_num}: {e}", exc_info=True)

                pbar.update(1)

        self.stdout.write(f"üîπ –ò—Ç–æ–≥–æ: –Ω–æ–≤—ã—Ö={len(to_create)}, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ={len(to_update)}, "
                         f"–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π={unchanged_count}, –æ—à–∏–±–æ–∫={len(error_reg_numbers)}")

        stats['skipped_by_date'] = len(skipped_by_date)
        stats['skipped'] += len(skipped_by_date)
        stats['errors'] = len(error_reg_numbers)
        stats['unchanged'] = unchanged_count

        # =====================================================================
        # –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ IPObject
        # =====================================================================
        if to_create and not self.command.dry_run:
            self.stdout.write(f"üîπ –°–æ–∑–¥–∞–Ω–∏–µ {len(to_create)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_create), desc="–°–æ–∑–¥–∞–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['created'] = self._bulk_create_objects(to_create, pbar)

        if to_update and not self.command.dry_run:
            self.stdout.write(f"üîπ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {len(to_update)} –∑–∞–ø–∏—Å–µ–π")
            with tqdm(total=len(to_update), desc="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ", unit="–∑–∞–ø") as pbar:
                stats['updated'] = self._bulk_update_objects(to_update, existing_objects, pbar)

        # =====================================================================
        # –®–ê–ì 5: –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ reg_number -> ip_id
        # =====================================================================
        self.stdout.write("üîπ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
        
        all_reg_numbers = list(set(
            list(existing_objects.keys()) + 
            [data['registration_number'] for data in to_create]
        ))
        
        reg_to_ip = {}
        with tqdm(total=len(all_reg_numbers), desc="–ó–∞–≥—Ä—É–∑–∫–∞ ID –æ–±—ä–µ–∫—Ç–æ–≤", unit="–∑–∞–ø") as pbar:
            batch_size = 1000
            for i in range(0, len(all_reg_numbers), batch_size):
                batch_nums = all_reg_numbers[i:i+batch_size]
                for obj in IPObject.objects.filter(
                    registration_number__in=batch_nums,
                    ip_type=ip_type
                ).only('id', 'registration_number'):
                    reg_to_ip[obj.registration_number] = obj.id
                pbar.update(len(batch_nums))

        self.stdout.write(f"üîπ –ó–∞–≥—Ä—É–∂–µ–Ω–æ ID –¥–ª—è {len(reg_to_ip)} –æ–±—ä–µ–∫—Ç–æ–≤")

        # =====================================================================
        # –®–ê–ì 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π DataFrame
        # =====================================================================
        if relations_data and not self.command.dry_run:
            self.stdout.write("üîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–µ–π")
            self._process_relations_dataframe(relations_data, reg_to_ip)

        gc.collect()

        stats['processed'] = len(df) - stats['skipped'] - stats['errors']

        year_info = f" –¥–ª—è {year} –≥–æ–¥–∞" if year else ""
        self.stdout.write(self.style.SUCCESS(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π{year_info} –∑–∞–≤–µ—Ä—à–µ–Ω"))
        self.stdout.write(f"   –°–æ–∑–¥–∞–Ω–æ: {stats['created']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, "
                         f"–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {stats['unchanged']}")
        self.stdout.write(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']} (–∏–∑ –Ω–∏—Ö –ø–æ –¥–∞—Ç–µ: {stats['skipped_by_date']})")
        self.stdout.write(f"   –û—à–∏–±–æ–∫: {stats['errors']}")

        return stats

    def _bulk_create_objects(self, to_create: List[Dict], pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        created_count = 0
        batch_size = 1000

        for batch in batch_iterator(to_create, batch_size):
            create_objects = [IPObject(**data) for data in batch]
            IPObject.objects.bulk_create(create_objects, batch_size=batch_size)
            created_count += len(batch)
            pbar.update(len(batch))

        return created_count

    def _bulk_update_objects(self, to_update: List[Dict], existing_objects: Dict, pbar) -> int:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ IPObject"""
        updated_count = 0
        BATCH_UPDATE_SIZE = 500

        for batch in batch_iterator(to_update, BATCH_UPDATE_SIZE):
            with transaction.atomic():
                for data in batch:
                    obj = existing_objects[data['registration_number']]
                    update_fields = []
                    for field, value in data.items():
                        if field != 'registration_number' and getattr(obj, field) != value:
                            setattr(obj, field, value)
                            update_fields.append(field)
                    if update_fields:
                        obj.save(update_fields=update_fields)
                        updated_count += 1
            pbar.update(len(batch))

        return updated_count
```


-----

# –§–∞–π–ª: management\parsers\__init__.py

```
"""
–ü–∞–∫–µ—Ç —Å –ø–∞—Ä—Å–µ—Ä–∞–º–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –†–ò–î
"""

from .invention import InventionParser
from .utility_model import UtilityModelParser
from .industrial_design import IndustrialDesignParser
from .integrated_circuit import IntegratedCircuitTopologyParser
from .computer_program import ComputerProgramParser
from .database import DatabaseParser

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –∏–∑ –ø–æ–¥–ø–∞–∫–µ—Ç–∞ processors
from .processors import (
    RussianTextProcessor,
    OrganizationNormalizer,
    PersonNameFormatter,
    RIDNameFormatter,
    EntityTypeDetector
)

__all__ = [
    'InventionParser',
    'UtilityModelParser',
    'IndustrialDesignParser',
    'IntegratedCircuitTopologyParser',
    'ComputerProgramParser',
    'DatabaseParser',
    'RussianTextProcessor',
    'OrganizationNormalizer',
    'PersonNameFormatter',
    'RIDNameFormatter',
    'EntityTypeDetector',
]
```


-----

# –§–∞–π–ª: management\parsers\processors\entity_detector.py

```
"""
–î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
"""

# –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –ø–∞–∫–µ—Ç–∞ (.text_processor)
from .text_processor import RussianTextProcessor


class EntityTypeDetector:
    """
    –î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –∏–º–µ–Ω–µ–º —á–µ–ª–æ–≤–µ–∫–∞ –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
    """

    def __init__(self, cache_size: int = 50000):
        self.processor = RussianTextProcessor()
        # –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —á—Ç–æ–±—ã –Ω–µ –≤—ã–∑—ã–≤–∞—Ç—å Natasha –ø–æ–≤—Ç–æ—Ä–Ω–æ
        self.cache = {}
        self.cache_size = cache_size
        self.cache_hits = 0
        self.cache_misses = 0

    def detect_type(self, text: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha

        Args:
            text: –ù–∞–∑–≤–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–§–ò–û –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)

        Returns:
            'person' –∏–ª–∏ 'organization'
        """
        if not text or len(text) < 2:
            return 'organization'

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if text in self.cache:
            self.cache_hits += 1
            return self.cache[text]

        self.cache_misses += 1

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å Natasha –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        # is_person() –≤–Ω—É—Ç—Ä–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç NER –∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã Natasha
        if self.processor.is_person(text):
            result = 'person'
        else:
            result = 'organization'

        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞–∑–º–µ—Ä–∞
        self._add_to_cache(text, result)

        return result

    def detect_type_batch(self, texts: list) -> dict:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤

        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å {—Ç–µ–∫—Å—Ç: —Ç–∏–ø}
        """
        result = {}

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        to_process = []
        for text in texts:
            if text in self.cache:
                result[text] = self.cache[text]
                self.cache_hits += 1
            else:
                to_process.append(text)
                self.cache_misses += 1

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
        for text in to_process:
            if self.processor.is_person(text):
                result[text] = 'person'
            else:
                result[text] = 'organization'
            self._add_to_cache(text, result[text])

        return result

    def _add_to_cache(self, text: str, result: str):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫—ç—à —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞–∑–º–µ—Ä–∞
        """
        if len(self.cache) >= self.cache_size:
            # –û—á–∏—â–∞–µ–º 20% —Å–∞–º—ã—Ö —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
            items = list(self.cache.items())
            self.cache = dict(items[-int(self.cache_size * 0.8):])

        self.cache[text] = result

    def get_cache_stats(self) -> dict:
        """
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        """
        total = self.cache_hits + self.cache_misses
        return {
            'size': len(self.cache),
            'hits': self.cache_hits,
            'misses': self.cache_misses,
            'hit_ratio': self.cache_hits / total if total > 0 else 0
        }

    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        self.cache.clear()
        self.cache_hits = 0
        self.cache_misses = 0
```


-----

# –§–∞–π–ª: management\parsers\processors\organization.py

```
"""
–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)
"""

import re
from typing import Dict, Any

import pandas as pd

from core.models import OrganizationNormalizationRule

# –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –ø–∞–∫–µ—Ç–∞ (.text_processor)
from .text_processor import RussianTextProcessor


class OrganizationNormalizer:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)"""

    def __init__(self):
        self.rules_cache = None
        self.processor = RussianTextProcessor()
        self.load_rules()

    def load_rules(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∏–∑ –ë–î"""
        try:
            rules = OrganizationNormalizationRule.objects.all().order_by('priority')
            self.rules_cache = [
                {
                    'original': rule.original_text.lower(),
                    'replacement': rule.replacement_text.lower(),
                    'type': rule.rule_type,
                    'priority': rule.priority
                }
                for rule in rules
            ]
        except Exception as e:
            self.rules_cache = []
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏, –Ω–æ –Ω–µ –ø–∞–¥–∞–µ–º

    def normalize_for_search(self, name: str) -> Dict[str, Any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –¢–û–õ–¨–ö–û –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        –°–∞–º–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –≤ CSV
        """
        if pd.isna(name) or not name:
            return {'normalized': '', 'keywords': [], 'original': name}

        original = str(name).strip()
        name_lower = original.lower()

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ –ë–î –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        normalized = name_lower
        if self.rules_cache:
            for rule in self.rules_cache:
                try:
                    if rule['type'] == 'ignore':
                        pattern = r'\b' + re.escape(rule['original']) + r'\b'
                        normalized = re.sub(pattern, '', normalized)
                    else:
                        pattern = r'\b' + re.escape(rule['original']) + r'\b'
                        normalized = re.sub(pattern, rule['replacement'], normalized)
                except Exception:
                    continue

        # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞
        normalized = re.sub(r'["\'¬´¬ª‚Äû‚Äú‚Äù]', '', normalized)
        normalized = re.sub(r'[^\w\s-]', ' ', normalized)
        normalized = ' '.join(normalized.split())

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        keywords = []

        # –°–ª–æ–≤–∞ –≤ –∫–∞–≤—ã—á–∫–∞—Ö
        quoted = re.findall(r'"([^"]+)"', original)
        for q in quoted:
            words = q.lower().split()
            keywords.extend([w for w in words if len(w) > 3])

        # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        abbrs = re.findall(r'\b[–ê-–Ø–ÅA-Z]{2,}\b', original)
        keywords.extend([a.lower() for a in abbrs if len(a) >= 2])

        # –ö–æ–¥—ã (–ò–ù–ù, –û–ì–†–ù –∏ —Ç.–¥.)
        codes = re.findall(r'\b\d{10,}\b', original)
        keywords.extend(codes)

        return {
            'normalized': normalized,
            'keywords': list(set(keywords)),
            'original': original,
        }

    def format_organization_name(self, name: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        return name

```


-----

# –§–∞–π–ª: management\parsers\processors\person.py

```
"""
–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω –ª—é–¥–µ–π
"""
from .text_processor import RussianTextProcessor


class PersonNameFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω –ª—é–¥–µ–π"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def format(self, name: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –§–ò–û"""
        return self.processor.format_person_name(name)

```


-----

# –§–∞–π–ª: management\parsers\processors\rid.py

```
"""
–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –†–ò–î
"""

from .text_processor import RussianTextProcessor


class RIDNameFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –†–ò–î"""

    def __init__(self):
        self.processor = RussianTextProcessor()

    def format(self, text: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –†–ò–î"""
        if not text or not isinstance(text, str):
            return text

        if len(text.strip()) <= 1:
            return text

        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ –¥–µ–ª–∞–µ–º –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –∑–∞–≥–ª–∞–≤–Ω–æ–π
        words = text.lower().split()
        if words:
            words[0] = words[0][0].upper() + words[0][1:]
        return ' '.join(words)

```


-----

# –§–∞–π–ª: management\parsers\processors\text_processor.py

```
"""
–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º natasha
"""

from natasha import (
    Segmenter,
    MorphVocab,
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    NewsNERTagger,
    Doc,
    NamesExtractor
)


class RussianTextProcessor:
    """
    –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º natasha
    """

    # –°–ø–∏—Å–æ–∫ —Ä–∏–º—Å–∫–∏—Ö —Ü–∏—Ñ—Ä
    ROMAN_NUMERALS = {
        'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X',
        'XI', 'XII', 'XIII', 'XIV', 'XV', 'XVI', 'XVII', 'XVIII', 'XIX', 'XX',
        'XXI', 'XXII', 'XXIII', 'XXIV', 'XXV', 'XXX', 'XL', 'L', 'LX', 'XC',
        'C', 'CD', 'D', 'DC', 'CM', 'M'
    }

    # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
    ORG_ABBR = {
        '–û–û–û', '–ó–ê–û', '–û–ê–û', '–ê–û', '–ü–ê–û', '–ù–ê–û',
        '–§–ì–£–ü', '–§–ì–ë–£', '–§–ì–ê–û–£', '–§–ì–ê–£', '–§–ì–ö–£',
        '–ù–ò–ò', '–ö–ë', '–û–ö–ë', '–°–ö–ë', '–¶–ö–ë', '–ü–ö–ë',
        '–ù–ü–û', '–ù–ü–ü', '–ù–ü–§', '–ù–ü–¶', '–ù–ò–¶',
        '–ú–£–ü', '–ì–£–ü', '–ò–ß–ü', '–¢–û–û', '–ê–û–ó–¢', '–ê–û–û–¢',
        '–†–§', '–†–ê–ù', '–°–û –†–ê–ù', '–£—Ä–û –†–ê–ù', '–î–í–û –†–ê–ù',
        '–ú–ì–£', '–°–ü–±–ì–£', '–ú–§–¢–ò', '–ú–ò–§–ò', '–ú–ì–¢–£', '–ú–ê–ò',
        '–õ–¢–î', '–ò–ù–ö', '–ö–û', '–ì–ú–ë–•', '–ê–ì', '–°–ê', '–ù–í', '–ë–í', '–°–ï',
        '–ö–æ', 'Ltd', 'Inc', 'GmbH', 'AG', 'SA', 'NV', 'BV', 'SE',
    }

    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ natasha
        self.segmenter = Segmenter()
        self.morph_vocab = MorphVocab()
        self.emb = NewsEmbedding()
        self.morph_tagger = NewsMorphTagger(self.emb)
        self.syntax_parser = NewsSyntaxParser(self.emb)
        self.ner_tagger = NewsNERTagger(self.emb)
        self.names_extractor = NamesExtractor(self.morph_vocab)

        # –ö—ç—à–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.doc_cache = {}
        self.morph_cache = {}

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã –≤ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        self.ORG_ABBR.update(self.ROMAN_NUMERALS)

    def get_doc(self, text: str) -> Doc:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not text:
            return None

        if text in self.doc_cache:
            return self.doc_cache[text]

        doc = Doc(text)
        doc.segment(self.segmenter)
        doc.tag_morph(self.morph_tagger)
        doc.parse_syntax(self.syntax_parser)
        doc.tag_ner(self.ner_tagger)

        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
        for token in doc.tokens:
            token.lemmatize(self.morph_vocab)

        for span in doc.spans:
            span.normalize(self.morph_vocab)

        self.doc_cache[text] = doc
        return doc

    def is_roman_numeral(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∏–º—Å–∫—É—é —Ü–∏—Ñ—Ä—É"""
        if not text:
            return False
        clean_text = text.strip('.,;:!?()').upper()
        return clean_text in self.ROMAN_NUMERALS

    def is_abbr(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        if not text:
            return False
        clean_text = text.strip('.,;:!?()').upper()
        return clean_text in self.ORG_ABBR

    def is_person(self, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –§–ò–û —á–µ–ª–æ–≤–µ–∫–∞"""
        if not text or len(text) < 6:
            return False

        # –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        if any(ind in text for ind in self.ORG_ABBR if len(ind) > 2):
            return False

        org_indicators = ['–û–±—â–µ—Å—Ç–≤–æ', '–ö–æ–º–ø–∞–Ω–∏—è', '–ö–æ—Ä–ø–æ—Ä–∞—Ü–∏—è', '–ó–∞–≤–æ–¥',
                         '–ò–Ω—Å—Ç–∏—Ç—É—Ç', '–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '–ê–∫–∞–¥–µ–º–∏—è', '–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è',
                         '–§–∏—Ä–º–∞', '–¶–µ–Ω—Ç—Ä']

        if any(ind.lower() in text.lower() for ind in org_indicators):
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ NER
        doc = self.get_doc(text)
        if doc and doc.spans:
            for span in doc.spans:
                if span.type == 'PER':
                    return True

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –§–ò–û
        words = text.split()
        if 2 <= len(words) <= 4:
            name_like = 0
            for word in words:
                clean = word.rstrip('.,')
                if clean and clean[0].isupper() and len(clean) > 1:
                    name_like += 1
            return name_like >= len(words) - 1

        return False

    def extract_person_parts(self, text: str) -> dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π –§–ò–û —Å –ø–æ–º–æ—â—å—é natasha"""
        matches = list(self.names_extractor(text))
        if matches:
            fact = matches[0].fact
            parts = []
            if fact.last:
                parts.append(fact.last)
            if fact.first:
                parts.append(fact.first)
            if fact.middle:
                parts.append(fact.middle)

            return {
                'last': fact.last or '',
                'first': fact.first or '',
                'middle': fact.middle or '',
                'full': ' '.join(parts)
            }

        # Fallback: —Ä—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥
        return self._parse_name_manually(text)

    def _parse_name_manually(self, text: str) -> dict:
        """–†—É—á–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –∏–º–µ–Ω–∏"""
        words = text.split()

        if len(words) == 3:
            return {
                'last': words[0],
                'first': words[1],
                'middle': words[2],
                'full': text
            }
        elif len(words) == 2:
            return {
                'last': words[0],
                'first': words[1],
                'middle': '',
                'full': text
            }
        else:
            return {
                'last': text,
                'first': '',
                'middle': '',
                'full': text
            }

    def format_person_name(self, name: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –§–ò–û —á–µ–ª–æ–≤–µ–∫–∞"""
        if not name:
            return name

        parts = self.extract_person_parts(name)
        if parts.get('full'):
            return parts['full']

        return name

```


-----

# –§–∞–π–ª: management\parsers\processors\__init__.py

```
from .text_processor import RussianTextProcessor
from .organization import OrganizationNormalizer
from .person import PersonNameFormatter
from .rid import RIDNameFormatter
from .entity_detector import EntityTypeDetector

__all__ = [
    'RussianTextProcessor',
    'OrganizationNormalizer',
    'PersonNameFormatter',
    'RIDNameFormatter',
    'EntityTypeDetector',
]
```


-----

# –§–∞–π–ª: management\utils\csv_loader.py

```
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ CSV —Ñ–∞–π–ª–æ–≤
"""

import pandas as pd


def load_csv_with_strategies(file_path, encoding, delimiter, stdout=None):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏
    """
    strategies = [
        {'encoding': encoding, 'delimiter': delimiter, 'skipinitialspace': True},
        {'encoding': 'cp1251', 'delimiter': delimiter, 'skipinitialspace': True},
        {'encoding': 'utf-8', 'delimiter': ';', 'skipinitialspace': True},
        {'encoding': 'cp1251', 'delimiter': ';', 'skipinitialspace': True},
        {'encoding': 'utf-8', 'delimiter': '\t', 'skipinitialspace': True},
    ]

    for strategy in strategies:
        try:
            df = pd.read_csv(file_path, **strategy, dtype=str, keep_default_na=False)
            if stdout:
                stdout.write(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {strategy}")

            df.columns = [col.strip().strip('\ufeff').strip('"') for col in df.columns]
            return df
        except Exception:
            continue

    raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π")

```


-----

# –§–∞–π–ª: management\utils\filters.py

```
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ DataFrame
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω—É –ª–µ—Ç
"""

from datetime import datetime
import pandas as pd


def filter_by_registration_year(df, min_year, stdout=None, max_year=None):
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è DataFrame –ø–æ –≥–æ–¥—É —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω–∞
    
    Args:
        df: DataFrame –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        min_year: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥
        stdout: –ø–æ—Ç–æ–∫ –≤—ã–≤–æ–¥–∞
        max_year: –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    def extract_year(date_str):
        try:
            if pd.isna(date_str) or not date_str:
                return None

            date_str = str(date_str).strip()
            if not date_str:
                return None

            for fmt in ['%Y%m%d', '%Y-%m-%d', '%d.%m.%Y', '%Y/%m/%d']:
                try:
                    return datetime.strptime(date_str, fmt).year
                except (ValueError, TypeError):
                    continue

            try:
                return pd.to_datetime(date_str).year
            except (ValueError, TypeError):
                return None
        except:
            return None

    if stdout:
        stdout.write("  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≥–æ–¥—É —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏...")

    if 'registration date' not in df.columns:
        if stdout:
            stdout.write("  ‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'registration date' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –≥–æ–¥—É")
        return df

    df['_year'] = df['registration date'].apply(extract_year)

    if stdout:
        # –§–∏–ª—å—Ç—Ä—É–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        valid_years = df['_year'].dropna()
        if not valid_years.empty:
            years_dist = valid_years.value_counts().sort_index()
            years_list = list(years_dist.items())
            if len(years_list) > 0:
                stdout.write(f"     –î–∏–∞–ø–∞–∑–æ–Ω –≥–æ–¥–æ–≤: {years_list[0][0]:.0f} - {years_list[-1][0]:.0f}")

    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –≥–æ–¥–∞–º
    condition = df['_year'] >= min_year
    if max_year:
        condition &= df['_year'] <= max_year
    
    filtered_df = df[condition].copy() if '_year' in df.columns else df.copy()
    
    if '_year' in filtered_df.columns:
        filtered_df.drop('_year', axis=1, inplace=True)

    return filtered_df


def filter_by_actual(df, stdout=None):
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è DataFrame –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (actual = True)
    """
    def parse_actual(value):
        if pd.isna(value) or not value:
            return False
        value = str(value).lower().strip()
        return value in ['1', 'true', 'yes', '–¥–∞', '–¥–µ–π—Å—Ç–≤—É–µ—Ç', 't', '1.0', '–∞–∫—Ç–∏–≤–µ–Ω']

    if 'actual' not in df.columns:
        if stdout:
            stdout.write("  ‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'actual' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏")
        return df

    df['_actual'] = df['actual'].apply(parse_actual)
    filtered_df = df[df['_actual'] == True].copy()
    filtered_df.drop('_actual', axis=1, inplace=True)

    return filtered_df


def apply_filters(df, min_year, only_active, stdout=None, max_year=None):
    """
    –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∫ DataFrame
    
    Args:
        df: DataFrame –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        min_year: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥
        only_active: —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ
        stdout: –ø–æ—Ç–æ–∫ –≤—ã–≤–æ–¥–∞
        max_year: –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    original_count = len(df)

    if min_year is not None:
        df = filter_by_registration_year(df, min_year, stdout, max_year)

    if only_active:
        df = filter_by_actual(df, stdout)

    filtered_count = len(df)
    if stdout and filtered_count < original_count:
        stdout.write(f"  üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {original_count} ‚Üí {filtered_count} –∑–∞–ø–∏—Å–µ–π")

    return df
```


-----

# –§–∞–π–ª: management\utils\progress.py

```
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–≤
–í—ã–Ω–µ—Å–µ–Ω—ã –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –ø–∞—Ä—Å–µ—Ä–∞—Ö
"""

import sys
from tqdm import tqdm
from contextlib import contextmanager
from typing import Optional, Iterable, Iterator, Any


class ProgressManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞–º–∏
    –í—Å–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º tqdm)
    """
    
    def __init__(self, enabled: bool = True, file=sys.stdout):
        self.enabled = enabled
        self.file = file
        self._current_bar = None  # –¢–µ–∫—É—â–∏–π –∞–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
    
    @contextmanager
    def task(self, description: str, total: Optional[int] = None, unit: str = "—ç–ª–µ–º"):
        """
        –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∑–∞–¥–∞—á–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        –í—Å–µ –∑–∞–¥–∞—á–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É (–ø—Ä–µ–¥—ã–¥—É—â–∞—è –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è)
        """
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π –±–∞—Ä, –∑–∞–∫—Ä—ã–≤–∞–µ–º –µ–≥–æ
        if self._current_bar is not None:
            self._current_bar.close()
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        bar = tqdm(
            total=total,
            desc=description,
            unit=unit,
            file=self.file,
            leave=False,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
        )
        
        self._current_bar = bar
        
        try:
            yield bar
        finally:
            bar.close()
            self._current_bar = None
            print(file=self.file)
    
    @contextmanager
    def subtask(self, description: str, total: Optional[int] = None, unit: str = "—ç–ª–µ–º"):
        """–ê–ª–∏–∞—Å –¥–ª—è task (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        with self.task(description, total, unit) as bar:
            yield bar
    
    def step(self, message: str):
        """–í—ã–≤–æ–¥ —Å–æ–æ–±—â–µ–Ω–∏—è –æ —à–∞–≥–µ (–≤—Å–µ–≥–¥–∞ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)"""
        if self._current_bar is not None:
            self._current_bar.write(f"üîπ {message}")
        else:
            print(f"üîπ {message}", file=self.file)

    def success(self, message: str):
        """–í—ã–≤–æ–¥ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± —É—Å–ø–µ—Ö–µ"""
        if self._current_bar is not None:
            self._current_bar.write(f"‚úÖ {message}")
        else:
            print(f"‚úÖ {message}", file=self.file)

    def warning(self, message: str):
        """–í—ã–≤–æ–¥ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è"""
        if self._current_bar is not None:
            self._current_bar.write(f"‚ö†Ô∏è {message}")
        else:
            print(f"‚ö†Ô∏è {message}", file=self.file)

    def error(self, message: str):
        """–í—ã–≤–æ–¥ –æ—à–∏–±–∫–∏"""
        if self._current_bar is not None:
            self._current_bar.write(f"‚ùå {message}")
        else:
            print(f"‚ùå {message}", file=self.file)


def batch_iterator(iterable, batch_size: int):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π –æ–±—ä–µ–∫—Ç –Ω–∞ –±–∞—Ç—á–∏"""
    batch = []
    for item in iterable:
        batch.append(item)
        if len(batch) >= batch_size:
            yield batch
            batch = []
    if batch:
        yield batch
```


-----

# –§–∞–π–ª: management\utils\__init__.py

```
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –ø–∞—Ä—Å–µ—Ä–æ–≤
"""

from .csv_loader import load_csv_with_strategies
from .filters import apply_filters
from .progress import ProgressManager, batch_iterator

__all__ = [
    'load_csv_with_strategies',
    'apply_filters',
    'ProgressManager',
    'batch_iterator',
]
```