# –§–∞–π–ª: fips_parser\config.py

```
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ –§–ò–ü–°

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–¥–µ—Ä–∂–µ–∫
DELAY_CONFIG = {
    'min_delay': 2,           # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫)
    'max_delay': 3,           # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫)
    'long_delay_frequency': 250, # –ö–∞–∂–¥—ã–µ N –∑–∞–ø—Ä–æ—Å–æ–≤ - –¥–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞
    'long_delay_min': 30,     # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞ (—Å–µ–∫)
    'long_delay_max': 60,     # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞ (—Å–µ–∫)
    'requests_per_minute': 15, # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–∏–Ω—É—Ç—É
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
RETRY_CONFIG = {
    'max_retries': 3,
    'backoff_factor': 1,
    'status_forcelist': [429, 500, 502, 503, 504],
    'allowed_methods': ["GET"],
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤
REQUEST_CONFIG = {
    'timeout': 30,
    'encoding': 'windows-1251',
}

# –ó–∞–≥–æ–ª–æ–≤–∫–∏ HTTP
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∞–π–ª–æ–≤
FILE_CONFIG = {
    'input_dir': '../data',
    'output_dir': '../data',
    'backup_dir': '../backups',
    'input_file': 'fips1.xlsx',
    'output_file': 'fips1_parsed.xlsx',
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–µ—Ä–∞
PARSER_CONFIG = {
    'save_progress_after_each': True,
    'default_start_row': 0,
    'max_requests_per_run': None,
}
```


-----

# –§–∞–π–ª: fips_parser\fips_parser.py

```
# fips_parser.py - –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –§–ê–ô–õ –° –õ–ï–ù–ò–í–û–ô –ó–ê–ì–†–£–ó–ö–û–ô –ü–ê–†–°–ï–†–û–í
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse, parse_qs
import os
import sys
from datetime import datetime, timedelta
import time
import random
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# –ò–º–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
try:
    from config import (
        DELAY_CONFIG, 
        RETRY_CONFIG, 
        REQUEST_CONFIG,
        HEADERS, 
        FILE_CONFIG,
        PARSER_CONFIG
    )
    CONFIG_LOADED = True
except ImportError as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
    print("‚ö†Ô∏è –ë—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    CONFIG_LOADED = False
    # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏
    DELAY_CONFIG = {
        'min_delay': 3, 'max_delay': 7, 'long_delay_frequency': 10,
        'long_delay_min': 30, 'long_delay_max': 60, 'requests_per_minute': 15
    }
    RETRY_CONFIG = {
        'max_retries': 3, 'backoff_factor': 1,
        'status_forcelist': [429, 500, 502, 503, 504], 'allowed_methods': ["GET"]
    }
    REQUEST_CONFIG = {'timeout': 30, 'encoding': 'windows-1251'}
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    }
    FILE_CONFIG = {
        'input_dir': '../data', 'output_dir': '../data', 'backup_dir': '../backups',
        'input_file': 'fips1.xlsx', 'output_file': 'fips1_parsed.xlsx'
    }
    PARSER_CONFIG = {
        'save_progress_after_each': True, 'default_start_row': 0, 'max_requests_per_run': None
    }

class ParserLoader:
    """–ö–ª–∞—Å—Å –¥–ª—è –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞—Ä—Å–µ—Ä–æ–≤"""
    
    _parsers = {}
    
    @classmethod
    def get_parser(cls, rid_type):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä—Å–µ—Ä –ø–æ —Ç–∏–ø—É –†–ò–î (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)"""
        if rid_type not in cls._parsers:
            cls._load_parser(rid_type)
        return cls._parsers.get(rid_type)
    
    @classmethod
    def _load_parser(cls, rid_type):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –ø–æ —Ç–∏–ø—É –†–ò–î"""
        try:
            if rid_type == '–ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ':
                from parsers.invention_parser import parse_invention
                cls._parsers[rid_type] = parse_invention
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π")
                
            elif rid_type == '–ü–æ–ª–µ–∑–Ω–∞—è –º–æ–¥–µ–ª—å':
                from parsers.utility_model_parser import parse_utility_model
                cls._parsers[rid_type] = parse_utility_model
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
                
            elif rid_type == '–ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü':
                from parsers.industrial_design_parser import parse_industrial_design
                cls._parsers[rid_type] = parse_industrial_design
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤")
                
            elif rid_type == '–ü—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è –≠–í–ú':
                from parsers.computer_program_parser import parse_computer_program
                cls._parsers[rid_type] = parse_computer_program
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –≠–í–ú")
                
            elif rid_type == '–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö':
                from parsers.database_parser import parse_database
                cls._parsers[rid_type] = parse_database
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –ø–∞—Ä—Å–µ—Ä –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö")
                
            elif rid_type == '–¢–æ–ø–æ–ª–æ–≥–∏—è –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–æ–π –º–∏–∫—Ä–æ—Å—Ö–µ–º—ã':
                from parsers.topology_parser import parse_topology
                cls._parsers[rid_type] = parse_topology
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–æ–ø–æ–ª–æ–≥–∏–π –º–∏–∫—Ä–æ—Å—Ö–µ–º")
                
            else:
                cls._parsers[rid_type] = cls._default_parser
                print(f"‚ö†Ô∏è –ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–∏–ø–∞ {rid_type} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–∞–≥–ª—É—à–∫–∞")
                
        except ImportError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è {rid_type}: {e}")
            cls._parsers[rid_type] = cls._default_parser
    
    @staticmethod
    def _default_parser(html):
        """–ü–∞—Ä—Å–µ—Ä-–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ç–∏–ø–æ–≤"""
        return {'–ü—Ä–∏–º–µ—á–∞–Ω–∏—è': f'–ü–∞—Ä—Å–µ—Ä –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –†–ò–î –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω'}

class FIPSParser:
    def __init__(self):
        self.session = requests.Session()
        self.parser_loader = ParserLoader()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        retry_strategy = Retry(
            total=RETRY_CONFIG['max_retries'],
            status_forcelist=RETRY_CONFIG['status_forcelist'],
            allowed_methods=RETRY_CONFIG['allowed_methods'],
            backoff_factor=RETRY_CONFIG['backoff_factor']
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.session.headers.update(HEADERS)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á–µ—Ç—á–∏–∫–æ–≤
        self.request_count = 0
        self.start_time = time.time()
        self.last_request_time = 0
        
    def determine_rid_type(self, url):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –†–ò–î –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—É DB –≤ URL"""
        if not url or pd.isna(url):
            return '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø'
            
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)
        db_value = query_params.get('DB', [''])[0]
        
        type_mapping = {
            'RUPAT': '–ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ',
            'RUPM': '–ü–æ–ª–µ–∑–Ω–∞—è –º–æ–¥–µ–ª—å', 
            'RUDE': '–ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü',
            'TIMS': '–¢–æ–ø–æ–ª–æ–≥–∏—è –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–æ–π –º–∏–∫—Ä–æ—Å—Ö–µ–º—ã',
            'EVM': '–ü—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è –≠–í–ú',
            'DB': '–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö'
        }
        
        return type_mapping.get(db_value, '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø')
    
    def get_parser_function(self, rid_type):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–µ—Ä–∞ –ø–æ —Ç–∏–ø—É –†–ò–î (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)"""
        return self.parser_loader.get_parser(rid_type)
    
    def smart_delay(self):
        """–£–º–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        current_time = time.time()
        
        # –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        base_delay = random.uniform(
            DELAY_CONFIG['min_delay'], 
            DELAY_CONFIG['max_delay']
        )
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
        self.request_count += 1
        
        # –î–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if self.request_count % DELAY_CONFIG['long_delay_frequency'] == 0:
            long_delay = random.uniform(
                DELAY_CONFIG['long_delay_min'], 
                DELAY_CONFIG['long_delay_max']
            )
            print(f"üîÅ –ë–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ {long_delay:.1f} —Å–µ–∫ –ø–æ—Å–ª–µ {self.request_count} –∑–∞–ø—Ä–æ—Å–æ–≤...")
            time.sleep(long_delay)
        else:
            # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è
            time.sleep(base_delay)
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        elapsed = current_time - self.start_time
        if elapsed < 60 and self.request_count >= DELAY_CONFIG['requests_per_minute']:
            excess_delay = 60 - elapsed
            if excess_delay > 0:
                print(f"‚è≥ –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤, –∂–¥–µ–º {excess_delay:.1f} —Å–µ–∫...")
                time.sleep(excess_delay)
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
                self.start_time = time.time()
                self.request_count = 0
        
        self.last_request_time = current_time
    
    def fetch_page(self, url):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            print(f"üì° –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
            response = self.session.get(
                url, 
                timeout=REQUEST_CONFIG['timeout']
            )
            response.encoding = REQUEST_CONFIG['encoding']
            
            if response.status_code == 200:
                print("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return response.text
            elif response.status_code == 429:
                print("‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤. –ñ–¥–µ–º 60 —Å–µ–∫—É–Ω–¥...")
                time.sleep(60)
                return None
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ HTTP {response.status_code} –¥–ª—è URL: {url}")
                return None
                
        except requests.exceptions.Timeout:
            print(f"‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}")
            return None
        except requests.exceptions.ConnectionError:
            print(f"üîå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–ª—è {url}")
            return None
        except requests.exceptions.RequestException as e:
            print(f"üö´ –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {url}: {e}")
            return None
        except Exception as e:
            print(f"üí• –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
            return None
    
    def parse_single_record(self, url):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –ø–æ URL"""
        if not url or pd.isna(url):
            result = {col: '' for col in self.get_columns()}
            result['–ü—Ä–∏–º–µ—á–∞–Ω–∏—è'] = '–ü—É—Å—Ç–æ–π URL'
            return result
        
        rid_type = self.determine_rid_type(url)
        print(f"üîç –û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ç–∏–ø –†–ò–î: {rid_type}")
        parser_function = self.get_parser_function(rid_type)
        
        if not parser_function:
            result = {col: '' for col in self.get_columns()}
            result['–¢–∏–ø –†–ò–î'] = rid_type
            result['–ü—Ä–∏–º–µ—á–∞–Ω–∏—è'] = f'–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–∏–ø–∞ {rid_type} –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω'
            return result
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
        self.smart_delay()
        
        html_content = self.fetch_page(url)
        if not html_content:
            result = {col: '' for col in self.get_columns()}
            result['–¢–∏–ø –†–ò–î'] = rid_type
            result['–ü—Ä–∏–º–µ—á–∞–Ω–∏—è'] = f'–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É'
            return result
        
        try:
            print("üîÑ –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ...")
            result = parser_function(html_content)
            result['–¢–∏–ø –†–ò–î'] = rid_type
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ –ø–æ–ª—è –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
            for col in self.get_columns():
                if col not in result:
                    result[col] = ''
            print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω—ã")
            return result
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            result = {col: '' for col in self.get_columns()}
            result['–¢–∏–ø –†–ò–î'] = rid_type
            result['–ü—Ä–∏–º–µ—á–∞–Ω–∏—è'] = f'–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}'
            return result
    
    def get_columns(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫ —Ç–∞–±–ª–∏—Ü—ã"""
        return [
            '–¢–∏–ø –†–ò–î', '–ù–∞–∑–≤–∞–Ω–∏–µ', '–ù–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏/–ø–∞—Ç–µ–Ω—Ç–∞', '–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏',
            '–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏', '–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏', '–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏', '–ê–≤—Ç–æ—Ä—ã',
            '–ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å/–ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å', '–†–µ—Ñ–µ—Ä–∞—Ç', '–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã/–ê–¥—Ä–µ—Å –¥–ª—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏',
            '–°—Ç–∞—Ç—É—Å', '–î–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞', '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—à–ª–∏–Ω–∞—Ö', '–Ø–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è',
            '–°–£–ë–î', '–û–±—ä–µ–º', '–§–æ—Ä–º—É–ª–∞', '–¶–∏—Ç–∏—Ä—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã', '–û–ø–∏—Å–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞',
            '–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞', '–ü—Ä–∏–º–µ—á–∞–Ω–∏—è'
        ]
    
    def ensure_string_columns(self, df):
        """–£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–º–µ—é—Ç —Å—Ç—Ä–æ–∫–æ–≤—ã–π —Ç–∏–ø"""
        for col in self.get_columns():
            if col in df.columns:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–æ–ª–æ–Ω–∫—É –≤ —Å—Ç—Ä–æ–∫–æ–≤—ã–π —Ç–∏–ø, –∑–∞–º–µ–Ω—è—è NaN –Ω–∞ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                df[col] = df[col].astype(str).replace('nan', '')
        return df
    
    def process_excel(self, input_file=None, output_file=None, start_from=None, max_requests=None):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ Excel —Ñ–∞–π–ª–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –µ—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        if input_file is None:
            input_file = os.path.join(FILE_CONFIG['input_dir'], FILE_CONFIG['input_file'])
        if output_file is None:
            output_file = os.path.join(FILE_CONFIG['output_dir'], FILE_CONFIG['output_file'])
        if start_from is None:
            start_from = PARSER_CONFIG['default_start_row']
        if max_requests is None:
            max_requests = PARSER_CONFIG['max_requests_per_run']
        
        try:
            df = pd.read_excel(input_file)
            print(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª {input_file} —Å {len(df)} –∑–∞–ø–∏—Å—è–º–∏")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {input_file}: {e}")
            return
        
        # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        for col in self.get_columns():
            if col not in df.columns:
                df[col] = ''
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ —Å—Ç—Ä–æ–∫–æ–≤—ã–π —Ç–∏–ø —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
        df = self.ensure_string_columns(df)
        
        total = len(df)
        processed = 0
        successful = 0
        
        for index, row in df.iterrows():
            if index < start_from:
                continue
                
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
            if max_requests and processed >= max_requests:
                print(f"‚èπÔ∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ {max_requests} –∑–∞–ø—Ä–æ—Å–æ–≤")
                break
                
            url = row['–°—Å—ã–ª–∫–∞ –Ω–∞ –†–æ—Å–ø–∞—Ç–µ–Ω—Ç']
            print(f"\nüéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ {index + 1}/{total}")
            print(f"   URL: {url}")
            
            result = self.parse_single_record(url)
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ DataFrame
            for key, value in result.items():
                if key in df.columns:
                    # –Ø–≤–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É
                    str_value = str(value) if value is not None else ''
                    df.at[index, key] = str_value
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —É—Å–ø–µ—à–Ω—ã—Ö –æ–±—Ä–∞–±–æ—Ç–æ–∫
            if not result.get('–ü—Ä–∏–º–µ—á–∞–Ω–∏—è') or '–æ—à–∏–±–∫–∞' not in result.get('–ü—Ä–∏–º–µ—á–∞–Ω–∏—è', '').lower():
                successful += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –≤ –∫–æ–Ω—Ñ–∏–≥–µ)
            if PARSER_CONFIG['save_progress_after_each']:
                try:
                    # –ü–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ
                    df = self.ensure_string_columns(df)
                    df.to_excel(output_file, index=False)
                    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –∑–∞–ø–∏—Å–∏ {index + 1}")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            
            processed += 1
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            elapsed = time.time() - self.start_time
            req_per_min = (self.request_count / elapsed * 60) if elapsed > 0 else 0
            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {self.request_count} –∑–∞–ø—Ä–æ—Å–æ–≤, {req_per_min:.1f} –∑–∞–ø—Ä/–º–∏–Ω, —É—Å–ø–µ—à–Ω–æ: {successful}/{processed}")
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        try:
            df = self.ensure_string_columns(df)
            df.to_excel(output_file, index=False)
            print(f"üíæ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_file}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        
        print(f"\nüéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed} –∑–∞–ø–∏—Å–µ–π, —É—Å–ø–µ—à–Ω–æ: {successful}")

def main():
    parser = FIPSParser()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    base_dir = os.path.dirname(os.path.abspath(__file__))
    input_file = os.path.join(base_dir, FILE_CONFIG['input_dir'], FILE_CONFIG['input_file'])
    output_file = os.path.join(base_dir, FILE_CONFIG['output_dir'], FILE_CONFIG['output_file'])
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
    os.makedirs(os.path.dirname(input_file), exist_ok=True)
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    if not os.path.exists(input_file):
        print(f"‚ùå –§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        print(f"üìÇ –¢–µ–∫—É—â–∞—è —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
        print(f"üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–∫—Ä–∏–ø—Ç–∞: {base_dir}")
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª –≤ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç–∞—Ö
        possible_paths = [
            os.path.join(base_dir, FILE_CONFIG['input_file']),
            os.path.join(os.getcwd(), FILE_CONFIG['input_file']),
            os.path.join(os.getcwd(), FILE_CONFIG['input_dir'], FILE_CONFIG['input_file']),
        ]
        for path in possible_paths:
            if os.path.exists(path):
                input_file = path
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {input_file}")
                break
        else:
            print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –≤ –æ–¥–Ω–æ–º –∏–∑ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–π")
            return
    
    print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {input_file}")
    print(f"üí° –†–µ–∑—É–ª—å—Ç–∞—Ç –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
    print(f"‚öôÔ∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {'–î–∞' if CONFIG_LOADED else '–ù–µ—Ç'}")
    print("üîÑ –ü–∞—Ä—Å–µ—Ä—ã –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏")
    print("‚ö†Ô∏è  –ü–∞—Ä—Å–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    start_from = PARSER_CONFIG['default_start_row']
    max_requests = PARSER_CONFIG['max_requests_per_run']
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –∫–æ–Ω—Ñ–∏–≥)
    if len(sys.argv) > 1:
        try:
            start_from = int(sys.argv[1])
            print(f"üîÅ –ù–∞—á–∏–Ω–∞–µ–º —Å –∑–∞–ø–∏—Å–∏ {start_from} (–∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏)")
        except ValueError:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä —Å—Ç–∞—Ä—Ç–æ–≤–æ–π –∑–∞–ø–∏—Å–∏")
    
    if len(sys.argv) > 2:
        try:
            max_requests = int(sys.argv[2])
            print(f"‚èπÔ∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: {max_requests} –∑–∞–ø—Ä–æ—Å–æ–≤ (–∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏)")
        except ValueError:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    try:
        parser.process_excel(input_file, output_file, start_from, max_requests)
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
```


-----

# –§–∞–π–ª: fips_parser\parsers\computer_program_parser.py

```
def parse_computer_program(html_content):
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞ –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è –≠–í–ú"""
    return {'–ü—Ä–∏–º–µ—á–∞–Ω–∏—è': '–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –≠–í–ú –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ'}
```


-----

# –§–∞–π–ª: fips_parser\parsers\database_parser.py

```
def parse_database(html_content):
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö"""
    return {'–ü—Ä–∏–º–µ—á–∞–Ω–∏—è': '–ü–∞—Ä—Å–µ—Ä –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ'}
```


-----

# –§–∞–π–ª: fips_parser\parsers\industrial_design_parser.py

```
import re
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

def parse_industrial_design(html_content):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤"""
    
    # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏
    def find_element_containing_text(soup, search_text):
        """–ù–∞—Ö–æ–¥–∏—Ç —ç–ª–µ–º–µ–Ω—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"""
        elements = soup.find_all(string=re.compile(re.escape(search_text)))
        for element in elements:
            return element.parent
        return None

    def find_text_after(soup, search_text):
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        elements = soup.find_all(string=re.compile(re.escape(search_text)))
        for element in elements:
            parent = element.parent
            if parent:
                full_text = parent.get_text()
                if search_text in full_text:
                    return full_text.split(search_text)[-1].strip()
        return None

    def find_b_tag_after(soup, search_text):
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–µ–≥ <b> –ø–æ—Å–ª–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        elements = soup.find_all(string=re.compile(re.escape(search_text)))
        for element in elements:
            parent = element.parent
            if parent:
                b_tag = parent.find('b')
                if b_tag:
                    return b_tag
        return None

    def extract_date(text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ –î–î.–ú–ú.–ì–ì–ì–ì –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if text:
            date_match = re.search(r'(\d{2}\.\d{2}\.\d{4})', text)
            return date_match.group(1) if date_match else ''
        return ''

    def format_names(text):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–º–µ–Ω, —É–±–∏—Ä–∞—è –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã"""
        text = re.sub(r'\s*<br\s*/?>\s*', ', ', text)
        text = re.sub(r'\s+', ' ', text)
        names = [name.strip() for name in text.split(',') if name.strip()]
        seen = set()
        unique_names = []
        for name in names:
            if name not in seen:
                seen.add(name)
                unique_names.append(name)
        return ', '.join(unique_names)

    soup = BeautifulSoup(html_content, 'html.parser')
    result = {
        '–ù–∞–∑–≤–∞–Ω–∏–µ': '',
        '–ù–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏/–ø–∞—Ç–µ–Ω—Ç–∞': '',
        '–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏': '',
        '–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏': '',
        '–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏': '',
        '–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏': '',
        '–ê–≤—Ç–æ—Ä—ã': '',
        '–ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å/–ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å': '',
        '–†–µ—Ñ–µ—Ä–∞—Ç': '#–ù/–ü',
        '–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã/–ê–¥—Ä–µ—Å –¥–ª—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏': '',
        '–°—Ç–∞—Ç—É—Å': '',
        '–î–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞': '',
        '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—à–ª–∏–Ω–∞—Ö': '',
        '–Ø–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è': '#–ù/–ü',
        '–°–£–ë–î': '#–ù/–ü',
        '–û–±—ä–µ–º': '#–ù/–ü',
        '–§–æ—Ä–º—É–ª–∞': '#–ù/–ü',
        '–¶–∏—Ç–∏—Ä—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã': '#–ù/–ü',
        '–û–ø–∏—Å–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞': '#–ù/–ó–∞–≥—Ä',
        '–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞': '',
        '–ü—Ä–∏–º–µ—á–∞–Ω–∏—è': ''
    }
    
    errors = []
    
    try:
        # –ù–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏/–ø–∞—Ç–µ–Ω—Ç–∞
        reg_elem = soup.find('a', title=lambda x: x and '–°—Å—ã–ª–∫–∞ –Ω–∞ —Ä–µ–µ—Å—Ç—Ä' in x)
        if reg_elem:
            result['–ù–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏/–ø–∞—Ç–µ–Ω—Ç–∞'] = reg_elem.get_text(strip=True).replace(' ', '')
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ
        title_elem = soup.find('p', id='B542')
        if title_elem:
            title_text = title_elem.get_text(strip=True)
            if '(54)' in title_text:
                result['–ù–∞–∑–≤–∞–Ω–∏–µ'] = title_text.split('(54)')[-1].strip()
        
        # –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
        reg_date_elem = find_element_containing_text(soup, '(15) –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏:')
        if reg_date_elem:
            reg_date_text = reg_date_elem.get_text()
            if '(15) –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏:' in reg_date_text:
                date_part = reg_date_text.split('(15) –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏:')[-1].strip()
                result['–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏'] = extract_date(date_part)
        
        # –ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏
        app_number_elem = find_element_containing_text(soup, '(21) –ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏:')
        if app_number_elem:
            app_number_text = app_number_elem.get_text()
            if '(21) –ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏:' in app_number_text:
                number_part = app_number_text.split('(21) –ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏:')[-1].strip()
                # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
                number_part = re.sub(r'[^\d]', '', number_part)
                result['–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏'] = number_part
        
        # –î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏
        app_date_elem = find_element_containing_text(soup, '(22) –î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏:')
        if app_date_elem:
            app_date_text = app_date_elem.get_text()
            if '(22) –î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏:' in app_date_text:
                date_part = app_date_text.split('(22) –î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏:')[-1].strip()
                result['–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏'] = extract_date(date_part)
        
        # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        pub_date_elem = find_element_containing_text(soup, '(45) –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:')
        if pub_date_elem:
            pub_date_text = pub_date_elem.get_text()
            if '(45) –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:' in pub_date_text:
                date_part = pub_date_text.split('(45) –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:')[-1].strip()
                # –£–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç "–ë—é–ª" –µ—Å–ª–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                if '–ë—é–ª' in date_part:
                    date_part = date_part.split('–ë—é–ª')[0].strip()
                result['–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏'] = extract_date(date_part)
        
        # –ê–≤—Ç–æ—Ä—ã
        authors_elem = find_element_containing_text(soup, '(72) –ê–≤—Ç–æ—Ä(—ã):')
        if authors_elem:
            authors_text = authors_elem.get_text()
            authors_clean = re.sub(r'<[^>]+>', '', str(authors_elem))
            authors_clean = authors_clean.replace('(72) –ê–≤—Ç–æ—Ä(—ã):', '').strip()
            result['–ê–≤—Ç–æ—Ä—ã'] = format_names(authors_clean)
        
        # –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å
        owner_elem = find_element_containing_text(soup, '(73) –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å(–∏):')
        if owner_elem:
            owner_text = owner_elem.get_text()
            owner_clean = re.sub(r'<[^>]+>', '', str(owner_elem))
            owner_clean = owner_clean.replace('(73) –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å(–∏):', '').strip()
            result['–ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å/–ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å'] = format_names(owner_clean)
        
        # –ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã
        address_elem = find_element_containing_text(soup, '–ê–¥—Ä–µ—Å –¥–ª—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏:')
        if address_elem:
            address_text = address_elem.get_text()
            if '–ê–¥—Ä–µ—Å –¥–ª—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏:' in address_text:
                address_part = address_text.split('–ê–¥—Ä–µ—Å –¥–ª—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏:')[-1].strip()
                result['–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã/–ê–¥—Ä–µ—Å –¥–ª—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏'] = address_part
        
        # –°—Ç–∞—Ç—É—Å –∏ –¥–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞
        status_rows = soup.find_all('tr')
        for row in status_rows:
            status_label = row.find('td', id='StatusL')
            if status_label and '–°—Ç–∞—Ç—É—Å:' in status_label.get_text():
                status_value = row.find('td', id='StatusR')
                if status_value:
                    status_text = status_value.get_text(strip=True)
                    if '(' in status_text:
                        result['–°—Ç–∞—Ç—É—Å'] = status_text.split('(')[0].strip()
                        date_match = re.search(r'\(([^)]+)\)', status_text)
                        if date_match:
                            date_str = date_match.group(1)
                            date_match2 = re.search(r'(\d{2}\.\d{2}\.\d{4})', date_str)
                            if date_match2:
                                result['–î–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞'] = date_match2.group(1)
                    else:
                        result['–°—Ç–∞—Ç—É—Å'] = status_text
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—à–ª–∏–Ω–∞—Ö
        for row in status_rows:
            status_label = row.find('td', id='StatusL')
            if status_label and '–ü–æ—à–ª–∏–Ω–∞:' in status_label.get_text():
                status_value = row.find('td', id='StatusR')
                if status_value:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ <br> —Ç–µ–≥–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–æ—à–ª–∏–Ω–µ
                    status_html = str(status_value)
                    if '<br/>' in status_html:
                        parts = status_html.split('<br/>')
                        if len(parts) > 1:
                            # –ë–µ—Ä–µ–º –≤—Ç–æ—Ä—É—é —á–∞—Å—Ç—å (–ø–æ—Å–ª–µ <br/>) –∏ –æ—á–∏—â–∞–µ–º –æ—Ç —Ç–µ–≥–æ–≤
                            fee_part = BeautifulSoup(parts[1], 'html.parser').get_text(strip=True)
                            result['–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—à–ª–∏–Ω–∞—Ö'] = fee_part
                    else:
                        # –ï—Å–ª–∏ –Ω–µ—Ç <br/>, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
                        result['–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—à–ª–∏–Ω–∞—Ö'] = status_value.get_text(strip=True)
        
        # –°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞ (–¥–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏ + 5 –ª–µ—Ç)
        if result['–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏']:
            try:
                app_date = datetime.strptime(result['–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏'], '%d.%m.%Y')
                expiry_date = app_date + timedelta(days=365*5)
                result['–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞'] = expiry_date.strftime('%d.%m.%Y')
            except ValueError as e:
                errors.append(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞: {str(e)}")
        else:
            errors.append("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è")
        
    except Exception as e:
        errors.append(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å –ø–æ–ª–µ–π –∏ –¥–æ–±–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏
    required_fields = ['–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏', '–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏', '–ê–≤—Ç–æ—Ä—ã', '–ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å/–ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å']
    for field in required_fields:
        if not result[field]:
            errors.append(f"–ù–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–ª–µ: {field}")
    
    if errors:
        result['–ü—Ä–∏–º–µ—á–∞–Ω–∏—è'] = '; '.join(errors)
    
    return result

# –î–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã
if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        with open(sys.argv[1], 'r', encoding='utf-8') as f:
            html_content = f.read()
        result = parse_industrial_design(html_content)
        print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞:")
        print("=" * 50)
        for key, value in result.items():
            if value and value != '#–ù/–ü' and value != '#–ù/–ó–∞–≥—Ä':  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –ø–æ–ª—è
                print(f"{key}: {value}")
```


-----

# –§–∞–π–ª: fips_parser\parsers\invention_parser.py

```
import re
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

def parse_invention(html_content):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–π"""
    
    # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏
    def find_element_containing_text(soup, search_text):
        """–ù–∞—Ö–æ–¥–∏—Ç —ç–ª–µ–º–µ–Ω—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"""
        elements = soup.find_all(string=re.compile(re.escape(search_text)))
        for element in elements:
            return element.parent
        return None

    def find_element_after_text(soup, search_text):
        """–ù–∞—Ö–æ–¥–∏—Ç —ç–ª–µ–º–µ–Ω—Ç –ø–æ—Å–ª–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        elements = soup.find_all(string=re.compile(re.escape(search_text)))
        for element in elements:
            return element
        return None

    def extract_date(text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ –î–î.–ú–ú.–ì–ì–ì–ì –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        date_match = re.search(r'(\d{2}\.\d{2}\.\d{4})', text)
        return date_match.group(1) if date_match else ''

    def format_names(text):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–º–µ–Ω, —É–±–∏—Ä–∞—è –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã"""
        text = re.sub(r'\s*<br\s*/?>\s*', ', ', text)
        text = re.sub(r'\s+', ' ', text)
        names = [name.strip() for name in text.split(',') if name.strip()]
        seen = set()
        unique_names = []
        for name in names:
            if name not in seen:
                seen.add(name)
                unique_names.append(name)
        return ', '.join(unique_names)

    soup = BeautifulSoup(html_content, 'html.parser')
    result = {
        '–ù–∞–∑–≤–∞–Ω–∏–µ': '',
        '–ù–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏/–ø–∞—Ç–µ–Ω—Ç–∞': '',
        '–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏': '',
        '–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏': '',
        '–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏': '',
        '–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏': '',
        '–ê–≤—Ç–æ—Ä—ã': '',
        '–ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å/–ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å': '',
        '–†–µ—Ñ–µ—Ä–∞—Ç': '',
        '–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã/–ê–¥—Ä–µ—Å –¥–ª—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏': '',
        '–°—Ç–∞—Ç—É—Å': '',
        '–î–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞': '',
        '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—à–ª–∏–Ω–∞—Ö': '',
        '–Ø–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è': '#–ù/–ü',
        '–°–£–ë–î': '#–ù/–ü',
        '–û–±—ä–µ–º': '#–ù/–ü',
        '–§–æ—Ä–º—É–ª–∞': '',
        '–¶–∏—Ç–∏—Ä—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã': '',
        '–û–ø–∏—Å–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞': '#–ù/–ü',
        '–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞': '',
        '–ü—Ä–∏–º–µ—á–∞–Ω–∏—è': ''
    }
    
    errors = []
    
    try:
        # –ù–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏/–ø–∞—Ç–µ–Ω—Ç–∞
        reg_elem = soup.find('a', title=lambda x: x and '–°—Å—ã–ª–∫–∞ –Ω–∞ —Ä–µ–µ—Å—Ç—Ä' in x)
        if reg_elem:
            result['–ù–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏/–ø–∞—Ç–µ–Ω—Ç–∞'] = reg_elem.get_text(strip=True).replace(' ', '')
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ
        title_elem = soup.find('p', id='B542')
        if title_elem:
            title_text = title_elem.get_text(strip=True)
            if '(54)' in title_text:
                result['–ù–∞–∑–≤–∞–Ω–∏–µ'] = title_text.split('(54)')[-1].strip()
        
        # –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
        reg_date_elem = find_element_after_text(soup, '–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏:')
        if reg_date_elem:
            b_tag = reg_date_elem.find_next('b')
            if b_tag:
                result['–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏'] = extract_date(b_tag.get_text(strip=True))
        
        # –ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏ –∏ –¥–∞—Ç–∞ –ø–æ–¥–∞—á–∏
        application_elem = find_element_containing_text(soup, '(21)(22) –ó–∞—è–≤–∫–∞:')
        if application_elem:
            app_link = application_elem.find('a')
            if app_link:
                result['–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏'] = app_link.get_text(strip=True)
            
            app_text = application_elem.get_text()
            if ',' in app_text:
                date_part = app_text.split(',')[-1].strip()
                result['–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏'] = extract_date(date_part)
        
        # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        pub_elem = find_element_containing_text(soup, '(45) –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ:')
        if pub_elem:
            pub_link = pub_elem.find('a')
            if pub_link:
                result['–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏'] = extract_date(pub_link.get_text(strip=True))
        
        # –ê–≤—Ç–æ—Ä—ã
        authors_elem = find_element_containing_text(soup, '(72) –ê–≤—Ç–æ—Ä(—ã):')
        if authors_elem:
            authors_text = authors_elem.get_text()
            authors_clean = re.sub(r'<[^>]+>', '', str(authors_elem))
            authors_clean = authors_clean.replace('(72) –ê–≤—Ç–æ—Ä(—ã):', '').strip()
            result['–ê–≤—Ç–æ—Ä—ã'] = format_names(authors_clean)
        
        # –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å
        owner_elem = find_element_containing_text(soup, '(73) –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å(–∏):')
        if owner_elem:
            owner_text = owner_elem.get_text()
            owner_clean = re.sub(r'<[^>]+>', '', str(owner_elem))
            owner_clean = owner_clean.replace('(73) –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å(–∏):', '').strip()
            result['–ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å/–ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å'] = format_names(owner_clean)
        
        # –†–µ—Ñ–µ—Ä–∞—Ç
        abs_div = soup.find('div', id='Abs')
        if abs_div:
            abs_text = abs_div.get_text(strip=True)
            if '–†–µ—Ñ–µ—Ä–∞—Ç:' in abs_text:
                abs_text = abs_text.split('–†–µ—Ñ–µ—Ä–∞—Ç:', 1)[-1].strip()
            result['–†–µ—Ñ–µ—Ä–∞—Ç'] = abs_text
        
        # –ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã
        address_elem = find_element_after_text(soup, '–ê–¥—Ä–µ—Å –¥–ª—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏:')
        if address_elem:
            b_tag = address_elem.find_next('b')
            if b_tag:
                result['–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã/–ê–¥—Ä–µ—Å –¥–ª—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏'] = b_tag.get_text(strip=True)
        
        # –°—Ç–∞—Ç—É—Å –∏ –¥–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞
        status_rows = soup.find_all('tr')
        for row in status_rows:
            status_label = row.find('td', id='StatusL')
            if status_label and '–°—Ç–∞—Ç—É—Å:' in status_label.get_text():
                status_value = row.find('td', id='StatusR')
                if status_value:
                    status_text = status_value.get_text(strip=True)
                    if '(' in status_text:
                        result['–°—Ç–∞—Ç—É—Å'] = status_text.split('(')[0].strip()
                        date_match = re.search(r'\(([^)]+)\)', status_text)
                        if date_match:
                            date_str = date_match.group(1)
                            date_match2 = re.search(r'(\d{2}\.\d{2}\.\d{4})', date_str)
                            if date_match2:
                                result['–î–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞'] = date_match2.group(1)
                    else:
                        result['–°—Ç–∞—Ç—É—Å'] = status_text
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—à–ª–∏–Ω–∞—Ö
        for row in status_rows:
            status_label = row.find('td', id='StatusL')
            if status_label and '–ü–æ—à–ª–∏–Ω–∞:' in status_label.get_text():
                status_value = row.find('td', id='StatusR')
                if status_value:
                    result['–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—à–ª–∏–Ω–∞—Ö'] = status_value.get_text(strip=True)
        
        # –§–æ—Ä–º—É–ª–∞ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è
        formula_start = soup.find('p', class_='TitCla', string=re.compile('–§–æ—Ä–º—É–ª–∞ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è'))
        if formula_start:
            formula_content = []
            next_elem = formula_start.find_next_sibling()
            while next_elem and not (hasattr(next_elem, 'name') and next_elem.name == 'a' and 'ClEnd' in next_elem.get('href', '')):
                if hasattr(next_elem, 'get_text'):
                    text = next_elem.get_text(strip=True)
                    if text:
                        formula_content.append(text)
                next_elem = next_elem.find_next_sibling()
            result['–§–æ—Ä–º—É–ª–∞'] = '\n'.join(formula_content)
        
        # –¶–∏—Ç–∏—Ä—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        cited_elem = find_element_containing_text(soup, '–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤ –æ—Ç—á–µ—Ç–µ –æ –ø–æ–∏—Å–∫–µ:')
        if cited_elem:
            cited_text = cited_elem.get_text()
            if '–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤ –æ—Ç—á–µ—Ç–µ –æ –ø–æ–∏—Å–∫–µ:' in cited_text:
                docs_part = cited_text.split('–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤ –æ—Ç—á–µ—Ç–µ –æ –ø–æ–∏—Å–∫–µ:')[-1].strip()
                b_tag = cited_elem.find('b')
                if b_tag:
                    result['–¶–∏—Ç–∏—Ä—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã'] = b_tag.get_text(strip=True)
                else:
                    result['–¶–∏—Ç–∏—Ä—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã'] = docs_part
        
        # –°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞ (–¥–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏ + 20 –ª–µ—Ç)
        if result['–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏']:
            try:
                app_date = datetime.strptime(result['–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏'], '%d.%m.%Y')
                expiry_date = app_date + timedelta(days=365*20)
                result['–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞'] = expiry_date.strftime('%d.%m.%Y')
            except ValueError as e:
                errors.append(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞: {str(e)}")
        else:
            errors.append("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è")
        
    except Exception as e:
        errors.append(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å –ø–æ–ª–µ–π –∏ –¥–æ–±–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏
    required_fields = ['–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏', '–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏', '–ê–≤—Ç–æ—Ä—ã', '–ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å/–ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å', '–†–µ—Ñ–µ—Ä–∞—Ç']
    for field in required_fields:
        if not result[field]:
            errors.append(f"–ù–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–ª–µ: {field}")
    
    if errors:
        result['–ü—Ä–∏–º–µ—á–∞–Ω–∏—è'] = '; '.join(errors)
    
    return result

# –î–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã
if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        with open(sys.argv[1], 'r', encoding='utf-8') as f:
            html_content = f.read()
        result = parse_invention(html_content)
        print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è:")
        print("=" * 50)
        for key, value in result.items():
            if value:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –ø–æ–ª—è
                print(f"{key}: {value}")
```


-----

# –§–∞–π–ª: fips_parser\parsers\topology_parser.py

```
def parse_topology(html_content):
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞ —Ç–æ–ø–æ–ª–æ–≥–∏–π –º–∏–∫—Ä–æ—Å—Ö–µ–º"""
    return {'–ü—Ä–∏–º–µ—á–∞–Ω–∏—è': '–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–æ–ø–æ–ª–æ–≥–∏–π –º–∏–∫—Ä–æ—Å—Ö–µ–º –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ'}
```


-----

# –§–∞–π–ª: fips_parser\parsers\utility_model_parser.py

```
import re
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

def parse_utility_model(html_content):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏
    def find_element_containing_text(soup, search_text):
        """–ù–∞—Ö–æ–¥–∏—Ç —ç–ª–µ–º–µ–Ω—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"""
        elements = soup.find_all(string=re.compile(re.escape(search_text)))
        for element in elements:
            return element.parent
        return None

    def find_text_after(soup, search_text):
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        elements = soup.find_all(string=re.compile(re.escape(search_text)))
        for element in elements:
            parent = element.parent
            if parent:
                full_text = parent.get_text()
                if search_text in full_text:
                    return full_text.split(search_text)[-1].strip()
        return None

    def find_b_tag_after(soup, search_text):
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–µ–≥ <b> –ø–æ—Å–ª–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        elements = soup.find_all(string=re.compile(re.escape(search_text)))
        for element in elements:
            parent = element.parent
            if parent:
                b_tag = parent.find('b')
                if b_tag:
                    return b_tag
        return None

    def extract_date(text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ –î–î.–ú–ú.–ì–ì–ì–ì –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if text:
            date_match = re.search(r'(\d{2}\.\d{2}\.\d{4})', text)
            return date_match.group(1) if date_match else ''
        return ''

    def format_names(text):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–º–µ–Ω, —É–±–∏—Ä–∞—è –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã"""
        text = re.sub(r'\s*<br\s*/?>\s*', ', ', text)
        text = re.sub(r'\s+', ' ', text)
        names = [name.strip() for name in text.split(',') if name.strip()]
        seen = set()
        unique_names = []
        for name in names:
            if name not in seen:
                seen.add(name)
                unique_names.append(name)
        return ', '.join(unique_names)

    soup = BeautifulSoup(html_content, 'html.parser')
    result = {
        '–ù–∞–∑–≤–∞–Ω–∏–µ': '',
        '–ù–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏/–ø–∞—Ç–µ–Ω—Ç–∞': '',
        '–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏': '',
        '–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏': '',
        '–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏': '',
        '–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏': '',
        '–ê–≤—Ç–æ—Ä—ã': '',
        '–ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å/–ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å': '',
        '–†–µ—Ñ–µ—Ä–∞—Ç': '',
        '–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã/–ê–¥—Ä–µ—Å –¥–ª—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏': '',
        '–°—Ç–∞—Ç—É—Å': '',
        '–î–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞': '',
        '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—à–ª–∏–Ω–∞—Ö': '',
        '–Ø–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è': '#–ù/–ü',
        '–°–£–ë–î': '#–ù/–ü',
        '–û–±—ä–µ–º': '#–ù/–ü',
        '–§–æ—Ä–º—É–ª–∞': '',
        '–¶–∏—Ç–∏—Ä—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã': '',
        '–û–ø–∏—Å–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞': '#–ù/–ü',
        '–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞': '',
        '–ü—Ä–∏–º–µ—á–∞–Ω–∏—è': ''
    }
    
    errors = []
    
    try:
        # –ù–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏/–ø–∞—Ç–µ–Ω—Ç–∞
        reg_elem = soup.find('a', title=lambda x: x and '–°—Å—ã–ª–∫–∞ –Ω–∞ —Ä–µ–µ—Å—Ç—Ä' in x)
        if reg_elem:
            result['–ù–æ–º–µ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏/–ø–∞—Ç–µ–Ω—Ç–∞'] = reg_elem.get_text(strip=True).replace(' ', '')
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ
        title_elem = soup.find('p', id='B542')
        if title_elem:
            title_text = title_elem.get_text(strip=True)
            if '(54)' in title_text:
                result['–ù–∞–∑–≤–∞–Ω–∏–µ'] = title_text.split('(54)')[-1].strip()
        
        # –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
        reg_date_elem = find_element_containing_text(soup, '–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏:')
        if reg_date_elem:
            b_tag = reg_date_elem.find('b')
            if b_tag:
                result['–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏'] = extract_date(b_tag.get_text(strip=True))
        
        # –ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏ –∏ –¥–∞—Ç–∞ –ø–æ–¥–∞—á–∏
        application_elem = find_element_containing_text(soup, '(21)(22) –ó–∞—è–≤–∫–∞:')
        if application_elem:
            app_b_tag = application_elem.find('b')
            if app_b_tag:
                app_text = app_b_tag.get_text(strip=True)
                if ',' in app_text:
                    parts = app_text.split(',')
                    result['–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏'] = parts[0].strip()
                    result['–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏'] = extract_date(parts[1].strip() if len(parts) > 1 else '')
        
        # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        pub_elem = find_element_containing_text(soup, '(45) –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ:')
        if pub_elem:
            pub_link = pub_elem.find('a')
            if pub_link:
                result['–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏'] = extract_date(pub_link.get_text(strip=True))
        
        # –ê–≤—Ç–æ—Ä—ã
        authors_elem = find_element_containing_text(soup, '(72) –ê–≤—Ç–æ—Ä(—ã):')
        if authors_elem:
            authors_text = authors_elem.get_text()
            authors_clean = re.sub(r'<[^>]+>', '', str(authors_elem))
            authors_clean = authors_clean.replace('(72) –ê–≤—Ç–æ—Ä(—ã):', '').strip()
            result['–ê–≤—Ç–æ—Ä—ã'] = format_names(authors_clean)
        
        # –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å
        owner_elem = find_element_containing_text(soup, '(73) –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å(–∏):')
        if owner_elem:
            owner_text = owner_elem.get_text()
            owner_clean = re.sub(r'<[^>]+>', '', str(owner_elem))
            owner_clean = owner_clean.replace('(73) –ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å(–∏):', '').strip()
            result['–ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å/–ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å'] = format_names(owner_clean)
        
        # –†–µ—Ñ–µ—Ä–∞—Ç
        abs_div = soup.find('div', id='Abs')
        if abs_div:
            abs_text = abs_div.get_text(strip=True)
            if '–†–µ—Ñ–µ—Ä–∞—Ç:' in abs_text:
                abs_text = abs_text.split('–†–µ—Ñ–µ—Ä–∞—Ç:', 1)[-1].strip()
            result['–†–µ—Ñ–µ—Ä–∞—Ç'] = abs_text
        
        # –ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã
        address_elem = find_element_containing_text(soup, '–ê–¥—Ä–µ—Å –¥–ª—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏:')
        if address_elem:
            b_tag = address_elem.find('b')
            if b_tag:
                result['–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã/–ê–¥—Ä–µ—Å –¥–ª—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏'] = b_tag.get_text(strip=True)
        
        # –°—Ç–∞—Ç—É—Å –∏ –¥–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞
        status_rows = soup.find_all('tr')
        for row in status_rows:
            status_label = row.find('td', id='StatusL')
            if status_label and '–°—Ç–∞—Ç—É—Å:' in status_label.get_text():
                status_value = row.find('td', id='StatusR')
                if status_value:
                    status_text = status_value.get_text(strip=True)
                    if '(' in status_text:
                        result['–°—Ç–∞—Ç—É—Å'] = status_text.split('(')[0].strip()
                        date_match = re.search(r'\(([^)]+)\)', status_text)
                        if date_match:
                            date_str = date_match.group(1)
                            date_match2 = re.search(r'(\d{2}\.\d{2}\.\d{4})', date_str)
                            if date_match2:
                                result['–î–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞'] = date_match2.group(1)
                    else:
                        result['–°—Ç–∞—Ç—É—Å'] = status_text
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—à–ª–∏–Ω–∞—Ö
        for row in status_rows:
            status_label = row.find('td', id='StatusL')
            if status_label and '–ü–æ—à–ª–∏–Ω–∞:' in status_label.get_text():
                status_value = row.find('td', id='StatusR')
                if status_value:
                    result['–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—à–ª–∏–Ω–∞—Ö'] = status_value.get_text(strip=True)
        
        # –§–æ—Ä–º—É–ª–∞ –ø–æ–ª–µ–∑–Ω–æ–π –º–æ–¥–µ–ª–∏
        formula_start = soup.find('p', class_='TitCla', string=re.compile('–§–æ—Ä–º—É–ª–∞ –ø–æ–ª–µ–∑–Ω–æ–π –º–æ–¥–µ–ª–∏'))
        if formula_start:
            formula_content = []
            next_elem = formula_start.find_next_sibling()
            while next_elem and not (hasattr(next_elem, 'name') and next_elem.name == 'a' and 'ClEnd' in next_elem.get('href', '')):
                if hasattr(next_elem, 'get_text'):
                    text = next_elem.get_text(strip=True)
                    if text:
                        formula_content.append(text)
                next_elem = next_elem.find_next_sibling()
            result['–§–æ—Ä–º—É–ª–∞'] = '\n'.join(formula_content)
        
        # –¶–∏—Ç–∏—Ä—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        cited_elem = find_element_containing_text(soup, '–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤ –æ—Ç—á–µ—Ç–µ –æ –ø–æ–∏—Å–∫–µ:')
        if cited_elem:
            cited_text = cited_elem.get_text()
            if '–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤ –æ—Ç—á–µ—Ç–µ –æ –ø–æ–∏—Å–∫–µ:' in cited_text:
                docs_part = cited_text.split('–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤ –æ—Ç—á–µ—Ç–µ –æ –ø–æ–∏—Å–∫–µ:')[-1].strip()
                b_tag = cited_elem.find('b')
                if b_tag:
                    result['–¶–∏—Ç–∏—Ä—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã'] = b_tag.get_text(strip=True)
                else:
                    result['–¶–∏—Ç–∏—Ä—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã'] = docs_part
        
        # –°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞ (–¥–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏ + 10 –ª–µ—Ç)
        if result['–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏']:
            try:
                app_date = datetime.strptime(result['–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏'], '%d.%m.%Y')
                expiry_date = app_date + timedelta(days=365*10)
                result['–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞'] = expiry_date.strftime('%d.%m.%Y')
            except ValueError as e:
                errors.append(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∞: {str(e)}")
        else:
            errors.append("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è")
        
    except Exception as e:
        errors.append(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å –ø–æ–ª–µ–π –∏ –¥–æ–±–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏
    required_fields = ['–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏', '–î–∞—Ç–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏', '–ê–≤—Ç–æ—Ä—ã', '–ü—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å/–ü–∞—Ç–µ–Ω—Ç–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª—å', '–†–µ—Ñ–µ—Ä–∞—Ç']
    for field in required_fields:
        if not result[field]:
            errors.append(f"–ù–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–ª–µ: {field}")
    
    if errors:
        result['–ü—Ä–∏–º–µ—á–∞–Ω–∏—è'] = '; '.join(errors)
    
    return result

# –î–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã
if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        with open(sys.argv[1], 'r', encoding='utf-8') as f:
            html_content = f.read()
        result = parse_utility_model(html_content)
        print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ–ª–µ–∑–Ω–æ–π –º–æ–¥–µ–ª–∏:")
        print("=" * 50)
        for key, value in result.items():
            if value:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –ø–æ–ª—è
                print(f"{key}: {value}")
```


-----

# –§–∞–π–ª: fips_parser\parsers\__init__.py

```

```
